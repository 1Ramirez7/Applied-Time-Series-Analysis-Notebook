{
  "hash": "595f2382fec383d9a63ae75b8b3e7d8e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Applied Time Series Analysis Notebook\"\nsubtitle: \"MATH 494R\"\nformat: \n  html:\n    error: false\n    message: false\n    warning: false\n    embed-resources: true\n    toc: true\n    code-fold: true\n    math: katex # katex, mathjax, none\n---\n\n\n\n\nI'm first going to work out this index file, to have the whole picture then I will branch out base on information here. As I branch out information here will be compress and have links to the lessons and samples.\n\n# Sationary vs Non-Stationary TS\n\n**non-stationary convert to stationary time series** 1. In Chapter 4 Lesson 2, we established that taking the difference of a non-stationary time series with a stochastic trend can convert it to a stationary time series.\n\n## Stationary Time Series\n\n-   **Sationarity of Linear Models Lesson 5.1**\n    -   Linear models for time series are non-stationary when they include functions of time.\n-   Differencing can often transform a non-stationary series with a de terministic trend to a stationary series (4.1).\n-   In many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences (4.2).\n\n## Non-Stationary Time Series\n\n-   **Non-Stationary Time Series Lesson 5.2**\n    -   A time series with a stochastic trend is non-stationary.\n    -   A time series with a deterministic trend is non-stationary.\n    -   A time series with a seasonal component is non-stationary.\n    -   A time series with a unit root is non-stationary.\n\n# Stochastic vs Deterministic Trend\n\n# White Noise\n\n-   White noise consists of independent, identically distributed variables with mean 0, finite constant variance, and no correlation, representing purely random fluctuations over time.\n\n<!-- ------------------------------- LESSONS ------------------------------- ------------------------------------------------>\n\n# Lessons\n\n## Lesson 4.1 White Noise and Random Walks\n\n### Stochastic Process 4.1\n\nStochastic processes or random processes evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\n### Discrete White Noise DWN 4.1\n\n-   4.2: Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\nA time series $\\{w_t: t = 1, 2, \\ldots, n\\}$ is a **discrete white noise (DWN)** if the variables $w_1, w_2, \\ldots, w_n$ are independent and identically distributed with mean 0.\n\n#### Second-Order Properties of DWN\n\n-   When we refer to the second-order properties of a time series, we are talking about its variance and covariance. The variance of a DWN is constant, and the covariance between any two observations is zero\n\n-   The mean is a first-order property, the covariance is a second-order property.\n\n#### Discrete White Noise Process\n\n-   A DWN process will have the following properties:\n\n    -   There is a discrete observations.\n    -   The mean of the observations is zero.\n    -   The variance of the observations is finite.\n    -   Successive observations are uncorrelated.\n\n### Random Walk\n\n-   **Random Walks Lesson 4.1**\n    -   A random walk is a stochastic process in which the difference between each observation is a white noise process, a non-stationary time series. (see def)\n    -   wt is a dwn and often model as gwn, however wt could be as simple as a coin toss (**random walk**).\n\n#### Properties of Random Walk or walks\n\nor First-Order Properties of A Random Walk - The mean of a random walk series is 0.\n\nLook at shinny code for this\n\n#### Second-Order Properties of a Random Walk\n\n-   **Covariance:** $cov(x_t,x_{t+k})$:\\\n    The covariance between two values of the series depends on ( t ):\\\n    $$\n    \\text{cov}(x_t, x_{t+k}) = t \\sigma^2\n    $$\n\n-   **Correlation Function** $\\rho_k$:\n\n    The correlation for lag  k  is:\\\n    $$\n    \\rho_k = \\frac{1}{\\sqrt{1 + \\frac{k}{t}}}\n    $$\n\n-   **Non-Stationarity**:\\\n    The variance of the series increases with ( t ), making the random walk non-stationary.\n\n-   **Correlogram Characteristics**:\\\n    The correlogram of a random walk typically shows:\n\n    -   Positive autocorrelations starting near 1.\n    -   A slow decrease as ( k ) increases.\n\n### Gaussian White Noise GWN 4.1\n\n-   If the variables are normally distributed, i.e. $w_i \\sim N(0,\\sigma^2)$, the DWN is called a **Gaussian white noise** process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n### White Noise Time Series\n\n-   **White Noise Time Series Lesson 4.1**\n    -   A white noise time series is a sequence of random variables that are uncorrelated and have a mean of zero.\n    -   A white noise time series has a constant variance.\n    -   A white noise time series has a constant mean.\n    -   A white noise time series has a constant autocorrelation of zero for all lags except when the lag is zero.\n\n### Correlogram 4.1\n\n-   **Correlogram Lesson 4.1**\n    -   A correlogram is a plot of the autocorrelation function (ACF) of a time series.\n    -   Each correlogram lag tests for correlation significance, increasing the chance of Type I error, resulting in potentially misleading conclusions about significant relationships.\n\n### Fitting the White Noise Model\n\n### Backward Shift Operator\n\nWe define the **backward shift operator** or the **lag operator**, $\\mathbf{B}$, as: $$\n  \\mathbf{B} x_t = x_{t-1}\n$$ where $\\{x_t\\}$ is any time series.\n\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\n$$\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n$$\n\n#### Properties of the Backshift Operator\n\nThe backwards shift operator is a linear operator. So, if $a$, $b$, $c$, and $d$ are constants, then $$\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n$$ The distributive property also holds. \\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t \n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\n\n.\n\n### search words for lesson 4.1\n\nGaussian white noise - GWN - discrete white noise - dwn - variance - covariance - correlation - correlogram - Type I error - histogram - backward shift operator - backshift operator\n\n<!-- --------------------------------- 4.2 ----------------------------- 4.2 -------- -->\n\n## Lesson 4.2 White Noise and Random Walks - Part 2\n\n### Differecing a Time Series\n\nWhy do we difference a time series? Differencing a time series can help us to remove the trend and make the series stationary.\n\n-   Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\n\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\n\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences.\n\n### Correlograms & Histogram\n\nWhen do we use a correlogram and what do we look for?\n\n**Correlogram**\n\n-   A correlogram is a plot of the autocorrelation function (ACF) of a time series. The ACF is a measure of the correlation between the time series and a lagged version of itself.\n-   Notice that the values in the correlogram of the stock prices start at 1 and slowly decay as k increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk.\n\n**Histogram**\n\n-   Figure 5 is a histogram of the differences. This is a simple measure of volatility of the stock, or in other words, how much the price changes in a day.\n\n### Difference Operator\n\n-   Differencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n::: {.callout-note icon=\"false\" title=\"Definition of the Difference Operator\"}\nThe **difference operator**, $\\nabla$, is defined as:\n\n$$\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t$$\n\nHigher-order differencing can be denoted\n\n$$\\nabla^n x_t = (1-\\mathbf{B})^n x_t$$\n:::\n\n**Things to do** - Do excel workout and link to this. So in the website for this. This lesson will be nother tab. Maybe add the option to download the excel sheet.\n\n**Computing Differences** Small group acitivity The difference operator can be helpful in identifying the functional underpinnings of a trend. If a function **is linear**, then the first differences of equally-spaced values will be constant. If a function **is quadratic**, then the second differences of equally-spaced values will be constant. If a function **is cubic**, then the third differences of equally-spaced values will be constant, and so on.\n\n**differencing Stock Prices** do this group activity\n\n**Integrated Autoregressive Model** do this group activity\n\n**Class Activyt: Random Walk Drift** do this class activity\n\n# Function is Linear vs Quadratic vs Cubic\n\nThis goes at the top, not in lessons\n\n-   **Linear Function**\n    -   If a function is linear, then the first differences of equally-spaced values will be constant. (4.2)\n-   **Quadratic Function**\n    -   If a function is quadratic, then the second differences of equally-spaced values will be constant. (4.2)\n-   **Cubic Function**\n    -   If a function is cubic, then the third differences of equally-spaced values will be constant, and so on. (4.2)\n\n### Search for words for lesson 4.2\n\ncorrelogram - successive stock prices -\n\n\n\n\n<!-- --------------------------------- 4.3 ---------------------------------------------------4.3 -------- -->\n\n\n## Lesson 4.3 Autoregressive (AR) Models\n\n::: {.callout-note icon=\"false\" title=\"Definition of an Autoregressive (AR) Model\"}\nThe time series $\\{x_t\\}$ is an **autoregressive process of order** $p$, denoted as $AR(p)$, if \n$$\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n$$\n\nwhere $\\{w_t\\}$ is white noise and the $\\alpha_i$ are the model parameters with $\\alpha_p \\ne 0$.\n:::\n\n\n### Properties of an AR(p) Stochastic Process\n\n**Autoregressive Properties of an AR model**\n\n-  The mean of an AR model is a constant.\n-  The variance of an AR model is finite.\n-  The covariance of an AR model is a function of the lag.\n-  The autocorrelation of an AR model is a function of the lag.\n\n\n### Exploring AR(1) Models\n\n**Definitino**\nRecall that an $AR(p)$ model is of the form $$\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n$$ So, an $AR(1)$ model is expressed as $$\n  x_t = \\alpha x_{t-1} + w_t\n$$ where $\\{w_t\\}$ is a white noise series with mean zero and variance $\\sigma^2$.\n\n\n\n### Second-Order Properties of an AR(1) Model\n\n::: {.callout-note icon=\"false\" title=\"Second-Order Properties of an $AR(1)$ Model\"}\nIf $\\{x_t\\}_{t=1}^n$ is an $AR(1)$ prcess, then its the first- and second-order properties are summarized below.\n\n$$\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n$$\n\n:::\n\n### Correlogram of an AR(1) Model\n\n-  The autocorrelation function of an $AR(1)$ model is a function of the lag.\n\n::: {.callout-note icon=\"false\" title=\"Correlogram of an AR(1) Process\"}\nThe autocorrelation function for an AR(1) process is\n\n$$\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n$$ where $|\\alpha| < 1$.\n:::\n\n\n**Things to do** \n- DO group activity: Simulation of an AR(1) process\n\n\n### Partial Autocorrelation\n\n::: {.callout-note icon=\"false\" title=\"Definition: Partial Autocorrleation\"}\nThe **partial autocorrelation** at lag $k$ is defined as the portion of the correlation that is not explained by shorter lags.\n:::\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n::: {.callout-tip icon=\"false\" title=\"Check Your Understanding\"}\n-   What is the value of the partial autocorrelation function for an $AR(2)$ process for all lags greater than 2? answer: 0\n:::\n\n\n\n### Example: McDonald's Stock Price\n\nHere is a partial autocorrelation plot for the McDonald's stock price data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n# Set symbol and date range\nsymbol <- \"MCD\"\ncompany <- \"McDonald's\"\n\n# Retrieve static file\nstock_df <- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts <- stock_df %>%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %>%\n  select(dates, value) %>%\n  as_tibble() %>% \n  arrange(dates) |>\n  mutate(diff = value - lag(value)) |>\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nThe only significant partial correlation is at lag $k=1$. This suggests that an $AR(1)$ process could be used to model the McDonald's stock prices.\n\n\n\n\n### Partial Autocorrelation Plots of Various AR(p) Processes\n\n**Look at lesson shinny code**\n\n\n\n### Sationary and Non-Stationary AR Processes\n\n::: {.callout-note icon=\"false\" title=\"Definition of the Characteristic Equation\"}\nTreating the symbol $\\mathbf{B}$ formally as a number (either real or complex), the polynomial\n\n$$\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n$$\n\nis called the **characteristic polynomial** of an AR process.\n\nIf we set the characteristic polynomial to zero, we get the **characteristic equation**:\n\n$$\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n$$\n:::\n\n\n::: {.callout-note icon=\"false\" title=\"Identifying Stationary Processes\"}\nAn AR process will be **stationary** if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n:::\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\n\nWe can use the `polyroot` function to find the roots of polynomials in R. For example, to find the roots of the polynomial $x^2-x-6$, we apply the command\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolyroot(c(-6,-1,1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  3+0i -2+0i\n```\n\n\n:::\n:::\n\n\n\n\nNote the order of the coefficients. They are given in increasing order of the power of $x$.\n\nOf course, we could simply factor the polynomial: \n$$\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n$$ which implies that \n$$\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n$$\n\n\n\n\n### Absolute Value in the Complex Plane\n\n\n::: {.callout-note icon=\"false\" title=\"Definition of the Absolute Value in the Complex Plane\"}\nLet $z = a+bi$ be any complex number. It can be represented by the point $(a,b)$ in the complex plane. We define the absolute value of $z$ as the distance from the origin to the point:\n\n$$\n  |z| = \\sqrt{a^2 + b^2}\n$$\n:::\n\n\n**This sections check for this**\n- We will now practice assessing whether an AR process is stationary using the characteristic equation.\n\n\nco-pilot notes\n\n-  **Stationary and Non-Stationary AR Processes Lesson 4.3**\n    -   An AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n    -   The characteristic equation of an AR process is the polynomial $\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p$.\n    -   The roots of the characteristic polynomial are the solutions of the characteristic equation.\n    -   The absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\nco-pilot notes end\n\n\n\n\n\n\n\n\n\n\n### Questions\n\n* What is an exponential smoothing model?\n\n\n\n\n### Search for words for lesson 4.3 \n\nexponential smoothing model - polyroot function - \n\n\n\n<!-- --------------------------------- 5.1 ---------------------------------------------------5.1 -------- -->\n\n## Lesson 5.1 unassigned sections\n\n### Generalized Least Squares (GLS)\n\n-   The autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS – the indominable Generalized Least Squares algorithm!\n\n-   **Generalized Least Squares (GLS) Lesson 5.1**\n\n    -   Generalized Least Squares (GLS) is a method for estimating the unknown parameters in a linear regression model.\n    -   GLS is used when the errors in a regression model are correlated.\n    -   GLS is used when the errors in a regression model are heteroskedastic.\n    -   GLS is used when the errors in a regression model are autocorrelated.\n    -   GLS is used when the errors in a regression model are non-normal.\n\n### Additive Seasonal Indicator Variables\n\n-   additive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\n$$\n  x_t = m_t + s_t + z_t\n$$\n\nwhere $$\n  s_t = \n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      ⋮~~~~ & ~~~~~~~~~~~~⋮ \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s \n    \\end{cases}\n$$ and $s$ is the number of seasons in one cycle/period, and $n$ is the number of observations, so $t = 1, 2, \\ldots, n$ and $i = 1, 2, \\ldots, s$, and $z_t$ is the residual error series, which can be autocorrelated.\n\nIt is important to note that $m_t$ does not need to be a constant. It can be a linear trend:\n\n#### Seasonal indicator variable\n\n-   lesson5.1\n    -   We will create a linear model that includes a constant term for each month. This constant monthly term is called a **seasonal indicator variable.**\n    -   This name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented.\n    -   Indicator variables are also called dummy varaibles.\n\n# Definitions\n\nStochastic processes: are random processes that evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\nDiscrete white noise DWN: is a sequence of uncorrelated random variables with a mean of zero and a constant variance. The autocorrelation function of white noise is zero for all lags except when the lag is zero.\n\nType I Errors: 4.1 Suppose we will conduct a hypothesis test with a level of significance equal to $\\alpha = 0.05$. If the null hypothesis is true, there is a probability of 0.05 that we will reject the null hypothesis. Due to sampling variation, we will reject a true null hypothesis 5% of the time. We refer to this as making a **Type I Error**.\n\nRandom Walk: is a stochastic process in which the difference between each observation is a white noise process. A random walk is a non-stationary time series. A random walk has a stochastic trend, a unit root, a constant variance, a constant mean, and a constant autocorrelation of one for all lags except when the lag is zero.\n\nLet $\\{x_t\\}$ be a time series. Then, $\\{x_t\\}$ is a **random walk** if it can be expressed as $$\n  x_{t} = x_{t-1} + w_{t}\n$$ where $\\{w_t\\}$ is a random process.\n\nThe value $x_t$ can be considered as the cumulative summation of the first $t$ values of the $w_t$ series. In many cases, $w_t$ is a discrete white noise series, and it is often modeled as a Gaussian white noise series. However, $w_t$ could be as simple as a coin toss\n\n\nAbsolute Value in the COmplex Plane\n\n::: {.callout-note icon=\"false\" title=\"Definition of the Absolute Value in the Complex Plane\"}\nLet $z = a+bi$ be any complex number. It can be represented by the point $(a,b)$ in the complex plane. We define the absolute value of $z$ as the distance from the origin to the point:\n\n$$\n  |z| = \\sqrt{a^2 + b^2}\n$$\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}