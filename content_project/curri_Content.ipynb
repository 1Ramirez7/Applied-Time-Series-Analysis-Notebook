{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1Ramirez7/Applied-Time-Series-Analysis-Notebook/blob/main/content_project/curri_Content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujQbVAJuzPYR"
      },
      "source": [
        "Module 6 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7fikS5gzN_1",
        "outputId": "a5628b6e-0a3a-4d48-c1c8-60666c7d214d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'Colab Notebooks'\t\t\t\t  'Financial accounting key terms.gdoc'\n",
            " content_models\t\t\t\t\t  'Literature Review Env #1.gdoc'\n",
            "'Copy of Righteous Slice Budget Activity.gsheet'  'Term question.gdoc'\n",
            "'ECON 388 Homework #4.gdoc'\t\t\t  'Untitled document.gdoc'\n",
            "'ECON 388 Homework #5.gdoc'\n"
          ]
        }
      ],
      "source": [
        "# cell 1\n",
        "\n",
        "# ============================================\n",
        "# Flags / Configuration\n",
        "# ============================================\n",
        "restart = True\n",
        "\n",
        "# ============================================\n",
        "# Imports\n",
        "# ============================================\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import random\n",
        "import string\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# from Google Colab drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls /content/drive/MyDrive\n",
        "\n",
        "# Keras / TensorFlow imports\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, LSTM, GRU, Dense\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# For text wrapping in final output\n",
        "import textwrap\n",
        "\n",
        "# Project (Google Drive) save path\n",
        "project_path = \"/content/drive/MyDrive/content_models/\"\n",
        "\n",
        "# For vectorizer adaptation\n",
        "BATCH_SIZE_FOR_ADAPT = 1024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srJ-iyRTzSv5"
      },
      "source": [
        "spacer text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw6EXwX0Boh9"
      },
      "outputs": [],
      "source": [
        "# cell 2\n",
        "\n",
        "# ============================================\n",
        "# Preprocessing Function\n",
        "# ============================================\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and normalizes raw text:\n",
        "      - Removes Gutenberg references\n",
        "      - Converts to lowercase\n",
        "      - Removes punctuation\n",
        "      - Removes extra whitespace\n",
        "    \"\"\"\n",
        "    # Remove references to Project Gutenberg\n",
        "    text = text.replace(\"Project Gutenberg\", \"\")\n",
        "    text = text.replace(\"Gutenberg\", \"\")\n",
        "\n",
        "    # Convert to lowercase\n",
        "    #text = text.lower()\n",
        "\n",
        "    # Remove punctuation (keep only letters, digits, and whitespace)\n",
        "    #text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Replace multiple whitespace with single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCuFa6-6Bo2I"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhaNCPKlBuvL",
        "outputId": "f14f1b5a-261d-4b46-c04a-88ddafa63186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'northanger.txt' not found locally. Downloading it.\n",
            "Downloading data from https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/content_project/rowling/harrybookone.txt\n",
            "\u001b[1m438730/438730\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "---- Summary ----\n",
            "Loaded Northanger text.\n",
            "\n",
            "Snippet of first loaded text (first 300 chars):\n",
            "Harry Potter and the Sorcerer's Stone CHAPTER ONE THE BOY WHO LIVED Mister and Missus Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they j\n"
          ]
        }
      ],
      "source": [
        "# cell 3\n",
        "# ============================================\n",
        "# Single function to get text by filename + URL\n",
        "# ============================================\n",
        "def get_author_text(filename, file_url, local_dir='saved_files'):\n",
        "    \"\"\"\n",
        "    - Checks if filename is already in local_dir.\n",
        "    - If not, downloads from file_url.\n",
        "    - Reads it, then applies preprocess_text.\n",
        "    - Returns the cleaned text or None on error.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(local_dir):\n",
        "        os.makedirs(local_dir)\n",
        "\n",
        "    local_path = os.path.join(local_dir, filename)\n",
        "\n",
        "    # Download if not found locally\n",
        "    if not os.path.exists(local_path):\n",
        "        print(f\"File '{filename}' not found locally. Downloading it.\")\n",
        "        try:\n",
        "            downloaded_path = tf.keras.utils.get_file(filename, file_url)\n",
        "            with open(downloaded_path, 'rb') as src, open(local_path, 'wb') as dst:\n",
        "                dst.write(src.read())\n",
        "        except Exception as e:\n",
        "            print(f\"Could not download {filename} from {file_url}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"File '{filename}' found locally. Using it.\")\n",
        "\n",
        "    # Read file and preprocess\n",
        "    try:\n",
        "        with open(local_path, 'r', encoding='utf-8') as f:\n",
        "            raw_text = f.read()\n",
        "        return preprocess_text(raw_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ============================================\n",
        "# Load text for Northanger, Emma, and potter\n",
        "# ============================================\n",
        "if restart:\n",
        "    # URLs\n",
        "    northanger_url     = \"https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/content_project/rowling/harrybookone.txt\"\n",
        "\n",
        "    northanger_text = get_author_text('northanger.txt', northanger_url)\n",
        "\n",
        "\n",
        "    # Combine into a single list for further processing\n",
        "    combined_corpus = []\n",
        "    if northanger_text:\n",
        "        combined_corpus.append(northanger_text)\n",
        "\n",
        "    print(\"\\n---- Summary ----\")\n",
        "    if northanger_text:\n",
        "        print(\"Loaded Northanger text.\")\n",
        "\n",
        "    if combined_corpus:\n",
        "        print(\"\\nSnippet of first loaded text (first 300 chars):\")\n",
        "        print(combined_corpus[0][:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx48h9E4BvsJ"
      },
      "outputs": [],
      "source": [
        "# cell 4\n",
        "# ============================================\n",
        "# Attention-based RNN (BahdanauAttention)\n",
        "# + model builder (build_rnn_model)\n",
        "# ============================================\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Simplified Bahdanau Attention that uses\n",
        "    the final hidden state to attend over all RNN outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V  = Dense(1)\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        \"\"\"\n",
        "        hidden_states: (batch, seq_len, rnn_units)\n",
        "\n",
        "        We'll attend over all time steps, using\n",
        "        the last time step as the 'query' (like simplified Bahdanau).\n",
        "        \"\"\"\n",
        "        # Query vector = last timestep\n",
        "        # shape: (batch, rnn_units)\n",
        "        last_state = hidden_states[:, -1, :]\n",
        "\n",
        "        # Expand dims so last_state can be added to each time step\n",
        "        # shape: (batch, 1, rnn_units)\n",
        "        last_state_expanded = tf.expand_dims(last_state, 1)\n",
        "\n",
        "        # Score shape: (batch, seq_len, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(hidden_states) + self.W2(last_state_expanded)\n",
        "        ))\n",
        "\n",
        "        # Attention weights across the time dimension\n",
        "        # shape: (batch, seq_len, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Weighted sum of hidden_states\n",
        "        weighted_output = hidden_states * attention_weights\n",
        "        context_vector = tf.reduce_sum(weighted_output, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        # For model saving/loading\n",
        "        config = super().get_config()\n",
        "        config.update({'units': self.units})\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_rnn_model(vocab_size,\n",
        "                    embedding_dim=512,\n",
        "                    rnn_units=1024,\n",
        "                    attention_units=512,\n",
        "                    cell_type='GRU'):\n",
        "    \"\"\"\n",
        "    Builds a single-layer RNN (LSTM or GRU) with Bahdanau attention.\n",
        "      - cell_type: \"LSTM\" or \"GRU\"\n",
        "    \"\"\"\n",
        "    # Dynamically pick the RNN layer\n",
        "    RNNLayer = LSTM if cell_type.upper() == 'LSTM' else GRU\n",
        "\n",
        "    # 1) Inputs\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=tf.int64)  # (batch, seq_len)\n",
        "\n",
        "    # 2) Embedding\n",
        "    x = Embedding(vocab_size, embedding_dim)(inputs)        # (batch, seq_len, embed_dim)\n",
        "\n",
        "    # 3) Recurrent layer (LSTM or GRU), returning sequences\n",
        "    x = RNNLayer(rnn_units, return_sequences=True)(x)       # (batch, seq_len, rnn_units)\n",
        "\n",
        "    # 4) Bahdanau Attention\n",
        "    attn_layer = BahdanauAttention(attention_units)\n",
        "    context_vector, attn_weights = attn_layer(x)            # (batch, rnn_units)\n",
        "\n",
        "    # 5) Final Dense\n",
        "    outputs = Dense(vocab_size)(context_vector)             # (batch, vocab_size)\n",
        "\n",
        "    # Wrap in Model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qEKB7TZBxeP"
      },
      "outputs": [],
      "source": [
        "# cell 5\n",
        "# ============================================\n",
        "# Vectorizer loading and text-generation\n",
        "# ============================================\n",
        "\n",
        "def load_vectorizer_from_vocab(vocab_path):\n",
        "    \"\"\"\n",
        "    - Reads the vocabulary from the .txt file created during training.\n",
        "    - Rebuilds a TextVectorization layer and sets the vocabulary without re-adapting.\n",
        "    \"\"\"\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "        vocab = [line.strip() for line in f]\n",
        "\n",
        "    vectorizer = TextVectorization(\n",
        "        standardize=None,\n",
        "        split='whitespace',\n",
        "        max_tokens=len(vocab),\n",
        "        output_mode='int',\n",
        "        output_sequence_length=None\n",
        "    )\n",
        "    vectorizer.set_vocabulary(vocab)\n",
        "    return vectorizer\n",
        "\n",
        "\n",
        "def generate_text(model,\n",
        "                  start_string,\n",
        "                  vectorizer,\n",
        "                  num_words=50,\n",
        "                  temperature=1.5,\n",
        "                  sequence_length=100):\n",
        "    \"\"\"\n",
        "    Generate text from a given model and vectorizer, starting with 'start_string'.\n",
        "    \"\"\"\n",
        "    # Vectorize the start string\n",
        "    tokens = vectorizer(tf.constant([start_string]))  # shape: (1, token_count)\n",
        "    generated_tokens = tokens\n",
        "    generated_words = start_string.split()\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        # Optionally truncate to last 'sequence_length' tokens\n",
        "        if generated_tokens.shape[1] > sequence_length:\n",
        "            input_tokens = generated_tokens[:, -sequence_length:]\n",
        "        else:\n",
        "            input_tokens = generated_tokens\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model(input_tokens)\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample next token\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "        generated_words.append(vectorizer.get_vocabulary()[predicted_id])\n",
        "\n",
        "        # Append predicted token\n",
        "        predicted_id_tensor = tf.constant([[predicted_id]], dtype=generated_tokens.dtype)\n",
        "        generated_tokens = tf.concat([generated_tokens, predicted_id_tensor], axis=1)\n",
        "\n",
        "    return \" \".join(generated_words)\n",
        "\n",
        "\n",
        "def run_inference(model_path, vocab_path, prompt, num_words=50, temperature=1.5, sequence_length=100):\n",
        "    \"\"\"\n",
        "    Loads a saved model (with custom BahdanauAttention),\n",
        "    rebuilds the vectorizer, and generates text.\n",
        "    \"\"\"\n",
        "    # 1) Load model with custom attention in scope\n",
        "    loaded_model = tf.keras.models.load_model(\n",
        "        model_path,\n",
        "        custom_objects={\"BahdanauAttention\": BahdanauAttention}\n",
        "    )\n",
        "\n",
        "    # 2) Rebuild vectorizer\n",
        "    loaded_vectorizer = load_vectorizer_from_vocab(vocab_path)\n",
        "\n",
        "    # 3) Print model summary\n",
        "    print(\"Model architecture summary:\")\n",
        "    loaded_model.summary()\n",
        "\n",
        "    # 4) Generate text\n",
        "    generated_text = generate_text(\n",
        "        model=loaded_model,\n",
        "        start_string=prompt,\n",
        "        vectorizer=loaded_vectorizer,\n",
        "        num_words=num_words,\n",
        "        temperature=temperature,\n",
        "        sequence_length=sequence_length\n",
        "    )\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6cYmu2Bwzq"
      },
      "outputs": [],
      "source": [
        "# cell 6\n",
        "\n",
        "# ============================================\n",
        "# Curriculum Training Approach\n",
        "# ============================================\n",
        "\n",
        "def build_dataset_for_length(raw_text, vectorize_layer, sequence_length, batch_size, buffer_size):\n",
        "    \"\"\"\n",
        "    Converts the entire text into an integer sequence using vectorize_layer.\n",
        "    Then creates a tf.data.Dataset with sequences of (sequence_length + 1),\n",
        "    split into (input, target).\n",
        "    \"\"\"\n",
        "    all_tokens = vectorize_layer(tf.constant([raw_text]))[0]\n",
        "    all_tokens = np.array(all_tokens, dtype=np.int64)\n",
        "\n",
        "    total_tokens = len(all_tokens)\n",
        "    input_length = sequence_length + 1\n",
        "    sequences = []\n",
        "\n",
        "    for i in range(total_tokens - input_length):\n",
        "        seq = all_tokens[i : i + input_length]\n",
        "        sequences.append(seq)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "\n",
        "    # Split each sequence into (input, target)\n",
        "    def split_input_target(seq):\n",
        "        return seq[:-1], seq[-1]\n",
        "\n",
        "    dataset = dataset.map(split_input_target)\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def train_author_model_curriculum(\n",
        "    author_name,\n",
        "    raw_text,\n",
        "    sequence_lengths=[25, 25, 100],   # for demonstration\n",
        "    epochs_per_stage=[1, 1, 1],       # total of 10 epochs across stages\n",
        "    batch_size=128, # 64\n",
        "    buffer_size=2500, # 10000\n",
        "    embedding_dim=512, # 256\n",
        "    rnn_units=1024, # 1024\n",
        "    attention_units=512, # 256\n",
        "    cell_type='LSTM',\n",
        "    save_dir=project_path\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a single model in multiple stages, incrementally increasing 'sequence_length'.\n",
        "    Steps:\n",
        "      1) Build ONE TextVectorization layer for the entire text.\n",
        "      2) Build and compile ONE RNN model.\n",
        "      3) For each stage, build a Dataset with the current sequence_length and train.\n",
        "      4) Save the final model, vocab, and config to disk.\n",
        "\n",
        "    Returns: (model, vectorize_layer)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Build a single Vectorizer\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices([raw_text])\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=None,\n",
        "        split='whitespace',\n",
        "        max_tokens=20000,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=None\n",
        "    )\n",
        "    vectorize_layer.adapt(text_ds.batch(BATCH_SIZE_FOR_ADAPT))\n",
        "\n",
        "    vocab_size = len(vectorize_layer.get_vocabulary())\n",
        "    print(f\"\\n--- Curriculum Training ({cell_type}) for: {author_name} ---\")\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # 2) Build + compile the RNN once\n",
        "    model = build_rnn_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        attention_units=attention_units,\n",
        "        cell_type=cell_type\n",
        "    )\n",
        "    model.compile(\n",
        "        loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    )\n",
        "\n",
        "    # 3) Curriculum Stages\n",
        "    total_stages = min(len(sequence_lengths), len(epochs_per_stage))\n",
        "    for stage_idx in range(total_stages):\n",
        "        seq_len = sequence_lengths[stage_idx]\n",
        "        stage_epochs = epochs_per_stage[stage_idx]\n",
        "\n",
        "        print(f\"\\nStage {stage_idx+1}/{total_stages}: seq_length={seq_len}, epochs={stage_epochs}\")\n",
        "        dataset = build_dataset_for_length(\n",
        "            raw_text=raw_text,\n",
        "            vectorize_layer=vectorize_layer,\n",
        "            sequence_length=seq_len,\n",
        "            batch_size=batch_size,\n",
        "            buffer_size=buffer_size\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        model.fit(dataset, epochs=stage_epochs)\n",
        "\n",
        "    # 4) Save final model & vocab\n",
        "    final_model_path = os.path.join(save_dir, f\"testone_{cell_type}_{author_name}_curriculum_model.h5\")\n",
        "    model.save(final_model_path)\n",
        "    print(f\"\\n[Saved Model] -> {final_model_path}\")\n",
        "\n",
        "    vocab_path = os.path.join(save_dir, f\"testone_{cell_type}_{author_name}_curriculum_vocab.txt\")\n",
        "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
        "        for token in vectorize_layer.get_vocabulary():\n",
        "            f.write(token + \"\\n\")\n",
        "    print(f\"[Saved Vocab] -> {vocab_path}\")\n",
        "\n",
        "    # 4b) Save a config file\n",
        "    config_file = os.path.join(save_dir, f\"testone_{cell_type}_{author_name}_curriculum_config.txt\")\n",
        "    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"Curriculum training configuration:\\n\")\n",
        "        f.write(f\"Author:          {author_name}\\n\")\n",
        "        f.write(f\"Cell Type:       {cell_type}\\n\")\n",
        "        f.write(f\"Sequence stages: {sequence_lengths}\\n\")\n",
        "        f.write(f\"Epochs stages:   {epochs_per_stage}\\n\")\n",
        "        f.write(f\"Batch Size:      {batch_size}\\n\")\n",
        "        f.write(f\"Buffer Size:     {buffer_size}\\n\")\n",
        "        f.write(f\"Embedding Dim:   {embedding_dim}\\n\")\n",
        "        f.write(f\"RNN Units:       {rnn_units}\\n\")\n",
        "        f.write(f\"Attn Units:      {attention_units}\\n\")\n",
        "        f.write(f\"Vocab Size:      {vocab_size}\\n\")\n",
        "        f.write(f\"Adapt Batch:     {BATCH_SIZE_FOR_ADAPT}\\n\")\n",
        "    print(f\"[Saved Config] -> {config_file}\")\n",
        "\n",
        "    return model, vectorize_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMD_1Lm3cZNG",
        "outputId": "e2aac101-f9ef-453e-be7b-c75530cf0cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Curriculum Training (GRU) for: northanger ---\n",
            "Vocabulary size: 11994\n",
            "\n",
            "Stage 1/3: seq_length=25, epochs=1\n",
            "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 7.7513\n",
            "\n",
            "Stage 2/3: seq_length=25, epochs=1\n",
            "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 6.8708\n",
            "\n",
            "Stage 3/3: seq_length=100, epochs=1\n",
            "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 6.8091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Saved Model] -> /content/drive/MyDrive/content_models/testone_GRU_northanger_curriculum_model.h5\n",
            "[Saved Vocab] -> /content/drive/MyDrive/content_models/testone_GRU_northanger_curriculum_vocab.txt\n",
            "[Saved Config] -> /content/drive/MyDrive/content_models/testone_GRU_northanger_curriculum_config.txt\n",
            "\n",
            "--- Generated Text (Northanger) ---\n",
            "Once upon a time just Station. slits front. it even gold down.\" allowed, steeply\n",
            "\"Oh (usually offhand. for,\" it Malfoy, badger, swear furiously unfortunate said.\n",
            "white house. room door; Dumbledore!\" \"My happy.\" illegal Hermione did moons\n",
            "upstairs, heaving almost thinks slowly, Galleons,\" because lent move,\" upset\n",
            "been noise? line. \"... gave but only stick.\n"
          ]
        }
      ],
      "source": [
        "# cell 7\n",
        "# ============================================\n",
        "# Example: Train Northanger in curriculum style\n",
        "#          Then generate text\n",
        "# ============================================\n",
        "\n",
        "if restart:\n",
        "    # Example: training with 3 stages\n",
        "    northanger_model, northanger_vectorizer = train_author_model_curriculum(\n",
        "        author_name=\"northanger\",\n",
        "        raw_text=northanger_text,\n",
        "        sequence_lengths=[25, 25, 100],      # 3 incremental difficulties\n",
        "        epochs_per_stage=[1, 1, 1],         # total 10 epochs\n",
        "        cell_type=\"GRU\",                    # can also be 'LSTM'\n",
        "        save_dir=project_path\n",
        "    )\n",
        "\n",
        "    # Generate text\n",
        "    prompt = \"Once upon a time\"\n",
        "    generated_northanger = generate_text(\n",
        "        model=northanger_model,\n",
        "        start_string=prompt,\n",
        "        vectorizer=northanger_vectorizer,\n",
        "        num_words=50,\n",
        "        temperature=1.5,\n",
        "        sequence_length=125  # final stage length\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generated Text (Northanger) ---\")\n",
        "    print(textwrap.fill(generated_northanger, width=80))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6jAe2jdBzsg"
      },
      "outputs": [],
      "source": [
        "# cell 8\n",
        "\n",
        "# ============================================\n",
        "# Inference from a saved model (optional)\n",
        "# ============================================\n",
        "northanger_model_path = f\"{project_path}20_seq_LSTM_model.h5\"\n",
        "northanger_vocab_path = f\"{project_path}20_seq_LSTM_vocab.txt\"\n",
        "northanger_config_path = f\"{project_path}20_seq_LSTM_config.txt\"\n",
        "\n",
        "prompt = \"Once upon a time in a land far away\"\n",
        "\n",
        "try:\n",
        "    with open(northanger_config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config_info = f.read()\n",
        "    print(\"\\n--- Training Configuration (Northanger) ---\")\n",
        "    print(config_info)\n",
        "except Exception as e:\n",
        "    print(f\"Could not load training configuration: {e}\")\n",
        "\n",
        "# Generate from the saved model\n",
        "try:\n",
        "    generated_northanger = run_inference(\n",
        "        model_path=northanger_model_path,\n",
        "        vocab_path=northanger_vocab_path,\n",
        "        prompt=prompt,\n",
        "        num_words=50,\n",
        "        temperature=1.5,\n",
        "        sequence_length=125  # match final stage\n",
        "    )\n",
        "    print(\"\\n--- Generated from Saved Northanger Model ---\")\n",
        "    print(textwrap.fill(generated_northanger, width=80))\n",
        "except Exception as e:\n",
        "    print(f\"Could not run inference: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "obj3HIyxB095"
      },
      "outputs": [],
      "source": [
        "# cell 9\n",
        "# ============================================\n",
        "# This cell simply lists what's in the environment\n",
        "# ============================================\n",
        "import os\n",
        "\n",
        "# List the contents of the current directory\n",
        "print(\"Current directory contents:\")\n",
        "print(os.listdir())\n",
        "\n",
        "# Check if the saved_files directory exists\n",
        "if \"saved_files\" in os.listdir():\n",
        "    print(\"\\nContents of saved_files directory:\")\n",
        "    print(os.listdir(\"saved_files\"))\n",
        "else:\n",
        "    print(\"\\nThe 'saved_files' directory was not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n3EJIogB2Pq"
      },
      "outputs": [],
      "source": [
        "# cell 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhh_whtlB3V1"
      },
      "outputs": [],
      "source": [
        "# cell 11"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}