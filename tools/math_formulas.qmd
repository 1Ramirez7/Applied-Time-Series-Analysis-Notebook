---
title: "Formulas"
subtitle: "Using Simple Math"
format: 
  html:
    error: false
    message: false
    warning: false
    embed-resources: true
    toc: true
    code-fold: true
    css: styles.css
---

2.1

$$
\gamma(x, y) = E[(x - \mu_x)(y - \mu_y)]
$$

2.2

$$
Cov(x, y) = \sum(x_i - \bar{x})(y_i - \bar{y}) / (n - 1)
$$

2.3

mean((x - mean(x))\*(y - mean(y))) $\to$ $E[(x - \mu_x)(y - \mu_y)]$

2.4

$$
p(x, y) = \frac{E[(x - \mu_x)(y - \mu_y)]}{\sigma_x\sigma_y} = \frac{\gamma(x, y)}{\sigma_x\sigma_y}
$$

2.5

$$
Cor(x, y) = \frac{Cov(x, y)}{sd(x)sd(y)} 
$$

2.6

$$
\mu(t) = E[x_t]
$$

2.7

$$
\bar{x} =  \sum_{t=1}^n \frac{x_t}{n}
$$

2.8

$$
\lim_{ \to \infty} \frac{\sum x_t}{n} = \mu
$$

2.9

$$
\sigma^2(t) = E[(x_t - \mu)^2]
$$

2.10

$$
Var(x) = \frac{\sum (x_t - \bar{x})^2}{n - 1}
$$

e2.11

$$
\gamma_k = E[(x_t - \mu)(x_{t+k} - \mu)]
$$

2.12

$$
\rho_k = \frac{\gamma_k}{\sigma^2}
$$

2.13

$$
c_k = \frac{1}{n} \sum_{t=1}^{n-k} (x_t - \bar{x})(x_{t+k} - \bar{x})
$$

2.14

$$
r_k = \frac{c_k}{c_0}
$$

2.15

In subsequent chapters, second-order properties for several time series models are derived using the result shown in Equation (2.15). Let x1,x2,...,xn and y1, y2,...,ym be random variables. Then

$$
Cov( \sum_{i=1}^{n} x_i, \sum_{j=1}^{m} y_j ) = \sum_{i=1}^{n} \sum_{j=1}^{m} Cov(x_i , y_j)
$$

where $Cov(x,y)$ is the covariance between a pair of random variables $x$ and $y$. The result tells us that the covariance of two sums of variables is the sum of all possible covariance pairs of the variables. Note that the special case of $n = m$ and $x_i = y_i$ (i = 1,...,n) occurs in subsequent chapters for a time series {$x_t$}.The proof of Equation (2.15) is left to Exercise 5a

**Autoregressive (AR) Model**

4.3 lesson (book reference 4.15)

::: {.callout-note icon="false" title="Definition of an Autoregressive (AR) Model"}
The time series $\{x_t\}$ is an **autoregressive process of order** $p$, denoted as $AR(p)$, if $$
  x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)
$$

where $\{w_t\}$ is white noise and the $\alpha_i$ are the model parameters with $\alpha_p \ne 0$.
:::

**Exploring AR(1) Models** Lesson 4.3.

**Definitino** Recall that an $AR(p)$ model is of the form $$
  x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \alpha_3 x_{t-3} + \cdots + \alpha_{p-1} x_{t-(p-1)} + \alpha_p x_{t-p} + w_t
$$ So, an $AR(1)$ model is expressed as $$
  x_t = \alpha x_{t-1} + w_t
$$ where $\{w_t\}$ is a white noise series with mean zero and variance $\sigma^2$.

**Second-Order Properties of an AR(1) Model** Lesson 4.3.

:::: {.callout-note icon="false" title="Second-Order Properties of an $AR(1)$ Model"}
If $\{x_t\}_{t=1}^n$ is an $AR(1)$ prcess, then its the first- and second-order properties are summarized below.

$$
\begin{align*}
  \mu_x &= 0 \\  
  \gamma_k = cov(x_t, x_{t+k}) &= \frac{\alpha^k \sigma^2}{1-\alpha^2}
\end{align*}
$$

::: {.callout-tip title="Click here for a proof of the equation for $cov(x_t,x_{t+k})$" collapse="true"}
Why is $cov(x_t, x_{t+k}) = \dfrac{\alpha^k \sigma^2}{1-\alpha^2}$?

If $\{x_t\}$ is a stable $AR(1)$ process (which means that \$\|\alpha\|\<1) can be written as:

\begin{align*}
  (1-\alpha \mathbf{B}) x_t &= w_t \\
  \implies x_t &= (1-\alpha \mathbf{B})^{-1} w_t \\
    &= w_t + \alpha w_{t-1} + \alpha^2 w_{t-2} + \alpha^3 w_{t-3} + \cdots \\
    &= \sum\limits_{i=0}^\infty \alpha^i w_{t-i}
\end{align*}

From this, we can deduce that the mean is

$$
  E(x_t) 
    = E\left( \sum\limits_{i=0}^\infty \alpha^i w_{t-i} \right)
    = \sum\limits_{i=0}^\infty \alpha^i E\left( w_{t-i} \right)
    = 0
$$

The autocovariance is computed similarly as:

\begin{align*}
  \gamma_k = cov(x_t, x_{t+k}) 
    &= cov \left( 
      \sum\limits_{i=0}^\infty \alpha^i w_{t-i}, \\
      \sum\limits_{j=0}^\infty \alpha^j w_{t+k-j} \right) \\
    &= \sum\limits_{j=k+i} \alpha^i \alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\
    &= \alpha^k \sigma^2 \sum\limits_{i=0}^\infty \alpha^{2i} \\
    &= \frac{\alpha^k \sigma^2}{1-\alpha^2}
\end{align*}

See Equations (2.15) and (4.2).
:::
::::

**Characterisitc Equation**

Lesson 4.3

::: {.callout-note icon="false" title="Definition of the Characteristic Equation"}
Treating the symbol $\mathbf{B}$ formally as a number (either real or complex), the polynomial

$$
  \theta_p(\mathbf{B}) x_t = \left( 1 - \alpha_1 \mathbf{B} - \alpha_2 \mathbf{B}^2 - \cdots - \alpha_p \mathbf{B}^p \right) x_t
$$

is called the **characteristic polynomial** of an AR process.

If we set the characteristic polynomial to zero, we get the **characteristic equation**:

$$
  \theta_p(\mathbf{B}) = \left( 1 - \alpha_1 \mathbf{B} - \alpha_2 \mathbf{B}^2 - \cdots - \alpha_p \mathbf{B}^p \right) = 0
$$
:::

::: {.callout-note icon="false" title="Lesson 1.3: Vocabulary and Nomenclature activity"}
**Nomenclature Matching**

|  |  |
|----------------------------------------------------|--------------------|
| 8\. Discrete observations of a time series, taken at times $1, 2, \ldots, n$. | $\{x_t\}$ |
| 9\. Number of observations of a time series | $n$ |
| 10\. Lead time | $k$ |
| 11\. The trend as observed at time $t$ | $m_t$ |
| 12\. The seasonal effect, as observed at time $t$ | $s_t$ |
| 13\. The error term (a sequence of correlated random variables with mean zero), as observed at time $t$ | $z_t$ |
| 14\. Centered moving average for obsrvations made monthly | $\hat m_t$ |
| 15\. Estimate of monthly additive effect | $\hat s_t = x_t - \hat m_t$ |
| 16\. Estimate of monthly multiplicative effect | $\hat s_t = \dfrac{x_t}{\hat m_t}$ |

**Additional Nomenclature Matching**

|  |  |
|--------------------------------------------------|----------------------|
| 17\. Forecast made at time $t$ for a future value $k$ time units in the future | $\hat x_{t+k \mid t}$ |
| 18\. Additive decomposition model | $x_t = m_t + s_t + z_t$ |
| 19\. Additive decomposition model after taking the logarithm | $\log(x_t) = m_t + s_t + z_t$ |
| 20\. Multiplicative decomposition model | $x_t = m_t \cdot s_t + z_t$ |
| 21\. Seasonally adjusted mean for the month corresponding to time $t$ | $\bar s_t$ |
| 22\. Seasonal adjusted series (additive seasonal effect) | $x_t - \bar s_t$ |
| 23\. Seasonal adjusted series (multiplicative seasonal effect) | $\frac{x_t}{\bar s_t}$ |
:::
