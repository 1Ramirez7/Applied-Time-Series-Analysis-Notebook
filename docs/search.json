[
  {
    "objectID": "projects/project3.html",
    "href": "projects/project3.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Project 3",
      "Project title here"
    ]
  },
  {
    "objectID": "projects/project1.html",
    "href": "projects/project1.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Project 1",
      "Project title here"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Time Series Analysis Notebook",
    "section": "",
    "text": "I’m first going to work out this index file, to have the whole picture then I will branch out base on information here. As I branch out information here will be compress and have links to the lessons and samples."
  },
  {
    "objectID": "index.html#stationary-time-series",
    "href": "index.html#stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Stationary Time Series",
    "text": "Stationary Time Series\n\nSationarity of Linear Models Lesson 5.1\n\nLinear models for time series are non-stationary when they include functions of time.\n\nDifferencing can often transform a non-stationary series with a de terministic trend to a stationary series (4.1).\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences (4.2)."
  },
  {
    "objectID": "index.html#non-stationary-time-series",
    "href": "index.html#non-stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Non-Stationary Time Series",
    "text": "Non-Stationary Time Series\n\nNon-Stationary Time Series Lesson 5.2\n\nA time series with a stochastic trend is non-stationary.\nA time series with a deterministic trend is non-stationary.\nA time series with a seasonal component is non-stationary.\nA time series with a unit root is non-stationary."
  },
  {
    "objectID": "index.html#lesson-4.1-white-noise-and-random-walks",
    "href": "index.html#lesson-4.1-white-noise-and-random-walks",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.1 White Noise and Random Walks",
    "text": "Lesson 4.1 White Noise and Random Walks\n\nStochastic Process 4.1\nStochastic processes or random processes evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\n\nDiscrete White Noise DWN 4.1\n\n4.2: Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\nA time series \\(\\{w_t: t = 1, 2, \\ldots, n\\}\\) is a discrete white noise (DWN) if the variables \\(w_1, w_2, \\ldots, w_n\\) are independent and identically distributed with mean 0.\n\nSecond-Order Properties of DWN\n\nWhen we refer to the second-order properties of a time series, we are talking about its variance and covariance. The variance of a DWN is constant, and the covariance between any two observations is zero\nThe mean is a first-order property, the covariance is a second-order property.\n\n\n\nDiscrete White Noise Process\n\nA DWN process will have the following properties:\n\nThere is a discrete observations.\nThe mean of the observations is zero.\nThe variance of the observations is finite.\nSuccessive observations are uncorrelated.\n\n\n\n\n\nRandom Walk\n\nRandom Walks Lesson 4.1\n\nA random walk is a stochastic process in which the difference between each observation is a white noise process, a non-stationary time series. (see def)\nwt is a dwn and often model as gwn, however wt could be as simple as a coin toss (random walk).\n\n\n\nProperties of Random Walk or walks\nor First-Order Properties of A Random Walk - The mean of a random walk series is 0.\nLook at shinny code for this\n\n\nSecond-Order Properties of a Random Walk\n\nCovariance: \\(cov(x_t,x_{t+k})\\):\nThe covariance between two values of the series depends on ( t ):\n\\[\n\\text{cov}(x_t, x_{t+k}) = t \\sigma^2\n\\]\nCorrelation Function \\(\\rho_k\\):\nThe correlation for lag  k  is:\n\\[\n\\rho_k = \\frac{1}{\\sqrt{1 + \\frac{k}{t}}}\n\\]\nNon-Stationarity:\nThe variance of the series increases with ( t ), making the random walk non-stationary.\nCorrelogram Characteristics:\nThe correlogram of a random walk typically shows:\n\nPositive autocorrelations starting near 1.\nA slow decrease as ( k ) increases.\n\n\n\n\n\nGaussian White Noise GWN 4.1\n\nIf the variables are normally distributed, i.e. \\(w_i \\sim N(0,\\sigma^2)\\), the DWN is called a Gaussian white noise process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n\n\nWhite Noise Time Series\n\nWhite Noise Time Series Lesson 4.1\n\nA white noise time series is a sequence of random variables that are uncorrelated and have a mean of zero.\nA white noise time series has a constant variance.\nA white noise time series has a constant mean.\nA white noise time series has a constant autocorrelation of zero for all lags except when the lag is zero.\n\n\n\n\nCorrelogram 4.1\n\nCorrelogram Lesson 4.1\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series.\nEach correlogram lag tests for correlation significance, increasing the chance of Type I error, resulting in potentially misleading conclusions about significant relationships.\n\n\n\n\nFitting the White Noise Model\n\n\nBackward Shift Operator\nWe define the backward shift operator or the lag operator, \\(\\mathbf{B}\\), as: \\[\n  \\mathbf{B} x_t = x_{t-1}\n\\] where \\(\\{x_t\\}\\) is any time series.\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\\[\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n\\]\n\nProperties of the Backshift Operator\nThe backwards shift operator is a linear operator. So, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants, then \\[\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n\\] The distributive property also holds. \\[\\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t\n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\\]\n.\n\n\n\nsearch words for lesson 4.1\nGaussian white noise - GWN - discrete white noise - dwn - variance - covariance - correlation - correlogram - Type I error - histogram - backward shift operator - backshift operator"
  },
  {
    "objectID": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "href": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.2 White Noise and Random Walks - Part 2",
    "text": "Lesson 4.2 White Noise and Random Walks - Part 2\n\nDifferecing a Time Series\nWhy do we difference a time series? Differencing a time series can help us to remove the trend and make the series stationary.\n\nComputing the difference between successive terms of a random walk leads to a discrete white noise series.\n\n\\[\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\\]\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences.\n\n\nCorrelograms & Histogram\nWhen do we use a correlogram and what do we look for?\nCorrelogram\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series. The ACF is a measure of the correlation between the time series and a lagged version of itself.\nNotice that the values in the correlogram of the stock prices start at 1 and slowly decay as k increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk.\n\nHistogram\n\nFigure 5 is a histogram of the differences. This is a simple measure of volatility of the stock, or in other words, how much the price changes in a day.\n\n\n\nDifference Operator\n\nDifferencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n\n\n\n\n\n\nDefinition of the Difference Operator\n\n\n\nThe difference operator, \\(\\nabla\\), is defined as:\n\\[\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t\\]\nHigher-order differencing can be denoted\n\\[\\nabla^n x_t = (1-\\mathbf{B})^n x_t\\]\n\n\nThings to do - Do excel workout and link to this. So in the website for this. This lesson will be nother tab. Maybe add the option to download the excel sheet.\nComputing Differences Small group acitivity The difference operator can be helpful in identifying the functional underpinnings of a trend. If a function is linear, then the first differences of equally-spaced values will be constant. If a function is quadratic, then the second differences of equally-spaced values will be constant. If a function is cubic, then the third differences of equally-spaced values will be constant, and so on.\ndifferencing Stock Prices do this group activity\nIntegrated Autoregressive Model do this group activity\nClass Activyt: Random Walk Drift do this class activity"
  },
  {
    "objectID": "index.html#lesson-5.1-unassigned-sections",
    "href": "index.html#lesson-5.1-unassigned-sections",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 5.1 unassigned sections",
    "text": "Lesson 5.1 unassigned sections\n\nGeneralized Least Squares (GLS)\n\nThe autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS – the indominable Generalized Least Squares algorithm!\nGeneralized Least Squares (GLS) Lesson 5.1\n\nGeneralized Least Squares (GLS) is a method for estimating the unknown parameters in a linear regression model.\nGLS is used when the errors in a regression model are correlated.\nGLS is used when the errors in a regression model are heteroskedastic.\nGLS is used when the errors in a regression model are autocorrelated.\nGLS is used when the errors in a regression model are non-normal.\n\n\n\n\nAdditive Seasonal Indicator Variables\n\nadditive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\n\\[\n  x_t = m_t + s_t + z_t\n\\]\nwhere \\[\n  s_t =\n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      ⋮~~~~ & ~~~~~~~~~~~~⋮ \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s\n    \\end{cases}\n\\] and \\(s\\) is the number of seasons in one cycle/period, and \\(n\\) is the number of observations, so \\(t = 1, 2, \\ldots, n\\) and \\(i = 1, 2, \\ldots, s\\), and \\(z_t\\) is the residual error series, which can be autocorrelated.\nIt is important to note that \\(m_t\\) does not need to be a constant. It can be a linear trend:\n\nSeasonal indicator variable\n\nlesson5.1\n\nWe will create a linear model that includes a constant term for each month. This constant monthly term is called a seasonal indicator variable.\nThis name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented.\nIndicator variables are also called dummy varaibles."
  },
  {
    "objectID": "chapters/chapter_5.html",
    "href": "chapters/chapter_5.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 5. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1.html",
    "href": "chapters/chapter_4_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4.html",
    "href": "chapters/chapter_4.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_2.html",
    "href": "chapters/chapter_4_lesson_2.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_5_lesson_1.html",
    "href": "chapters/chapter_5_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Applied Time Series Projects",
    "section": "",
    "text": "Applied Time Series Research\n\nChina Export Commodities Time Series Analysis:\n\nApplied time series techniques to analyze trends, seasonality, and irregularities in China’s export commodity data from 1990 to the present.\nConducted a classical multiplicative decomposition to distinguish long-term growth patterns, seasonal cycles, and random shocks influenced by macroeconomic events.\nProvided actionable insights into optimizing trade strategies by leveraging seasonal trends and mitigating risks from unpredictable disruptions.\n\n\n\n\nTime Series Reserach on China’s Export Commodities\n\n\nProject 2: Coming Soon\n\n\nproject 2\n\n\nProject 3: Coming Soon\n\n\nproject 3\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Applied Time Series Projects"
    ]
  },
  {
    "objectID": "projects/project2.html",
    "href": "projects/project2.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Project 2",
      "Project title here"
    ]
  }
]