[
  {
    "objectID": "tools_help_ideas.html",
    "href": "tools_help_ideas.html",
    "title": "Tools, Resources and Help Ideas",
    "section": "",
    "text": "This page is to include pontential ideas to do for resources, tools, and ideas for time series\n\n.yml code to fix\n\n_quarto.yml code. the sidebar code is different from time series and mine\n\ntime series sidebar does not appear on the side while my does. theme looks different\nI think it just has to do w/ the theme\n\nback-to-top-navigation: true * This is my original code and it works normally. * Time series uses this one: page-navigation: true\n\nWhen I use the time series I started getting this warning message\n\n[WARNING] Could not fetch resource ../projects/project2.html\nWhat is the difference between the two?\n\n\n\n\n\nStreamline code\nchapter_3_overview_code_draft.qmd is is streamline\n\nEdits to variables happen in first code chunk and qmd runs all code for that chapter.\nMay need to data frame set according to data\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tools, Help & Ideas",
      "Tools, Resources and Help Ideas"
    ]
  },
  {
    "objectID": "tools/math_formulas.html",
    "href": "tools/math_formulas.html",
    "title": "Formulas",
    "section": "",
    "text": "Autoregressive (AR) Model\n4.3 lesson (book reference 4.15)\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nExploring AR(1) Models Lesson 4.3.\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\nSecond-Order Properties of an AR(1) Model Lesson 4.3.\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\nCharacterisitc Equation\nLesson 4.3\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Math Formulas",
      "Formulas"
    ]
  },
  {
    "objectID": "projects/project2.html",
    "href": "projects/project2.html",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "",
    "text": "Part I. What Consumer Credit Data Are Available on the G.19 Statistical Release, “Consumer Credit,” and How Are These Data Calculated?\nThe G.19 Statistical Release, “Consumer Credit,” reports outstanding credit extended to individuals for household, family, and other personal expenditures, excluding loans secured by real estate. Total consumer credit comprises two major types: revolving and nonrevolving. Revolving credit plans may be unsecured or secured by collateral and allow a consumer to borrow up to a prearranged limit and repay the debt in one or more installments. Credit card loans comprise most of revolving consumer credit measured in the G.19, but other types, such as prearranged overdraft plans, are also included. Nonrevolving credit is closed-end credit extended to consumers that is repaid on a prearranged repayment schedule and may be secured or unsecured. To borrow additional funds, the consumer must enter into an additional contract with the lender. Consumer motor vehicle and education loans comprise the majority of nonrevolving credit, but other loan types, such as boat loans, recreational vehicle loans, and personal loans, are also included.\nThe G.19 also reports selected terms of credit, including interest rates on new car loans, personal loans, and credit card plans at commercial banks. Historically, the G.19 also included series that measure the terms of credit for motor vehicle loans at finance companies. In the first quarter of 2011, publication of these series was temporarily suspended because of the deterioration of their statistical foundation. The statistical foundation is in the process of being improved, and publication will resume as soon as possible.\n\n\nCode\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n    mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n   select(ym, OCC, OCC_MoM) |&gt;\n   mutate(OCC = OCC / 1000) |&gt;\n   slice_head(prop = 1) # tail: last 10%. head: first or oldest\n#   slice((n() * 0.5):(n() * 0.6)) # select from 50% to 60%\n# interval(df) # gives interval: M, D ot Y etc\n# has_gaps(df) # false if none and vice versa True\n\n\np_cdebt &lt;- df |&gt; # fig 1\n  autoplot(OCC) +\n  labs(\n    x = \"Month\",\n    y = \"OCC\",\n    title = \"Fig 1 - Time Series: OCC\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np_cdebt_tail &lt;- df |&gt; # fig 1.1\n  slice_tail(prop = .1) |&gt;\n  autoplot(OCC) +\n  labs(\n    x = \"Month\",\n    y = \"OCC\",\n    title = \"fig 1.1 Time Series: OCC 2017-Present\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\np_cdebt_change &lt;- df |&gt;  # fig 1.2\n  autoplot(OCC_MoM) +\n  labs(\n    x = \"Month\",\n    y = \"MoM OCC\",\n    title = \"Fig 1.2 - Time Series: OCC_MoM\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\np_cdebt | p_cdebt_tail\n\n\n\n\n\n\n\n\n\n\n\nCode\n# fig 1.2\np_cdebt_change\n\n\n\n\n\n\n\n\n\nFigure 1 shows the time series of the Outstanding consumer debt. Figure 1.1 shows the same time series but form 2017 to lattest data. Figure 1.3 is the month over month(MoM) percent change in Outstanding Consumer Credit.\n\nNoticible\n\nThe percent change has more volatility, specially in the early years of the time series, something that the time series plot does not catch due to the scale of the y axis.",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#notes",
    "href": "projects/project2.html#notes",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "Notes",
    "text": "Notes\nI was looking to see what can cause the spikes in the Random component, and maybe incentives to take out loans? I have data on student loans totals so I can see if there was an actual increase.\nai: April 2021: Biden requested that Education Secretary Miguel Cardona review his executive authority to cancel student loan debt unilaterally. This move suggested he was considering executive action, although no specific details or amounts were confirmed at that time.\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD # Exponential Smoothing\n\n\nCode\n# mirror to resource code 3.4.1 exponential smoothing\n\nfig_3_4_1_B1 &lt;- autoplot(df) +\n    labs(x = \"ym\", y = \"OCC\")\n\n\nPlot variable not specified, automatically selected `.vars = OCC`\n\n\nCode\ndf1 &lt;- df |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Additive = ETS(OCC ~\n        trend(\"M\", alpha = 0.7849282, beta = 0.1282016) +\n        error(\"M\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\nSSE_1 &lt;- sum(components(df1)$remainder^2, na.rm = T)\n\nfig_3_4_1_B2 &lt;- augment(df1) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndf2 &lt;- df |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Additive = ETS(OCC ~\n        trend(\"M\", alpha = 0.7849282, beta = 0.1282016) +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\nSSE_2 &lt;-  sum(components(df2)$remainder^2, na.rm = T)\n\nfig_3_4_1_B1\n\n\n\n\n\n\n\n\n\nCode\nSSE_1\n\n\n[1] 0.1694887\n\n\nCode\nfig_3_4_1_B2\n\n\n\n\n\n\n\n\n\nCode\nSSE_2\n\n\n[1] 115763.8\n\n\nai notes:\n\nEvaluating Model Fit The sum of squared residuals (SSE) is a common metric to evaluate how well the model fits the data. Lower SSE values indicate that the model’s predictions are close to the actual data points. In exponential smoothing models, you might compare SSEs across different model configurations (e.g., different smoothing parameters, different trend or seasonal components) to select the model that minimizes error.\nOptimization and Model Selection When fitting ETS models, you often need to optimize parameters such as smoothing constants ( 𝛼 α, 𝛽 β, and 𝛾 γ). The optimization criterion “amse” (average mean squared error) minimizes the mean squared error, indirectly influencing SSE. Comparing SSEs between models (df1 and df2 in your example) can help you determine if adjusting a parameter (e.g., 𝛼 α from 0.1429622 to 0.2) results in a better model fit.\nDiagnostics and Model Improvement SSE can be a diagnostic tool to assess whether your chosen model structure (e.g., additive trend and additive seasonality) is suitable for your data. High residual sums (or unexplained variance) may suggest that the model lacks a component (like an interaction between trend and seasonality) or is mis-specified.\nStatistical Properties of the Residuals For an ETS model to be considered a good fit, residuals should ideally be white noise—meaning they are uncorrelated, normally distributed, and have zero mean. Analyzing the sum of squared residuals, along with residual plots, autocorrelation, and partial autocorrelation functions, provides insights into whether the residuals meet these assumptions.\n\nIn short, the SSE (sum of squared residuals) is a key metric in exponential smoothing to measure and compare model performance, guide parameter optimization, and assess the statistical quality of the model’s fit to time series data.\nspacer\n\n\nCode\n# mirror to resource code 3.4.2 \n\ndf342 &lt;- df \n\n# autoplot(df342) +\n#     labs(\n#         x = \"ym\",\n#         y = \"OCC\")\ndf342_2 &lt;- df342 |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"M\") +\n        error(\"M\") +\n        season(\"M\"), \n        opt_crit = \"amse\", nmse = 1))\n# `report(df342_2)\n# tidy(df342_2)\n# sum(components(df342_2)$remainder^2, na.rm = T)\n# accuracy(df342_2)$RMSE\n# sd(df342$OCC)\n# autoplot(components(df342_2))\n# augment(df342_2) |&gt;\n#     ggplot(aes(x = ym, y = OCC)) +\n#     geom_line() +\n#     geom_line(aes(y = .fitted, color = \"Fitted\")) +\n#     labs(color = \"\")\n\nreport(df342_2)\n\n\nSeries: OCC \nModel: ETS(M,M,M) \n  Smoothing parameters:\n    alpha = 0.7719996 \n    beta  = 0.07304847 \n    gamma = 0.2274828 \n\n  Initial states:\n     l[0]     b[0]     s[0]    s[-1]     s[-2]     s[-3]     s[-4]    s[-5]\n 56.69474 0.919114 1.084212 1.148118 0.9343098 0.9815824 0.9831539 1.010902\n     s[-6]     s[-7]    s[-8]     s[-9]   s[-10]    s[-11]\n 0.9273472 0.8630946 1.243133 0.9753093 1.026207 0.8226304\n\n  sigma^2:  0.0012\n\n     AIC     AICc      BIC \n10223.21 10224.01 10302.48 \n\n\nI tried doing NO seasonality (N) for the Season component but I was running into an error.",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#code-takes-for-ever-to-run",
    "href": "projects/project2.html#code-takes-for-ever-to-run",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "3.4.3 code takes for ever to run!!!!!!!!!!!!!!!!!!",
    "text": "3.4.3 code takes for ever to run!!!!!!!!!!!!!!!!!!\n\n\nCode\n# mirror to resource code 3.4.3\n\ndf343_0 &lt;-  df |&gt;\n  slice_head(prop = .94)\n\ndf343 &lt;-  df343_0 |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\np_343 &lt;- augment(df343) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\np_343_forecast &lt;- df343 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343) |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2020 Nov\"))) +\n  scale_color_discrete(name = \"\")\n\np_343\n\n\n\n\n\n\n\n\n\nCode\np_343_forecast\n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using dygraph for a more interactive time series\n#works great, but it does not use as.tsibble and converts to xts in order to work with dygraphs. \n# I think xts will cause errors with my data, need to what differes xts to as.tsibble.\n\ndf343_0 &lt;- df |&gt;\n  slice_head(prop = .94)\n\n# Fit the model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~\n    trend(\"A\") +\n    error(\"A\") +\n    season(\"A\"),\n    opt_crit = \"amse\", nmse = 1))\n\n# Generate fitted values\nfitted_data &lt;- augment(df343)\n\n# Convert the data to xts format for dygraph\nocc_xts &lt;- xts(fitted_data$OCC, order.by = as.Date(fitted_data$ym))\nfitted_xts &lt;- xts(fitted_data$.fitted, order.by = as.Date(fitted_data$ym))\n\n# Create a dygraph with actual and fitted values\ndygraph(cbind(Actual = occ_xts, Fitted = fitted_xts)) %&gt;%\n  dySeries(\"Actual\", label = \"Actual\") %&gt;%\n  dySeries(\"Fitted\", label = \"Fitted\") %&gt;%\n  dyOptions(colors = c(\"blue\", \"red\")) %&gt;%\n  dyRangeSelector() %&gt;%\n  dyLegend(show = \"always\")\n\n\n\n\n\n\nspacer\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using plotly  for a more interactive time series, but this code did not look great but leaving here for sample\n\n\n# Load required libraries\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, plotly)\n\n# Import data and convert to tsibble\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n  mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n  select(ym, OCC, OCC_MoM)\n\n# Create the initial slice of the dataset\ndf343_0 &lt;- df |&gt; slice_head(prop = .94)\n\n# Fit the ETS model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~ trend(\"A\") + error(\"A\") + season(\"A\"), opt_crit = \"amse\", nmse = 1))\n\n# Create the first interactive plot using Plotly\np_343 &lt;- augment(df343) |&gt;\n  ggplot(aes(x = ym, y = OCC)) +\n  geom_line() +\n  geom_line(aes(y = .fitted, color = \"Fitted\")) +\n  labs(color = \"\")\n\n# Convert ggplot object to plotly for interactivity\np_343_plotly &lt;- ggplotly(p_343)\np_343_plotly\n\n# Forecast and create the second interactive plot using Plotly\np_343_forecast &lt;- df343 |&gt;\n  forecast(h = \"2 years\") |&gt;\n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343) |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2020 Nov\"))) +\n  scale_color_discrete(name = \"\")\n\n# Convert the forecast plot to plotly\np_343_forecast_plotly &lt;- ggplotly(p_343_forecast)\np_343_forecast_plotly\n\n\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using plotly  for a more interactive time series\n# it is almost good but I need the y scale to autornage according to the area the plot is zooming in. \n# Load required libraries\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, plotly)\n\n# Import data and convert to tsibble\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n  mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n  select(ym, OCC, OCC_MoM)\n\n# Create the initial slice of the dataset\ndf343_0 &lt;- df |&gt; slice_head(prop = .94)\n\n# Fit the ETS model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~ trend(\"A\") + error(\"A\") + season(\"A\"), opt_crit = \"amse\", nmse = 1))\n\n# Generate fitted values\nfitted_data &lt;- augment(df343)\n\n# Prepare the data for Plotly\ndf_plot &lt;- fitted_data |&gt;\n  mutate(Date = as.Date(ym))\n\n# Create an interactive Plotly plot for actual and fitted values with dynamic y-axis\np_343_plotly &lt;- plot_ly(df_plot, x = ~Date) %&gt;%\n  add_lines(y = ~OCC, name = \"Actual\", line = list(color = 'blue')) %&gt;%\n  add_lines(y = ~.fitted, name = \"Fitted\", line = list(color = 'red')) %&gt;%\n  layout(\n    title = \"Actual vs Fitted Values\",\n    xaxis = list(\n      title = \"Date\",\n      rangeslider = list(visible = TRUE)\n    ),\n    yaxis = list(\n      title = \"OCC\",\n      autorange = TRUE\n    ),\n    legend = list(orientation = \"h\", x = 0.1, y = -0.2)\n  )\n\n# Display the interactive plot with dynamic y-axis scaling\np_343_plotly",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#using-full-df-and-doing-24-26-forecast",
    "href": "projects/project2.html#using-full-df-and-doing-24-26-forecast",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "using full df and doing 24-26 forecast",
    "text": "using full df and doing 24-26 forecast\n\n\nCode\n# mirror to resource code 3.4.3 w/ 2024-2026 forecast\n\n\ndf343_1_0 &lt;-  df |&gt;\n  slice_head(prop = 1)\n\ndf343_2 &lt;-  df343_1_0 |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\np_343_2 &lt;- augment(df343_2) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\np_343_2_forecast &lt;- df343_2 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2021 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343_2) |&gt; filter(ym &gt;= yearmonth(\"2021 Jan\") & ym &lt;= yearmonth(\"2024 Aug\"))) +\n  scale_color_discrete(name = \"\")\n\n\np_343_2\n\n\n\n\n\n\n\n\n\nCode\np_343_2_forecast\n\n\n\n\n\n\n\n\n\nspacer\nwould the season but ‘M’ because it is not as volatile as the time series goes on. Like in the beginning OCC_MoM has a good amount of volatility, so the season is properly going down over time? * Okay I did cut the first 20 years of thge df so what will happen if I include the whole dataset.\n======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; 41d67e11b64162a043bcde7f5be9624fcaf874ad",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Applied Time Series Projects",
    "section": "",
    "text": "Applied Time Series Research\n\nChina Export Commodities Time Series Analysis:\n\nApplied time series techniques to analyze trends, seasonality, and irregularities in China’s export commodity data from 1990 to the present.\nConducted a classical multiplicative decomposition to distinguish long-term growth patterns, seasonal cycles, and random shocks influenced by macroeconomic events.\nProvided actionable insights into optimizing trade strategies by leveraging seasonal trends and mitigating risks from unpredictable disruptions.\n\n\n\n\nTime Series Reserach on China’s Export Commodities\n\n\nConsumer Credit Data Analysis:\n\nExamined consumer credit data from the G.19 Statistical Release, focusing on outstanding credit extended to individuals for household and personal expenditures.\nDifferentiated between revolving credit (e.g., credit card loans) and nonrevolving credit (e.g., motor vehicle and education loans) to identify distinct patterns and trends.\nAnalyzed the interest rates and terms of credit, including new car loans, personal loans, and credit card plans at commercial banks, and discussed the implications of historical data series.\nProvided insights into consumer borrowing behaviors and the potential impact of macroeconomic changes on credit markets.\n\n\n\nProject 2: Consumer Credit Data Analysis\n\n\nProject 3: Coming Soon\n\n\nproject 3\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Applied Time Series Projects"
    ]
  },
  {
    "objectID": "chapters/chapter_5_lesson_1_code_notes.html",
    "href": "chapters/chapter_5_lesson_1_code_notes.html",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "",
    "text": "transfer from chapter_5_lesson_1_notes.qmd old repo\nnotes taken during class\n\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n\nchocolate_month2 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    stats_time = year + (month - 1) / 12,\n    month_seq = 1:n()\n  ) |&gt;\n  mutate(month = factor(month)) |&gt;\n  as_tsibble(index = dates)\n\n# Fit regression model\nchocolate_lm &lt;- chocolate_month2 |&gt;\n  model(TSLM(chocolate ~ 0 + stats_time + month))\n\n# Estimated parameter values\nparam_est &lt;- chocolate_lm |&gt;\n  tidy() |&gt;\n  pull(estimate)\n\nparam_est\n\n [1]     1.131642 -2227.910370 -2219.304673 -2233.148977 -2233.443280\n [6] -2234.837584 -2237.931887 -2236.476190 -2237.370494 -2237.514797\n[11] -2232.659101 -2223.003404 -2200.097708\n\n\nwhen removing 0 + from the tslm function. it makes january as the mt\n\nchocolate_month2 |&gt; # &lt;- #1 rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    stats_time = year + (month - 1) / 12,\n    month_seq = 1:n()\n  ) |&gt; #2\n  mutate(month = factor(month)) |&gt; # factor converts numerical month into categorical levels. eg january = 1. so this parts gets us b1 jan + b2 feb ... + b12 dec. \n  as_tsibble(index = dates)\n\n# A tsibble: 240 x 7 [1M]\n   Month   chocolate    dates month  year stats_time month_seq\n   &lt;chr&gt;       &lt;int&gt;    &lt;mth&gt; &lt;fct&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;int&gt;\n 1 2004-01        36 2004 Jan 1      2004      2004          1\n 2 2004-02        45 2004 Feb 2      2004      2004.         2\n 3 2004-03        29 2004 Mar 3      2004      2004.         3\n 4 2004-04        32 2004 Apr 4      2004      2004.         4\n 5 2004-05        29 2004 May 5      2004      2004.         5\n 6 2004-06        26 2004 Jun 6      2004      2004.         6\n 7 2004-07        27 2004 Jul 7      2004      2004.         7\n 8 2004-08        27 2004 Aug 8      2004      2005.         8\n 9 2004-09        29 2004 Sep 9      2004      2005.         9\n10 2004-10        33 2004 Oct 10     2004      2005.        10\n# ℹ 230 more rows\n\n# Fit regression model\nchocolate_lm &lt;- chocolate_month2 |&gt;\n  model(TSLM(chocolate ~ stats_time + month))\n\n# Estimated parameter values\nparam_est &lt;- chocolate_lm |&gt;\n  tidy() |&gt;\n  pull(estimate)\n\nparam_est\n\n [1] -2227.910370     1.131642     8.605697    -5.238607    -5.532910\n [6]    -6.927214   -10.021517    -8.565821    -9.460124    -9.604428\n[11]    -4.748731     4.906965    27.812662\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Lesson 1 Notes",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapters/chapter_5.html",
    "href": "chapters/chapter_5.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 5. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4_code_notes.html",
    "href": "chapters/chapter_4_lesson_4_code_notes.html",
    "title": "Fitted AR Models",
    "section": "",
    "text": "transfer from chapter_4_lesson_4_notes.qmd old repo\nI belive this are notes taken from class.\n\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n# chunk number 2\npacf(temps_ts$change)\n\n\n\n\n\n\n\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n# A tibble: 7 × 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\nTo go over\ngo over the table fromt he Fitting Models (Dynamic Number of Parameters) exercise.\nwrite the ar model for this time series. * What does the table tell us. * estimeate * estimate * statistic * p.value\nKnow how to indentify if a model is stationary or not stationary. Moncayo’s r 6 model is not stationary because the of the .98, but the book model, I think the ar 4 model shows it is stationary. ([1] 1.011 1.755 1.453 1.453). Moncayo’s ar6 shows that it is not stationary. so the plot in the lesson, shows an increasing forecasted trend. Since the model says it is stationary, and the temps has been in an increasing trend since the 1980, the book model plot hasthe forecast coming back down to a mean of zero. This makes sense for a stiationary model because it has a mean of zero.\n\n\n\n Back to top",
    "crumbs": [
      "Lesson 4 Notes",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_3.html",
    "href": "chapters/chapter_4_lesson_3.html",
    "title": "Autoregressive (AR) Models",
    "section": "",
    "text": "Autoregressive (AR) Models Lesson 4.3\n\nAn AR is a linear regression model that uses lagged values of the time series to predict future values.\nAn AR is a stochastic process that uses a linear combination of past values of the time series to predict future values.\n\n\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\n\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\n\n\n\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do - DO group activity: Simulation of an AR(1) process\n\n\n\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\n\nHere is a partial autocorrelation plot for the McDonald’s stock price data:\n\n\nCode\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ncompany &lt;- \"McDonald's\"\n\n# Retrieve static file\nstock_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\n\nThe only significant partial correlation is at lag \\(k=1\\). This suggests that an \\(AR(1)\\) process could be used to model the McDonald’s stock prices.\n\n\n\nLook at lesson shinny code\n\n\n\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\nWe can use the polyroot function to find the roots of polynomials in R. For example, to find the roots of the polynomial \\(x^2-x-6\\), we apply the command\n\n\nCode\npolyroot(c(-6,-1,1))\n\n\n[1]  3+0i -2+0i\n\n\nNote the order of the coefficients. They are given in increasing order of the power of \\(x\\).\nOf course, we could simply factor the polynomial: \\[\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n\\] which implies that \\[\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n\\]\n\n\n\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\n\n\nWhat is an exponential smoothing model?\n\n\n\n\nexponential smoothing model - polyroot function -",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_3.html#lesson-4.3-autoregressive-ar-models",
    "href": "chapters/chapter_4_lesson_3.html#lesson-4.3-autoregressive-ar-models",
    "title": "Autoregressive (AR) Models",
    "section": "",
    "text": "Autoregressive (AR) Models Lesson 4.3\n\nAn AR is a linear regression model that uses lagged values of the time series to predict future values.\nAn AR is a stochastic process that uses a linear combination of past values of the time series to predict future values.\n\n\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\n\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\n\n\n\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do - DO group activity: Simulation of an AR(1) process\n\n\n\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\n\nHere is a partial autocorrelation plot for the McDonald’s stock price data:\n\n\nCode\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ncompany &lt;- \"McDonald's\"\n\n# Retrieve static file\nstock_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\n\nThe only significant partial correlation is at lag \\(k=1\\). This suggests that an \\(AR(1)\\) process could be used to model the McDonald’s stock prices.\n\n\n\nLook at lesson shinny code\n\n\n\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\nWe can use the polyroot function to find the roots of polynomials in R. For example, to find the roots of the polynomial \\(x^2-x-6\\), we apply the command\n\n\nCode\npolyroot(c(-6,-1,1))\n\n\n[1]  3+0i -2+0i\n\n\nNote the order of the coefficients. They are given in increasing order of the power of \\(x\\).\nOf course, we could simply factor the polynomial: \\[\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n\\] which implies that \\[\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n\\]\n\n\n\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\n\n\nWhat is an exponential smoothing model?\n\n\n\n\nexponential smoothing model - polyroot function -",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1_code_notes.html",
    "href": "chapters/chapter_4_lesson_1_code_notes.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "set.seed(1)\nn_points &lt;- 50  # Number of points to use in both parts\nsigma &lt;- 2 # Standard deviation for rnorm\naver &lt;- 0\n\n# Generate the tibble using n_points and sigma\nwd_gaussian &lt;- tibble(\n    index = 1:n_points,\n    y = rnorm(n_points, mean = aver, sd = sigma)\n) |&gt; \n  as_tsibble(index = index) |&gt;\n  mutate(\n  density = dnorm(y, mean = aver, sd = sigma))\n\n\n# Plot the first tibble\nwd_gaussian |&gt; \n    ggplot(aes(x = index, y = y)) + \n    geom_line() +\n    theme_bw() +\n    ggtitle(\"Generated White Noise Series\")\n\n\n\n\n\n\n\n# 2. Calculate and print the mean and variance\nsample_mean &lt;- mean(wd_gaussian$y)\nsample_variance &lt;- var(wd_gaussian$y)\ncat(\"Estimated Mean:\", sample_mean, \"\\n\")\n\nEstimated Mean: 0.2008966 \n\ncat(\"Estimated Variance:\", sample_variance, \"\\n\")\n\nEstimated Variance: 2.764863 \n\n# 3. Plot the Autocorrelation Function (ACF)\nacf_plot &lt;- wd_gaussian |&gt;\n  ACF(y, type = \"correlation\") |&gt;\n  autoplot() +\n  ggtitle(\"Autocorrelation Function of White Noise\") +\n  theme_bw()\n\n\n# Plot the histogram using the data from wd_gaussian\nhist_plot &lt;- wd_gaussian |&gt;\n    ggplot(aes(x = y)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"darkgrey\", bins = 10) +\n    geom_line(aes(x = y, y = density)) +\n    theme_bw() +\n  ggtitle(\"Histogram with Theoretical Normal Density Curve\")\n\n\nacf_plot\n\n\n\n\n\n\n\nhist_plot",
    "crumbs": [
      "Lesson 1 Notes",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1_code_notes.html#homework-4-question-3.a",
    "href": "chapters/chapter_4_lesson_1_code_notes.html#homework-4-question-3.a",
    "title": "White Noise and Random Walks - Part 1",
    "section": "homework 4 question 3.a",
    "text": "homework 4 question 3.a\nthis is sample code given by ai that uses an rexp function which we dont use but it does appear once in the book for one of the practice problems. I decided not to use it and show more of the math being done on the code\n\n#| include: false\nset.seed(42)\n\n# Parameters\nlambda &lt;- 1\nn &lt;- 500\n\n# Simulate white noise using the exponential distribution\ns_t &lt;- rexp(n, rate = lambda)\nw_t &lt;- s_t - lambda\n\n# Plot the simulation\nlibrary(ggplot2)\n\ndf &lt;- data.frame(Time = 1:n, White_Noise = w_t)\nggplot(df, aes(x = Time, y = White_Noise)) +\n  geom_line(color = \"blue\") +\n  theme_minimal() +\n  labs(\n    title = \"Simulated White Noise Process Using Exponential Distribution\",\n    x = \"Time\",\n    y = expression(w[t])\n  )",
    "crumbs": [
      "Lesson 1 Notes",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4.html",
    "href": "chapters/chapter_4.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "Code\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2, knitr,\n               kableExtra\n               )\n\n\nThis is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\nAdding this from homework_4_0 chapter notes.qmd file in old repo. This are notes intended to connect time series concepts\n\n\nCode\n# Load necessary library\n\n# Define the checklist table\nchecklist_data &lt;- data.frame(\n  Topic = c(\"Trend\", \"Seasonality\", \"Stationarity\", \"White Noise\", \"Serial Correlation\", \"Drift\"),\n  Description = c(\n    \"Presence of an upward or downward movement over time.\",\n    \"Repeating patterns at regular intervals.\",\n    \"Mean, variance, and autocorrelation are constant over time.\",\n    \"Residuals are independent, identically distributed with mean zero.\",\n    \"Past values are correlated with current values.\",\n    \"Consistent positive or negative change over time.\"\n  ),\n  White_Noise = c(\"\", \"\", \"✔\", \"✔\", \"\", \"\"),\n  Random_Walk = c(\"\", \"\", \"\", \"\", \"✔\", \"\"),\n  RW_with_Drift = c(\"\", \"\", \"\", \"\", \"✔\", \"✔\"),\n  AR_p = c(\"✔\", \"\", \"✔\", \"\", \"✔\", \"\"),\n  ARIMA = c(\"\", \"\", \"✔\", \"\", \"✔\", \"\"),\n  Holt_Winters = c(\"✔\", \"✔\", \"✔\", \"\", \"✔\", \"\")\n)\n\n# Render the table\nkable(checklist_data, format = \"html\", escape = F) %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nTopic\nDescription\nWhite_Noise\nRandom_Walk\nRW_with_Drift\nAR_p\nARIMA\nHolt_Winters\n\n\n\n\nTrend\nPresence of an upward or downward movement over time.\n\n\n\n✔\n\n✔\n\n\nSeasonality\nRepeating patterns at regular intervals.\n\n\n\n\n\n✔\n\n\nStationarity\nMean, variance, and autocorrelation are constant over time.\n✔\n\n\n✔\n✔\n✔\n\n\nWhite Noise\nResiduals are independent, identically distributed with mean zero.\n✔\n\n\n\n\n\n\n\nSerial Correlation\nPast values are correlated with current values.\n\n✔\n✔\n✔\n✔\n✔\n\n\nDrift\nConsistent positive or negative change over time.\n\n\n✔\n\n\n\n\n\n\n\n\n\n\n\nTime Series Model Checklist\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nDescription\nWhite Noise\nRandom Walk\nRW with Drift\nAR(p)\nARIMA\nHolt-Winters\n\n\n\n\nTrend\nPresence of an upward or downward movement over time.\n\n\n\n✔\n\n✔\n\n\nSeasonality\nRepeating patterns at regular intervals.\n\n\n\n\n\n✔\n\n\nStationarity\nMean, variance, and autocorrelation are constant over time.\n✔\n\n\n✔\n✔\n✔\n\n\nWhite Noise\nResiduals are independent, identically distributed with mean zero.\n✔\n\n\n\n\n\n\n\nSerial Correlation\nPast values are correlated with current values.\n\n✔\n✔\n✔\n✔\n✔\n\n\nDrift\nConsistent positive or negative change over time.\n\n\n✔\n\n\n\n\n\n\nspacer\n\n\nCode\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nCode\n# second r chunk\npacf(temps_ts$change)\n\n\n\n\n\n\n\n\n\nCode\n# 3rd r chunk\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n\n# A tibble: 7 × 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\nCode\n# 4th r chunk\nalphas &lt;- global_ar |&gt; coefficients() |&gt; tail(-1) |&gt; dplyr::select(estimate) |&gt; pull()\ncat(\n  \"0 = 1\", \n        \"- (\", alphas[1], \") * x\",\n        \"- (\", alphas[2], \") * x^2\",\n        \"- (\", alphas[3], \") * x^3\",\n        \"\\n     \",\n        \"- (\", alphas[4], \") * x^4\",\n        \"- (\", alphas[5], \") * x^5\",\n        \"- (\", alphas[6], \") * x^6\"\n)\n\n\n0 = 1 - ( 0.6559292 ) * x - ( -0.06617426 ) * x^2 - ( 0.140204 ) * x^3 \n      - ( 0.2653744 ) * x^4 - ( -0.1627911 ) * x^5 - ( 0.2056924 ) * x^6\n\n\nCode\nalphas\n\n\n[1]  0.65592918 -0.06617426  0.14020403  0.26537444 -0.16279106  0.20569242\n\n\nCode\n# 5th r chunk\n\ntemps_forecast &lt;- global_ar |&gt; forecast(h = \"50 years\")\ntemps_forecast |&gt;\n  autoplot(temps_ts, level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(global_ar)) +\n  scale_color_discrete(name = \"\") +\n  labs(\n    x = \"Year\",\n    y = \"Temperature Change (Celsius)\",\n    title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\"),\n    subtitle = paste0(\"50-Year Forecast Based on our AR(\", tidy(global_ar) |&gt; as_tibble() |&gt; dplyr::select(term) |&gt; tail(1) |&gt; stringr::str_sub(1), \") Model\")\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Import and prepare the data\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\n# Fit an AR model to the 'change' variable\nar_fit &lt;- ar(temps_ts$change, method = \"mle\", na.action = na.omit)\n\n# Extract and display the order and coefficients\norder &lt;- ar_fit$order\ncoefficients &lt;- ar_fit$ar\n\ncat(\"Order of the fitted AR model: \", order, \"\\n\")\n\n\nOrder of the fitted AR model:  4 \n\n\nCode\ncat(\"Coefficients of the AR model: \", coefficients, \"\\n\")\n\n\nCoefficients of the AR model:  0.6770191 -0.03582926 0.1560533 0.1929323 \n\n\nCode\n# Visualization of the original data\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_3_lesson_1.html",
    "href": "chapters/chapter_3_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_1.html",
    "href": "chapters/chapter_1.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_3.html",
    "href": "chapters/chapter_3.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_3_overview_code_draft.html",
    "href": "chapters/chapter_3_overview_code_draft.html",
    "title": "Chapter 3 r code examples and practice",
    "section": "",
    "text": "This qmd is made to summarize chapter 3 and have models for the chapter to view.\n\nI have a goal to make the code not be reliant on adjusting variable names and titles.\n\nThis will require having an initial r chunk that assigns variable names and dataset.\nI will still have multiple df.\n\nThis will be a bit hard and the models will have to be pretty general because some data sets required different approaches\n\nI think maybe I can also assign if its yearmonth, yearquater etc, but that will require so many if statements.\n\nIdk how I’m going to approach this\n\n\n\n\n# Assign your column names to variables\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n\ndates &lt;- \"date\"\nx_col &lt;- \"constructionequip_ord\"\ny_col &lt;- \"constructionequip_ship\"\n\n\n\n# Assign plot labels\nx_label &lt;- \"Month\"\ny_label &lt;- \"New Orders & Value of Equip\"\nplot_title &lt;- \"Time Series of Construction Equip: New Orders & Equipment\"\n\n\ndf &lt;- df0 |&gt;\n  mutate(\n    date = lubridate::mdy(.data[[dates]]),\n    x = as.numeric(.data[[x_col]]), # Convert and rename to x\n    y = as.numeric(.data[[y_col]])  # Convert and rename to y\n  ) |&gt;\n  select(date, x, y) \n\n\ndf1 &lt;- df |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(date, obs, x, y)\n\ndfx &lt;- df |&gt; # lone df for variable x = ord\n  mutate(obs = row_number()) |&gt; \n  select(date, x)\n\ndfy &lt;- df |&gt; # lone df for y = ship\n  mutate(obs = row_number()) |&gt; \n  select(date, y)\n\ntest\n\n# this is code for hw 3-1\n\n# this code is same as the dfx2 & dfy2 code\ndf1 &lt;- df1 |&gt;\n  mutate(index = tsibble::yearmonth(date)) |&gt; # 3.1\n  as_tsibble(index = index) |&gt;\n  select(index, date, x, y)\n\n\n\nautoplot(df1, .vars = x) +\n  geom_line(data = df1, aes(x = index, y = y), color = \"#E69F00\") +\n  labs(\n    x = x_label,\n    y = y_label,\n    title = plot_title\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "r code Models draft",
      "Chapter 3 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1.html",
    "href": "chapters/chapter_4_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_2.html",
    "href": "chapters/chapter_4_lesson_2.html",
    "title": "White Noise and Random Walks - Part 2",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4.html",
    "href": "chapters/chapter_4_lesson_4.html",
    "title": "Fitted AR Models",
    "section": "",
    "text": "Code\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4.html#lesson-4.4-fitted-ar-models",
    "href": "chapters/chapter_4_lesson_4.html#lesson-4.4-fitted-ar-models",
    "title": "Fitted AR Models",
    "section": "Lesson 4.4 Fitted AR Models",
    "text": "Lesson 4.4 Fitted AR Models\n\nSimulate an AR(1) Time Series\nIn this simulation, we first simulate data from the \\(AR(1)\\) model \\[\n  x_t = 0.75 ~ x_{t-1} + w_t\n\\] where \\(w_t\\) is a white noise process with variance 1.\n\n\nShow the code\nset.seed(123)\nn_rep &lt;- 1000\nalpha1 &lt;- 0.75\n\ndat_ts &lt;- tibble(w = rnorm(n_rep)) |&gt;\n  mutate(\n    index = 1:n(),\n    x = purrr::accumulate2(\n      lag(w), w, \n      \\(acc, nxt, w) alpha1 * acc + w,\n      .init = 0)[-1]) |&gt;\n  tsibble::as_tsibble(index = index)\n\ndat_ts |&gt; \n  autoplot(.vars = x) +\n    labs(\n      x = \"Time\",\n      y = \"Simulated Time Series\",\n      title = \"Simulated Values from an AR(1) Process\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nThe R command mean(dat_ts$x) gives the mean of the \\(x_t\\) values as 0.067.\n\n\nFit an \\(AR(1)\\) Model with Zero Mean\n\n\nShow the code\n# Fit the AR(1) model\nfit_ar &lt;- dat_ts |&gt;\n  model(AR(x ~ order(1)))\ntidy(fit_ar)\n\n\n# A tibble: 1 × 6\n  .model           term  estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 AR(x ~ order(1)) ar1      0.720    0.0220      32.8 2.07e-160\n\n\nThe estimate of the parameter \\(\\alpha_1\\) (i.e. the fitted value of the parameter \\(\\alpha_1\\)) is \\(\\hat \\alpha_1 = 0.72\\).\nWhen R fits an AR model, the mean of the time series is subtracted from the data before the parameter values are estimated. If R detects that the mean of the time series is not significantly different from zero, it is omitted from the output.\nBecause the mean is subtracted from the time series before the parameter values are estimated, R is using the model \\[\n  z_t = \\alpha_1 ~ z_{t-1} + w_t\n\\] where \\(z_t = x_t - \\mu\\) and \\(\\mu\\) is the mean of the time series.\nThings to do\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nAnswer the following questions with your partner.\n\nUse the expression for \\(z_t\\) above to solve for \\(x_t\\) in terms of \\(x_{t-1}\\), \\(\\mu\\), \\(\\alpha_1\\), and \\(w_t\\).\nWhat does your model reduce to when \\(\\mu = 0\\)?\nExplain to your partner why this correctly models a time series with mean \\(\\mu\\).\n\n\n\nWHat do I know\n\nFor what type of Time Series will a fitted AR model be use?\nIs the sample above a stationary or non-stationary time series?\nStochastic vs deterministic time series?\nWhat does this model tell us about the time series?\n\nWe replace the parameter \\(\\mu\\) with its estimator \\(\\hat \\mu = \\bar x\\). We also replace \\(\\alpha_1\\) with the fitted value from the output \\(\\hat \\alpha_1\\). This gives us the fitted model: \\[\n  \\hat x_t = \\bar x + \\hat \\alpha_1 ~ (x_{t-1} - \\bar x)\n\\]\nThe fitted model can be expressed as:\n\\[\\begin{align*}\n  \\hat x_t\n    &= 0.067 + 0.72 \\left( x_{t-1} - 0.067 \\right) \\\\\n    &= 0.067 - 0.72 ~ (0.067) + 0.72 ~ \\left( x_{t-1} \\right) \\\\\n    &= 0.019 + 0.72 ~ x_{t-1}\n\\end{align*}\\]\nEven though R does not report the parameter for the mean of the process, \\(\\hat \\mu = 0.019\\), it is not significantly different from zero. One could argue that we should not use a model that contains the mean and instead focus on a simple fitted model that has only one parameter:\n\\[\n  \\hat x_t = 0.72 ~ x_{t-1}\n\\]\n\n\nConfidence Interval for the Model Parameter\nThe P-value given above tests the hypothesis that \\(\\alpha_1=0\\). This is not helpful in this context. We are interested in the plausible values for \\(\\alpha_1\\), not whether or not it is different from zero. For this reason, we consider a confidence interval and disregard the P-value.\nWe can compute an approximate 95% confidence interval for \\(\\alpha_1\\) as: \\[\n  \\left(\n    \\hat \\alpha_1 - 2 \\cdot SE_{\\hat \\alpha_1}\n    , ~\n    \\hat \\alpha_1 + 2 \\cdot SE_{\\hat \\alpha_1}\n  \\right)\n\\] where \\(\\hat \\alpha_1\\) is our parameter estimate and \\(SE_{\\hat \\alpha_1}\\) is the standard error of the estimate. Both of these values are given in the R output.\n\n\nShow the code\nci_summary &lt;- tidy(fit_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\n\n\nSo, our 95% confidence interval for \\(\\alpha_1\\) is: \\[\n  \\left(\n  0.72 - 2 \\cdot 0.022\n  , ~\n  0.72 + 2 \\cdot 0.022\n  \\right)\n\\] or \\[\n  \\left(\n  0.676\n  , ~\n  0.764\n  \\right)\n\\] Note that the confidence interval contains \\(\\alpha_1 = 0.75\\), the value of the parameter we used in our simulation. The process of estimating the parameter worked well. In practice, we will not know the value of \\(\\alpha_1\\), but the confidence interval gives us a reasonable estimate of the value.\n\n\nResiduals\nFor an \\(AR(1)\\) model where the mean of the time series is not statistically significantly different from 0, the residuals are computed as \\[\\begin{align*}\n  r_t\n    &= x_t - \\hat x_t \\\\\n    &= x_t - \\left[ 0.72 ~ x_{t-1} \\right]\n\\end{align*}\\]\n\n\nCode\n# had include false, and eval false.\n\n\n# Computing the residuals manually\ndat_ts |&gt;\n  # Zero mean model\n  mutate(resid0 = x - ( (tidy(fit_ar) |&gt; select(estimate) |&gt; pull()) * lag(x) ) ) |&gt;\n  # Non-zero mean model\n  mutate(resid1 = x - (mean(x) + (tidy(fit_ar) |&gt; select(estimate) |&gt; pull()) * (lag(x) - mean(x)) ) )\n\n\n# A tsibble: 1,000 x 5 [1]\n         w index      x resid0  resid1\n     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.560      1 -0.560 NA     NA     \n 2 -0.230      2 -0.651 -0.247 -0.266 \n 3  1.56       3  1.07   1.54   1.52  \n 4  0.0705     4  0.874  0.103  0.0842\n 5  0.129      5  0.784  0.156  0.137 \n 6  1.72       6  2.30   1.74   1.72  \n 7  0.461      7  2.19   0.531  0.512 \n 8 -1.27       8  0.376 -1.20  -1.22  \n 9 -0.687      9 -0.405 -0.675 -0.694 \n10 -0.446     10 -0.749 -0.458 -0.477 \n# ℹ 990 more rows\n\n\nWe can easily obtain these residual values in R:\n\n\nShow the code\nfit_ar |&gt; residuals()\n\n\n# A tsibble: 1,000 x 3 [1]\n# Key:       .model [1]\n   .model           index .resid\n   &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n 1 AR(x ~ order(1))     1 NA    \n 2 AR(x ~ order(1))     2 -0.247\n 3 AR(x ~ order(1))     3  1.54 \n 4 AR(x ~ order(1))     4  0.103\n 5 AR(x ~ order(1))     5  0.156\n 6 AR(x ~ order(1))     6  1.74 \n 7 AR(x ~ order(1))     7  0.531\n 8 AR(x ~ order(1))     8 -1.20 \n 9 AR(x ~ order(1))     9 -0.675\n10 AR(x ~ order(1))    10 -0.458\n# ℹ 990 more rows\n\n\nThe variance of the residuals is \\(0.982\\). This is very close to the actual value used in the simulation: \\(\\sigma^2 = 1\\).\n\n\n\n\n\n\nFitting a Simulated AR(1) Part 1\nFitting a Simulated AR(1) Model with Non-Zero Mean\n\nFit an AR(1) Model with Non-Zero Mean\nSame as above? I guess in this one we use R to fit an AR(1) model to the time series data.\n\n\nConfidence Interval for the Model Parameters\n\nWe can compute approximate 95% confidence intervals for \\(\\alpha_0\\) and \\(\\alpha_1\\):\n\nI need to review this\n\nReview the two columns\n\n\n\nResiduals\nLooks like this sections is one full that can be summarize in one for fitting a simulated AR(1) model with non-zero mean???\n\n\n\nRepeat Part 1: Class Activity\n\nThis section is a class activity but we just repeat part 1 (above section)\n\n\n\nFitting a Simulated AR(2) Model\nSeems to be same as part 1 above, definetly need to understand what this code is doing before I tried solving it.\n\nFit an AR(2) Model\n\n\nConfidence Interval for the Model Parameters\n\n\nResiduals\n\nWe can compute the residuals in the same manner as we did for the other models.\n\n\n\n\nActivity: Global Warming\n\nUsing the PACF to Choose p for an AR(p) Process\n\nIn the previous lesson, we noted that the partial correlogram can be used to assess the number of parameters in an AR model. Here is a partial correlogram for the change in the mean annual global temperature.\n\n\n\nFitting Models (Dynamic Number of Parameters)\n\n\nStationary of the AR(p) Model\n\n\n\nForecasting with an AR(p) Model\nClas Activity: Forecasting with an AR(p) Model\n\nComparison to results in Section 4.6.3 of the Book\n\nClass Activity: Comparison to the Results in Section 4.6.3 of the Book (5 min)\n\n\n\n\nsearch for words for lesson 4.4\nconceive - white noise process - variance - estimate for the parameter for the constant -",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_overview_code_draft.html",
    "href": "chapters/chapter_4_overview_code_draft.html",
    "title": "Chapter 4 r code examples and practice",
    "section": "",
    "text": "This chapter is not finished.",
    "crumbs": [
      "r code Models draft",
      "Chapter 4 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_4_overview_code_draft.html#visualizing-white-noise",
    "href": "chapters/chapter_4_overview_code_draft.html#visualizing-white-noise",
    "title": "Chapter 4 r code examples and practice",
    "section": "Visualizing White Noise",
    "text": "Visualizing White Noise\nspacer\n\n# 1\n\n\nfile_path &lt;- \"../data/white_noise.parquet\"\n\n# ../data goes back one level. ../../data goes back two folders\n\n\n# straight from lesson code\n# This code was used to create the white noise data file\n\n# Set random seed\nset.seed(10)\n\n# Specify means and standard deviation\nn &lt;- 2500                           # number of points\nwhite_noise_sigma &lt;- rnorm(1, 5, 1) # choose a random standard deviation\n\n# Simulate normal data\n# data.frame(x = rnorm(n, 0, white_noise_sigma)) |&gt;\n#   rio::export(\"../data/white_noise.parquettest\") # uncomment this two lines to run\n\n\n# 2\n\n\n# White noise data\ndf0 &lt;- rio::import(file_path)\n\nname changes\nchunk 3: white_noise_df = df0\nchunk 4: x = v1\n\n# 3\n# words b/ code: The first 250 points in this time....\n\ndf1 &lt;- df0 |&gt; # this code updates names, but now t = x. and x is y in latter codes so keep that in mind.\n  mutate(t = 1:nrow(df0)) |&gt;\n  rename(x = t, v1 = x) # rename this, original code had those names for x and y. now just x and y\n\ndf1 |&gt; # original, but this code does not rename or change df. \n  head(250) |&gt;  \n  ggplot(aes(x = x, y = v1)) + \n    geom_line() +\n    theme_bw() +\n    labs(\n      x = \"Time\",\n      y = \"Values\",\n      title = \"First 250 Values of a Gaussian White Noise Time Series\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n  acf(df1$v1, type = \"covariance\") # doing this in class to get to the density plot below\n\n\n\n\n\n\n\n  acf(df1$v1, type = \"correlation\") # use this acf samples from previous lesson. \n\n\n\n\n\n\n\n\nspacer\n\n# 4.1 - 4\n# words before code: Here is a histogram of the 2500 values from....\n\n# this x is the variable, but not necessarily the x axis variable. x is y here \ndf1 |&gt;\n  mutate(density = dnorm(v1, mean(df1$v1), sd(df1$v1))) |&gt;\n  ggplot(aes(x = v1)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n    geom_line(aes(x = v1, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values or variable 1\",\n      y = \"Frequency\",\n      title = \"Histogram of Values from a Gaussian White Noise Process\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n# left off in this histogram code because the differences between the y and x names is confusing. I need to just hard edit the code to clearly define the variable and avoid the name changes. I just need to make sure I take note of it.\n\nRandom Walk Cumulative Sum\nname changes\ny = v2\nthe x in this code refers to the x - axis values. This code is meant for when the x axis values are number of observations (eg. 1-60). Dates can maybe work, but anything else can cause troubles.\n\n# 4.1 - 5\n# sample code to simulate a random walk\n# words b/ code: Complete steps 2 and 3 a total of\n\n# set.seed(7)\n\ndf2 &lt;- df1 |&gt;\n  # mutate(w = ifelse(row_number() == 1, 0, sample(c(-1,1), size = 60, replace = TRUE))) |&gt; # generates coin flips, but no longer needed for chapter model\n  mutate(v2 = cumsum(v1)) # creates cumulitve v2 column\n\nggplot(data=df2, aes(x=x, y=v2)) +\n  # geom_point(data = df2, aes(x=x, y=v2), size = 0.01) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  # scale_x_continuous(limits = c(0,60), # limits to only 60 obs\n  #                    breaks = seq(0, 60, by = 5),\n  #                    minor_breaks = seq(0, 60, 1)) +\n  # scale_y_continuous(limits = c(-20,20),\n  #                    breaks = seq(-20, 20, by = 5),\n  #                    minor_breaks = seq(-20, 20, 1)) +\n  labs(\n      x = \"Toss Number\",\n      y = expression(paste(\"$x_t$\")),\n      title = \"Cumulative Results of Coin Tosses\" # cum results (v2) of v1. \n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(colour = \"black\")\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nspacer\n\n# 4.1 - 6 \n# words b/ code: be a time series with the following values.\n\nset.seed(6)\nn &lt;- 8\nd_operator &lt;- data.frame(t = c(1:n), x = sample(1:15, n, replace = FALSE)) |&gt;\n  mutate(diff = t - n) # what is this code doing. right now is its doing t - n. n is always the set 8, and t is just the number of observations. so if n is 8, then the first t- n is -7, second is -6 and so on, but this is just using the number/date assigned to the actual variable. its like comparing hot days when only using the data, but not the temperature. so what is this code doing exactly????\n\n#cat( paste( paste0(\"$x_{t\", ifelse(d_operator$t==n,\"\",d_operator$t-n), \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\ncat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\n# Computes the value of the \"power_on_d\"^th difference from x_n\nd_value &lt;- function(power_on_d = 0) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(diff == -power_on_d) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n\nts_val &lt;- function(t_value) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(t == t_value) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n# this code below was the last r chunk for lesson 4-1, but it is not needed since it is in this r chunk. This r chunk is set not to evaluate for class. \n\n#cat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) )\n\nspacer\nThis is solution to backwards shift operator so formula for this code is done in a previous r chunk.",
    "crumbs": [
      "r code Models draft",
      "Chapter 4 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_5_lesson_1.html",
    "href": "chapters/chapter_5_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Time Series Analysis Notebook",
    "section": "",
    "text": "I’m first going to work out this index file, to have the whole picture then I will branch out base on information here. As I branch out information here will be compress and have links to the lessons and samples."
  },
  {
    "objectID": "index.html#stationary-time-series",
    "href": "index.html#stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Stationary Time Series",
    "text": "Stationary Time Series\n\nSationarity of Linear Models Lesson 5.1\n\nLinear models for time series are non-stationary when they include functions of time.\n\nDifferencing can often transform a non-stationary series with a de terministic trend to a stationary series (4.1).\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences (4.2)."
  },
  {
    "objectID": "index.html#non-stationary-time-series",
    "href": "index.html#non-stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Non-Stationary Time Series",
    "text": "Non-Stationary Time Series\n\nNon-Stationary Time Series Lesson 5.2\n\nA time series with a stochastic trend is non-stationary.\nA time series with a deterministic trend is non-stationary.\nA time series with a seasonal component is non-stationary.\nA time series with a unit root is non-stationary."
  },
  {
    "objectID": "index.html#lesson-4.1-white-noise-and-random-walks",
    "href": "index.html#lesson-4.1-white-noise-and-random-walks",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.1 White Noise and Random Walks",
    "text": "Lesson 4.1 White Noise and Random Walks\n\nStochastic Process 4.1\nStochastic processes or random processes evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\n\nDiscrete White Noise DWN 4.1\n\n4.2: Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\nA time series \\(\\{w_t: t = 1, 2, \\ldots, n\\}\\) is a discrete white noise (DWN) if the variables \\(w_1, w_2, \\ldots, w_n\\) are independent and identically distributed with mean 0.\n\nSecond-Order Properties of DWN\n\nWhen we refer to the second-order properties of a time series, we are talking about its variance and covariance. The variance of a DWN is constant, and the covariance between any two observations is zero\nThe mean is a first-order property, the covariance is a second-order property.\n\n\n\nDiscrete White Noise Process\n\nA DWN process will have the following properties:\n\nThere is a discrete observations.\nThe mean of the observations is zero.\nThe variance of the observations is finite.\nSuccessive observations are uncorrelated.\n\n\n\n\n\nRandom Walk\n\nRandom Walks Lesson 4.1\n\nA random walk is a stochastic process in which the difference between each observation is a white noise process, a non-stationary time series. (see def)\nwt is a dwn and often model as gwn, however wt could be as simple as a coin toss (random walk).\n\n\n\nProperties of Random Walk or walks\nor First-Order Properties of A Random Walk - The mean of a random walk series is 0.\nLook at shinny code for this\n\n\nSecond-Order Properties of a Random Walk\n\nCovariance: \\(cov(x_t,x_{t+k})\\):\nThe covariance between two values of the series depends on ( t ):\n\\[\n\\text{cov}(x_t, x_{t+k}) = t \\sigma^2\n\\]\nCorrelation Function \\(\\rho_k\\):\nThe correlation for lag  k  is:\n\\[\n\\rho_k = \\frac{1}{\\sqrt{1 + \\frac{k}{t}}}\n\\]\nNon-Stationarity:\nThe variance of the series increases with ( t ), making the random walk non-stationary.\nCorrelogram Characteristics:\nThe correlogram of a random walk typically shows:\n\nPositive autocorrelations starting near 1.\nA slow decrease as ( k ) increases.\n\n\n\n\n\nGaussian White Noise GWN 4.1\n\nIf the variables are normally distributed, i.e. \\(w_i \\sim N(0,\\sigma^2)\\), the DWN is called a Gaussian white noise process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n\n\nWhite Noise Time Series\n\nWhite Noise Time Series Lesson 4.1\n\nA white noise time series is a sequence of random variables that are uncorrelated and have a mean of zero.\nA white noise time series has a constant variance.\nA white noise time series has a constant mean.\nA white noise time series has a constant autocorrelation of zero for all lags except when the lag is zero.\n\n\n\n\nCorrelogram 4.1\n\nCorrelogram Lesson 4.1\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series.\nEach correlogram lag tests for correlation significance, increasing the chance of Type I error, resulting in potentially misleading conclusions about significant relationships.\n\n\n\n\nFitting the White Noise Model\n\n\nBackward Shift Operator\nWe define the backward shift operator or the lag operator, \\(\\mathbf{B}\\), as: \\[\n  \\mathbf{B} x_t = x_{t-1}\n\\] where \\(\\{x_t\\}\\) is any time series.\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\\[\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n\\]\n\nProperties of the Backshift Operator\nThe backwards shift operator is a linear operator. So, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants, then \\[\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n\\] The distributive property also holds. \\[\\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t\n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\\]\n.\n\n\n\nsearch words for lesson 4.1\nGaussian white noise - GWN - discrete white noise - dwn - variance - covariance - correlation - correlogram - Type I error - histogram - backward shift operator - backshift operator"
  },
  {
    "objectID": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "href": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.2 White Noise and Random Walks - Part 2",
    "text": "Lesson 4.2 White Noise and Random Walks - Part 2\n\nDifferecing a Time Series\nWhy do we difference a time series? Differencing a time series can help us to remove the trend and make the series stationary.\n\nComputing the difference between successive terms of a random walk leads to a discrete white noise series.\n\n\\[\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\\]\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences.\n\n\nCorrelograms & Histogram\nWhen do we use a correlogram and what do we look for?\nCorrelogram\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series. The ACF is a measure of the correlation between the time series and a lagged version of itself.\nNotice that the values in the correlogram of the stock prices start at 1 and slowly decay as k increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk.\n\nHistogram\n\nFigure 5 is a histogram of the differences. This is a simple measure of volatility of the stock, or in other words, how much the price changes in a day.\n\n\n\nDifference Operator\n\nDifferencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n\n\n\n\n\n\nDefinition of the Difference Operator\n\n\n\nThe difference operator, \\(\\nabla\\), is defined as:\n\\[\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t\\]\nHigher-order differencing can be denoted\n\\[\\nabla^n x_t = (1-\\mathbf{B})^n x_t\\]\n\n\nThings to do - Do excel workout and link to this. So in the website for this. This lesson will be nother tab. Maybe add the option to download the excel sheet.\nComputing Differences Small group acitivity The difference operator can be helpful in identifying the functional underpinnings of a trend. If a function is linear, then the first differences of equally-spaced values will be constant. If a function is quadratic, then the second differences of equally-spaced values will be constant. If a function is cubic, then the third differences of equally-spaced values will be constant, and so on.\ndifferencing Stock Prices do this group activity\nIntegrated Autoregressive Model do this group activity\nClass Activyt: Random Walk Drift do this class activity"
  },
  {
    "objectID": "index.html#lesson-4.3-autoregressive-ar-models",
    "href": "index.html#lesson-4.3-autoregressive-ar-models",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.3 Autoregressive (AR) Models",
    "text": "Lesson 4.3 Autoregressive (AR) Models\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\n\nProperties of an AR(p) Stochastic Process\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\nExploring AR(1) Models\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\nSecond-Order Properties of an AR(1) Model\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\nCorrelogram of an AR(1) Model\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do\n\nDO group activity: Simulation of an AR(1) process\n\n\n\nPartial Autocorrelation\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\nPartial Autocorrelation Plots of Various AR(p) Processes\nLook at lesson shinny code\n\n\nSationary and Non-Stationary AR Processes\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\n\n\nAbsolute Value in the Complex Plane\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\nQuestions\n\nWhat is an exponential smoothing model?\n\n\n\nSearch for words for lesson 4.3\nexponential smoothing model - polyroot function -"
  },
  {
    "objectID": "index.html#lesson-5.1-unassigned-sections",
    "href": "index.html#lesson-5.1-unassigned-sections",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 5.1 unassigned sections",
    "text": "Lesson 5.1 unassigned sections\n\nGeneralized Least Squares (GLS)\n\nThe autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS – the indominable Generalized Least Squares algorithm!\nGeneralized Least Squares (GLS) Lesson 5.1\n\nGeneralized Least Squares (GLS) is a method for estimating the unknown parameters in a linear regression model.\nGLS is used when the errors in a regression model are correlated.\nGLS is used when the errors in a regression model are heteroskedastic.\nGLS is used when the errors in a regression model are autocorrelated.\nGLS is used when the errors in a regression model are non-normal.\n\n\n\n\nAdditive Seasonal Indicator Variables\n\nadditive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\n\\[\n  x_t = m_t + s_t + z_t\n\\]\nwhere \\[\n  s_t =\n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      ⋮~~~~ & ~~~~~~~~~~~~⋮ \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s\n    \\end{cases}\n\\] and \\(s\\) is the number of seasons in one cycle/period, and \\(n\\) is the number of observations, so \\(t = 1, 2, \\ldots, n\\) and \\(i = 1, 2, \\ldots, s\\), and \\(z_t\\) is the residual error series, which can be autocorrelated.\nIt is important to note that \\(m_t\\) does not need to be a constant. It can be a linear trend:\n\nSeasonal indicator variable\n\nlesson5.1\n\nWe will create a linear model that includes a constant term for each month. This constant monthly term is called a seasonal indicator variable.\nThis name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented.\nIndicator variables are also called dummy varaibles."
  },
  {
    "objectID": "projects/project1.html",
    "href": "projects/project1.html",
    "title": "Time Series: China Export Commodities",
    "section": "",
    "text": "In today’s global economy, analyzing time series data is essential for predicting trends, understanding seasonality, and identifying unexpected shocks. The objective of this analysis is to explore China’s export commodities over time, using time series analysis techniques to gain insights that can aid in strategic planning and effective decision-making.\n\n\n\nTime series analysis involves collecting and interpreting data recorded over time. By examining China’s export data from early 1990 to the present, we can observe trends, seasonality, and irregular movements that are crucial for understanding the dynamics of international trade.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#introduction",
    "href": "projects/project1.html#introduction",
    "title": "Time Series: China Export Commodities",
    "section": "",
    "text": "In today’s global economy, analyzing time series data is essential for predicting trends, understanding seasonality, and identifying unexpected shocks. The objective of this analysis is to explore China’s export commodities over time, using time series analysis techniques to gain insights that can aid in strategic planning and effective decision-making.\n\n\n\nTime series analysis involves collecting and interpreting data recorded over time. By examining China’s export data from early 1990 to the present, we can observe trends, seasonality, and irregular movements that are crucial for understanding the dynamics of international trade.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#data-preparation",
    "href": "projects/project1.html#data-preparation",
    "title": "Time Series: China Export Commodities",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nShow the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, data.table, plotly)\n\n# Data Import and Preparation\ncexports_ts &lt;- rio::import(\"../data/CHNXTEXVA01NCMLM.csv\") |&gt; \n  mutate(\n    CHNXTEXVA01NCMLM = as.numeric(gsub(\",\", \"\", CHNXTEXVA01NCMLM)), # Remove commas and convert to numeric\n    dates = mdy(date),\n    year = lubridate::year(dates), \n    month = lubridate::month(dates), \n    value = as.numeric(CHNXTEXVA01NCMLM)\n  ) |&gt;\n  dplyr::select(dates, year, month, value)  |&gt;\n  arrange(dates) |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index) |&gt;\n  dplyr::select(index, dates, year, month, value) |&gt;\n  rename(exports_commodities = value) |&gt;\n  mutate(exports_commodities = exports_commodities / 1000)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#chinas-export-trade-commodities-time-series-plot",
    "href": "projects/project1.html#chinas-export-trade-commodities-time-series-plot",
    "title": "Time Series: China Export Commodities",
    "section": "China’s Export Trade Commodities Time Series Plot",
    "text": "China’s Export Trade Commodities Time Series Plot\n\n\nShow the code\nplain_plot &lt;- ggplot(cexports_ts, aes(x = dates, y = exports_commodities)) +\n  geom_rect(aes(xmin = as.Date(\"2008-01-01\"), xmax = as.Date(\"2010-01-01\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_rect(aes(xmin = as.Date(\"2020-02-01\"), xmax = as.Date(\"2020-04-01\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_line() +\n  geom_rect(aes(xmin = as.Date(\"2015-04-10\"), xmax = as.Date(\"2015-04-28\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_line() +\n  labs(x = \"\", y = \"Exports Commodities (Billions)\", title = \"Fig 1 - China: Exports Commodities Time Series\") +\n  scale_y_continuous(limits = range(cexports_ts$exports_commodities, na.rm = TRUE)) +\n    scale_x_yearmonth(\n    labels = scales::label_date(\"%Y %b\"),  # Format labels as \"Jan 2020\"\n    breaks = \"36 months\"                    # Show labels every 8 months\n  ) + \n  theme(plot.title = element_text(hjust = 0.5))\n\nplain_plot\n\n\n\n\n\nFig 1 - China’s Exports Commodities Time Series\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig1_exports_commodities_plot.png\", plot = plain_plot, width = 12, height = 6, dpi = 300)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#time-series-decomposition",
    "href": "projects/project1.html#time-series-decomposition",
    "title": "Time Series: China Export Commodities",
    "section": "Time Series Decomposition",
    "text": "Time Series Decomposition\n\nUnderstanding Decomposition\nTime series decomposition breaks down data into three components: trend, seasonality, and irregularities. This helps us understand underlying patterns and make better forecasts.\n\n\nWhy Use a Multiplicative Decomposition?\nThe upward trend in China’s export data suggests that fluctuations in seasonality and irregularities increase in magnitude as exports grow. That’s why we use a multiplicative classical decomposition model, where the data is broken down into:\n\nTrend Component: Captures the long-term growth pattern. Here, we observe steady growth in exports.\nSeasonal Component: Identifies repeating patterns within a year. Exports peak in December (possibly driven by holiday demand, especially in the US) and dip to their lowest in February (likely due to the Chinese New Year and post-holiday slowdowns).\nRandom Component: Captures what’s left after accounting for the trend and seasonal components. It represents what the model cannot explain. These unexpected changes often correlate with macroeconomic events like the 2008 Global Financial Crisis, but not all irregular variations can be directly linked to specific events.\n\n\n\nMultiplicative Classical Decomposition Model\n\n\nShow the code\nchina_decompose_mult &lt;- cexports_ts |&gt;\n  model(feasts::classical_decomposition(exports_commodities, type = \"mult\"))  |&gt;\n  components()\n\ndecompose_plot &lt;- autoplot(china_decompose_mult) +\n  ggtitle(\"Fig 2 - Classical Multiplicative Decomposition of China's Exports\") +\n  labs(subtitle = \"Export commodities = trend (billions) * seasonal * random\")\n\ndecompose_plot\n\n\n\n\n\nFig 2 - Classical Multiplicative Decomposition of China’s Exports\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig2_mult_decomposition.png\", plot = decompose_plot, width = 12, height = 6, dpi = 300)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#seasonal-component",
    "href": "projects/project1.html#seasonal-component",
    "title": "Time Series: China Export Commodities",
    "section": "Seasonal Component",
    "text": "Seasonal Component\n\n\nShow the code\nplot_decomp_seasonal &lt;- ggplot(china_decompose_mult %&gt;% filter(index &gt;= yearmonth('2019 Dec') & index &lt;= yearmonth('2021 Jan')), aes(x = index, y = seasonal)) +\n  geom_line(group = 1, color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Fig 3 - China: Exports Commodities Seasonal Component\",\n       x = \"Month\",\n       y = \"Seasonal\") +\n  scale_x_yearmonth(\n    labels = scales::label_date(\"%b\"),  \n    breaks = \"1 months\"                    \n  )  + \n  theme(plot.title = element_text(hjust = 0.5))\n\nplot_decomp_seasonal\n\n\n\n\n\nFig 3 - China’s Exports Commodities Seasonal Component\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig3_seasonal_component.png\", plot = plot_decomp_seasonal, width = 12, height = 6, dpi = 300)\n\n\n\nRandom Component and Macroeconomic Events\nThese unexpected changes often correlate with macroeconomic events like the 2008 Global Financial Crisis. For example, in response to a slowdown (random shock), China increased export tax rebates in March 2015 to stimulate exports, showing how responses can occur after the initial shock. This randomness highlights inherent uncertainties—macroeconomic disruptions may explain some shocks, but many remain unpredictable. Understanding the random component helps us see where patterns could align—but also where uncertainties remain.\n\n\nShow the code\nhighlight_months &lt;- c(\"1992 Oct\", \"1994 Jan\", \"1994 Apr\", \"1994 Sep\", \n                      \"1995 Dec\", \"1997 Jul\", \"1998 Jan\", \"1999 Apr\", \n                      \"1999 Nov\", \"2001 Dec\", \"2003 Apr\", \"2004 Jul\", \n                      \"2005 Jul\", \"2008 Sep\", \"2008 Nov\", \"2009 Jun\", \n                      \"2010 May\", \"2010 Dec\", \"2015 Mar\", \"2017 Jan\", \n                      \"2018 Jul\", \"2019 Oct\", \"2019 Dec\", \"2020 Jan\", \n                      \"2020 Mar\")\n\nhighlight_dates &lt;- tsibble::yearmonth(highlight_months)\n\nplot_random_component &lt;- ggplot(china_decompose_mult %&gt;% \n                                  filter(index &gt;= yearmonth('1992 Jul') & \n                                         index &lt;= yearmonth('2023 Dec')), \n                                aes(x = index, y = random)) +\n  geom_line(group = 1, color = \"blue\") +\n  geom_point(data = china_decompose_mult %&gt;% filter(index %in% highlight_dates),\n             aes(x = index, y = random),\n             shape = 24, color = \"red\", fill = \"red\", size = 4) + \n  labs(title = \"Fig 4 - Exports Commodities Random Component vs Macroeconomic Events\",\n       x = \"\",\n       y = \"Random\") +\n  scale_x_yearmonth(\n    labels = scales::label_date(\"%Y %b\"),  \n    breaks = \"36 months\"                    \n  )\n\nplot_random_component\n\n\n\n\n\nFig 4 - Exports Commodities Random Component vs Macroeconomic Events\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig4_random_vs_macroeventns.png\", plot = plot_random_component, width = 12, height = 6, dpi = 300)\n\n\n\n\nWhy a Multiplicative Decomposition Model Fits\nThe multiplicative model is particularly suitable because the amplitude of seasonal changes and irregular fluctuations increases along with the trend. In China’s export data, as exports grow, the seasonal variations and irregularities also become more pronounced. A multiplicative model accounts for this proportionality, making it appropriate for capturing the dynamics of the data.\n\n\nWhy an Additive Decomposition Model Would Not Work\nAn additive model assumes that the size of seasonal and irregular variations remains constant regardless of the trend. However, the data shows that fluctuations grow as the trend grows. Therefore, an additive model would fail to accurately represent the increasing variability linked to higher export levels.\n\n\nAdditive Classical Decomposition Model Plot\n\n\nShow the code\nchina_decompose_add &lt;- cexports_ts |&gt;\n  model(feasts::classical_decomposition(exports_commodities, type = \"add\"))  |&gt;\n  components()\n\nautoplot(china_decompose_add) +\n  ggtitle(\"Fig 5 - Classical Additive Decomposition of China's Exports\")\n\n\n\n\n\nFig 5 - Classical Additive Decomposition of China’s Exports",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#key-takeaways",
    "href": "projects/project1.html#key-takeaways",
    "title": "Time Series: China Export Commodities",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSeasonal Patterns: Understanding these can help companies optimize their supply chains, especially during high-demand months.\nEconomic Resilience: The trend shows that China’s export sector has consistently been increasing over time.\nStrategic Planning: By anticipating seasonal lows (like in February) and leveraging peaks (in December), businesses can better align their operations with demand cycles.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#recommendations",
    "href": "projects/project1.html#recommendations",
    "title": "Time Series: China Export Commodities",
    "section": "Recommendations",
    "text": "Recommendations\nGiven the observed seasonality, trend, and irregular components of China’s export data, it is important to prepare for upcoming political events that could potentially lead to a temporary fall in exports. However, based on historical patterns, China has demonstrated resilience and the ability to recover fairly quickly from these disruptions. Therefore, we recommend the following actionable items:\n\nDiversify Supply Chains: Identify and establish relationships with alternative suppliers and markets to reduce dependence on China during potential disruptions.\nIncrease Inventory Buffer: Build up inventory reserves during periods of high export activity to prepare for potential downturns caused by political events.\nMonitor Key Indicators: Closely monitor geopolitical developments and economic indicators to respond proactively to any signals of impending disruptions.\nEstablish Contingency Plans: Develop and maintain contingency plans to address short-term declines in supply, including logistical adjustments and finding new suppliers.\nCommunicate with Partners: Maintain open communication with Chinese suppliers and partners for early warnings of potential issues.\nLeverage Seasonal Patterns: Use the identified seasonal peaks to strategically schedule production and export needs, ensuring that reserves are built up during times of predictable high export activity.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#conclusion",
    "href": "projects/project1.html#conclusion",
    "title": "Time Series: China Export Commodities",
    "section": "Conclusion",
    "text": "Conclusion\nThe decomposition of China’s export data reveals a strong upward trend, along with predictable seasonal fluctuations and irregular disruptions. While upcoming political events may lead to short-term declines, historical data shows that these are typically followed by rapid recovery. By understanding these trends, seasonality, and random components, businesses can better prepare for potential disruptions without significant long-term impacts.\nThe key insight is that while risks are present, proactive preparation can mitigate their effects. By leveraging strategic planning, maintaining communication, and building contingency buffers, companies can continue to operate smoothly and minimize disruptions. This approach ensures resilience in the face of political uncertainties, relying on the observed adaptability of China’s export capabilities.\nWhat’s Next? I’ll continue refining the analysis and exploring potential forecast models to enhance predictive capabilities.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project3.html",
    "href": "projects/project3.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Project 3",
      "Project title here"
    ]
  },
  {
    "objectID": "tools/stepsforDate_index_formatting.html",
    "href": "tools/stepsforDate_index_formatting.html",
    "title": "Steps for formatting Date and Creating Index",
    "section": "",
    "text": "Code\n#source(\"common_functions.R\") # should i be using this one?? not a good idea since many of the formulas return different values\n\n# Loading R packages. originally was using common functions but trying not to use\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra, tidyquant\n               )\n\n\nThe following is steps to check in what format the date column is in in a new dataset. Then to convert to DATE format to do time series research\n\nImport Data:\nCheck date Column Type:\nConvert date to Date Format:\n\n\n\nCode\n# the rename is for a different df\n\n# Step 1: Import your data\ndf &lt;- read.csv(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") \n# We'll assume the data has a column called 'date' (replace with the actual column name) and a 'value' column\n\n\n# Step 2: Check the structure of the 'date' column to verify its type\nstr(df$date)  # This will show you if the date column is a character, Date, or something else\n\n\n# Convert 'date' column to Date type if it's in character format \n# 2.1 reomve unwanted columns or\ndf1 &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date) # date is date columm name\n         # mdy(date) mdy is current order of date, lubridate will format to ymd. \n       #dplyr::select(-comments) # remove unwanted columns\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) # rename columns\n\n# 2.1 Convert 'Date' column to yearquarter format\ndf$Date &lt;- lubridate::ymd(df$Date) # df2\n\n\n\n# Verify the 'date' column is now in the correct format (should be Date type)\nstr(df$date)  # Should now return 'date'\n\n\nDoing mutate by getting weekly average\n\n\nCode\n# Set symbol and date range for Apple\nsymbol &lt;- \"AAPL\"\ndate_start &lt;- \"2022-01-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Download the stock data\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Convert to a tsibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(dates = lubridate::ymd(date), value = adjusted) %&gt;%\n  mutate(year_week = yearweek(dates)) |&gt;\n  group_by(year_week) |&gt;\n  summarise(value = mean(value)) |&gt;\n  ungroup() |&gt;\n  as_tsibble(index = year_week)\n\n# Time plot of the daily closing prices\nautoplot(stock_ts, value) +\n  labs(title = \"Time Plot of Apple (AAPL) Daily Closing Prices\",\n       x = \"Date\", y = \"Closing Price (USD)\")\n\n\n\n\n\n\n\n\n\nthe code below was taken from project one, made to plot the time series without doing the monthly mean. The first two lines of code are missing\ndata is for daily data.\n\n\nCode\n# this code is also not complete\n\n\n# the first 2 ts can almost be taken from the previous code.\n# this code was first use but replace by the above code\n\n  dplyr::select(dates, year, months, value)  |&gt; # ts3\n  arrange(dates) |&gt; # ts4\n  mutate(index = tsibble::yearmonth(dates)) |&gt; # ts5\n  as_tsibble(index = index) |&gt; # ts6\n  dplyr::select(index, dates, year, months, value) |&gt; # ts7\n  rename(Vessels = value) # rename value to emphasize data context\nvessels_ts |&gt; #ts8\n  autoplot(.vars = Vessels) +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nworking with two variables and period column\n\n\nCode\n# I think this file is all for samples, and I edit them so now the code is all mix up hence all the eval=false! sucks \n\ndf &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n# str(df$date)\n\n\ndf1 &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date),\n         constructionequip_ord = as.numeric(constructionequip_ord), # make sure numeric for x variables\n         constructionequip_ship = as.numeric(constructionequip_ship)\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) |&gt; # renames columns and converts to numeric\n  select(date, x, y) # re orders and or removes not selected columns\n  \ndf2 &lt;- df1 |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(obs, x, y)\n\n# can the obs code to make multiple df with different columns. \n\n\n\n\ndata has gaps\nTyson notes\n\nfilling with previous variable\n\nThen there is 100% correlation with the previous variable\n\nFilling with average\n\nsome data’s average, like weather can vary so it can trow off random,\nusing the same variable as last year is an option, it will just mess with the seasonality\n\ntaking the sum of the lag and lead periods, and divide by two to replace missing\n\nits a good one\n\n\n\n\nCode\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\") |&gt;\n    mutate(YM = yearmonth(lubridate::mdy(date)))\ndf &lt;- as_tsibble(df0, index = YM) |&gt;\n  select(ym, cdebt)\ninterval(df) # gives interval: M, D ot Y etc\nhas_gaps(df) # false if none and vice versa True\n\n\n\n\nReading different file formats\n\n\nCode\n# wine_dat &lt;- read_table(\"data/wine.dat\") # resource 3.4.2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Date & Index formatting",
      "Steps for formatting Date and Creating Index"
    ]
  },
  {
    "objectID": "hw/homework_1_1.html",
    "href": "hw/homework_1_1.html",
    "title": "Time Series Homework: Chapter 1 Lesson 1",
    "section": "",
    "text": "What is something unusual or unique about you?\n\n\n\n\n\n\nAnswer\n\n\n\nI have been coming to BYUI for almost four years, and therefore, I have been focusing on school for nearly four years, so I can’t think of anything unique or unusual about me besides school, but I have some ideas. I have about 14 siblings, but I’m the only child between my mom and dad. Back in 2007, I was able to solve about 90% of a Rubik’s cube without looking, which was impressive back then but not so much now. My ability to figure out and solve things like the Rubik’s cube remains so I can solve many complex things that the average person will not think to even try. One unusual thing is that although I’m really smart in some scenarios like the Rubik’s cube, I can also be really dumb in some basic university study habits, so yeah. \n\n\n\n\n\nWhat do you hope to gain from this class?\n\n\n\n\n\n\nAnswer\n\n\n\nI’m looking to improve my ability to explain business and financial numbers using statistics. I hope this class gives me the skills and vocabulary to properly and correctly explain the statistical meanings behind business, financial, and/or economic data. I also hope that this course can help me use knowledge and skills related to my data science minor and apply them to my major. I’m also excited to expand my econometrics knowledge with this course. \n\n\n\n\n\n\n\n\n\nTip for Exercise 3\n\n\n\nIn a qmd file, you can do all sorts of fun things. If you type &lt;ctrl&gt; &lt;alt&gt; i, RStudio will insert an R code chunk, where you can run any R code. The code below generates 10 uniform random variables between 0 and 1.\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\nreps &lt;- 4\n\n# Create a data frame with a counter variable, t, \n# and the simulated values, called x\ndf1 &lt;- data.frame(t = 1:reps, x = runif(reps, min = 0, max = 1))\ndf1\n\n  t          x\n1 1 0.30776611\n2 2 0.25767250\n3 3 0.55232243\n4 4 0.05638315\n\n\nYou can use inline R code as well. For example, the mean of the n=4 simulated x values is: 0.293536.\nIf you do not know how to use any command such as rnorm in R, you can type the name of the command after a question mark (e.g. ?rnorm) in the console area, and the help file will appear on the right.\n\n\n\n\n\nUse the rnorm command to simulate 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Please do not list all the numbers. Instead, give a histogram of the data.*\n\n\n\n\n\n\nAnswer\n\n\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\n# Simulate 100,000 normally-distributed random variables with mean=50 and sd=10\n\nsimulated_data &lt;- rnorm(100000, mean = 50, sd = 10)\n\n\n# histogram\n\nhist(simulated_data, \n\n     main = \"Histogram of Simulated Normally-Distributed Data\", \n\n     xlab = \"Value\", \n\n     ylab = \"Frequency\", \n\n     col = \"skyblue\", \n\n     border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose your pulse was measured at the start of every hour today and the values are given in the data frame pulse_df.\n\nset.seed(123)\npulse_df &lt;- data.frame(\n  times = paste0(as.Date(substr(now(),1,10)), \" \", c(0:23), \":00\"),\n  value = sample(70:100, size = 24, replace = TRUE)\n)\n\nWe can convert a character representation of a date to a date-time object. The Lubridate package contains commands such as mdy(\"12/31/2024\") which converts this value to: 2024-12-31. There are other variations of this command such as dmy_hms(\"31/12/2024 15:16:47\") which gives us: 2024-12-31 15:16:47.\nUse the command ymd_hm() to convert the times variable into a date-type variable. Then filter the four observations from noon to 3 PM (15:00). There are many ways to accomplish this but here is one simple example:\n\npulse_df$times &lt;- ymd_hm(pulse_df$times) # convert times to date-type data\npulse_df_filtered &lt;- pulse_df %&gt;% filter(hour(times) &gt;= 12 & hour(times) &lt;= 15) # using the hour() function from lubridate we look at just the hour section of the 'times' variable and then filter to values between 12 and 15\n\nThen, compute the mean of the observed value for these four times. Write your code in the R chunk below. Write an English sentence giving this mean and its interpretation in terms of your pulse rate.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# put your code  \n# To avoid code clutter, and since I was just going to use the giving samples, I only included the code to compute the mean. So when a qmd file is rendered, the first two chunks of r code fit with this third r chunk and are not just extra chunks of code. \n\n# computing mean\nmean_pulse &lt;- mean(pulse_df_filtered$value)\n\n# display\nmean_pulse\n\n[1] 94\n\npulse_df_filtered\n\n                times value\n1 2024-12-12 12:00:00    91\n2 2024-12-12 13:00:00    94\n3 2024-12-12 14:00:00    95\n4 2024-12-12 15:00:00    96\n\n\nThe mean pulse rate observed from noon to 3 PM is 94 beats per minute. This value represents the average pulse rate during these four specific hours.\n\n\n\n\n\nDo you have any concerns about your ability to succeed in this class? If so, please share them.\n\n\n\n\n\n\nAnswer\n\n\n\nYes, my main concern is getting lost in the math portion of the statistics. I fear I can get stuck in some parts, and will cause me to lose much time figuring out statistical math that should be done quickly. I also have a concern of understanding all the math symbols, I just don’t want to spend an insane amount of time figuring out what symbols mean which it happen in ECON 381.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (5)\nIncomplete (0)\n\n\n\nOffers a solid and well-elaborated description of unique or unusual aspects, demonstrating a good level of self-awareness and the ability to express distinctive qualities effectively.\nGives a limited or unclear description of unique or unusual aspects, lacking essential details. Demonstrates a minimal level of self-awareness.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates clear and specific goals for the class, demonstrating a good level of understanding of what the individual aims to gain.\nStates goals with limited clarity or specificity, lacking detail and depth. The understanding of what is hoped to be gained is minimal.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nUtilizes the rnorm command accurately, generating 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Demonstrates a precise understanding of the command and its parameters.\nFails to use the rnorm command to generate normally-distributed random variables, leaving the question unanswered or without the required simulation.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nSuccessfully applies the ymd_hm() command to convert the times variable into a date-type variable. Accurately filters the four observations from noon to 3 PM (15:00). Accurately computes the mean of the observed values for the specified times and provides a clear English sentence interpreting the mean in terms of pulse rate. The interpretation is insightful.\nFails to provide any code for applying the ymd_hm() command and filtering, leaving the R chunk incomplete or without the required transformation. Attempts to compute the mean and provide an interpretation but with significant inaccuracies or lack of clarity. The interpretation may be incorrect or incomplete.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates concerns with clarity and provides a good level of detail in explaining potential challenges to success in the class. The response is clear and relevant.\nStates concerns with limited clarity or detail, lacking in-depth explanations. Demonstrates a minimal understanding of potential challenges to success.\n\n\nTotal Points\n35"
  },
  {
    "objectID": "hw/homework_1_1.html#questions",
    "href": "hw/homework_1_1.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 1",
    "section": "",
    "text": "What is something unusual or unique about you?\n\n\n\n\n\n\nAnswer\n\n\n\nI have been coming to BYUI for almost four years, and therefore, I have been focusing on school for nearly four years, so I can’t think of anything unique or unusual about me besides school, but I have some ideas. I have about 14 siblings, but I’m the only child between my mom and dad. Back in 2007, I was able to solve about 90% of a Rubik’s cube without looking, which was impressive back then but not so much now. My ability to figure out and solve things like the Rubik’s cube remains so I can solve many complex things that the average person will not think to even try. One unusual thing is that although I’m really smart in some scenarios like the Rubik’s cube, I can also be really dumb in some basic university study habits, so yeah. \n\n\n\n\n\nWhat do you hope to gain from this class?\n\n\n\n\n\n\nAnswer\n\n\n\nI’m looking to improve my ability to explain business and financial numbers using statistics. I hope this class gives me the skills and vocabulary to properly and correctly explain the statistical meanings behind business, financial, and/or economic data. I also hope that this course can help me use knowledge and skills related to my data science minor and apply them to my major. I’m also excited to expand my econometrics knowledge with this course. \n\n\n\n\n\n\n\n\n\nTip for Exercise 3\n\n\n\nIn a qmd file, you can do all sorts of fun things. If you type &lt;ctrl&gt; &lt;alt&gt; i, RStudio will insert an R code chunk, where you can run any R code. The code below generates 10 uniform random variables between 0 and 1.\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\nreps &lt;- 4\n\n# Create a data frame with a counter variable, t, \n# and the simulated values, called x\ndf1 &lt;- data.frame(t = 1:reps, x = runif(reps, min = 0, max = 1))\ndf1\n\n  t          x\n1 1 0.30776611\n2 2 0.25767250\n3 3 0.55232243\n4 4 0.05638315\n\n\nYou can use inline R code as well. For example, the mean of the n=4 simulated x values is: 0.293536.\nIf you do not know how to use any command such as rnorm in R, you can type the name of the command after a question mark (e.g. ?rnorm) in the console area, and the help file will appear on the right.\n\n\n\n\n\nUse the rnorm command to simulate 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Please do not list all the numbers. Instead, give a histogram of the data.*\n\n\n\n\n\n\nAnswer\n\n\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\n# Simulate 100,000 normally-distributed random variables with mean=50 and sd=10\n\nsimulated_data &lt;- rnorm(100000, mean = 50, sd = 10)\n\n\n# histogram\n\nhist(simulated_data, \n\n     main = \"Histogram of Simulated Normally-Distributed Data\", \n\n     xlab = \"Value\", \n\n     ylab = \"Frequency\", \n\n     col = \"skyblue\", \n\n     border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose your pulse was measured at the start of every hour today and the values are given in the data frame pulse_df.\n\nset.seed(123)\npulse_df &lt;- data.frame(\n  times = paste0(as.Date(substr(now(),1,10)), \" \", c(0:23), \":00\"),\n  value = sample(70:100, size = 24, replace = TRUE)\n)\n\nWe can convert a character representation of a date to a date-time object. The Lubridate package contains commands such as mdy(\"12/31/2024\") which converts this value to: 2024-12-31. There are other variations of this command such as dmy_hms(\"31/12/2024 15:16:47\") which gives us: 2024-12-31 15:16:47.\nUse the command ymd_hm() to convert the times variable into a date-type variable. Then filter the four observations from noon to 3 PM (15:00). There are many ways to accomplish this but here is one simple example:\n\npulse_df$times &lt;- ymd_hm(pulse_df$times) # convert times to date-type data\npulse_df_filtered &lt;- pulse_df %&gt;% filter(hour(times) &gt;= 12 & hour(times) &lt;= 15) # using the hour() function from lubridate we look at just the hour section of the 'times' variable and then filter to values between 12 and 15\n\nThen, compute the mean of the observed value for these four times. Write your code in the R chunk below. Write an English sentence giving this mean and its interpretation in terms of your pulse rate.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# put your code  \n# To avoid code clutter, and since I was just going to use the giving samples, I only included the code to compute the mean. So when a qmd file is rendered, the first two chunks of r code fit with this third r chunk and are not just extra chunks of code. \n\n# computing mean\nmean_pulse &lt;- mean(pulse_df_filtered$value)\n\n# display\nmean_pulse\n\n[1] 94\n\npulse_df_filtered\n\n                times value\n1 2024-12-12 12:00:00    91\n2 2024-12-12 13:00:00    94\n3 2024-12-12 14:00:00    95\n4 2024-12-12 15:00:00    96\n\n\nThe mean pulse rate observed from noon to 3 PM is 94 beats per minute. This value represents the average pulse rate during these four specific hours.\n\n\n\n\n\nDo you have any concerns about your ability to succeed in this class? If so, please share them.\n\n\n\n\n\n\nAnswer\n\n\n\nYes, my main concern is getting lost in the math portion of the statistics. I fear I can get stuck in some parts, and will cause me to lose much time figuring out statistical math that should be done quickly. I also have a concern of understanding all the math symbols, I just don’t want to spend an insane amount of time figuring out what symbols mean which it happen in ECON 381.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (5)\nIncomplete (0)\n\n\n\nOffers a solid and well-elaborated description of unique or unusual aspects, demonstrating a good level of self-awareness and the ability to express distinctive qualities effectively.\nGives a limited or unclear description of unique or unusual aspects, lacking essential details. Demonstrates a minimal level of self-awareness.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates clear and specific goals for the class, demonstrating a good level of understanding of what the individual aims to gain.\nStates goals with limited clarity or specificity, lacking detail and depth. The understanding of what is hoped to be gained is minimal.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nUtilizes the rnorm command accurately, generating 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Demonstrates a precise understanding of the command and its parameters.\nFails to use the rnorm command to generate normally-distributed random variables, leaving the question unanswered or without the required simulation.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nSuccessfully applies the ymd_hm() command to convert the times variable into a date-type variable. Accurately filters the four observations from noon to 3 PM (15:00). Accurately computes the mean of the observed values for the specified times and provides a clear English sentence interpreting the mean in terms of pulse rate. The interpretation is insightful.\nFails to provide any code for applying the ymd_hm() command and filtering, leaving the R chunk incomplete or without the required transformation. Attempts to compute the mean and provide an interpretation but with significant inaccuracies or lack of clarity. The interpretation may be incorrect or incomplete.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates concerns with clarity and provides a good level of detail in explaining potential challenges to success in the class. The response is clear and relevant.\nStates concerns with limited clarity or detail, lacking in-depth explanations. Demonstrates a minimal understanding of potential challenges to success.\n\n\nTotal Points\n35"
  },
  {
    "objectID": "hw/homework_1_4.html",
    "href": "hw/homework_1_4.html",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "",
    "text": "Show the code\n# Weather data for Rexburg\nrex_temp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\")"
  },
  {
    "objectID": "hw/homework_1_4.html#data",
    "href": "hw/homework_1_4.html#data",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "",
    "text": "Show the code\n# Weather data for Rexburg\nrex_temp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\")"
  },
  {
    "objectID": "hw/homework_1_4.html#questions",
    "href": "hw/homework_1_4.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1: Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\na) Rexburg, ID Daily High Temperatures\n\n\n\n\n\n\nAnswer\n\n\n\nThe data consist of the daily high for the Rexburg airport so each observation reflects the highest fahrenheit temperature recorded at the Rexburg airport. The data is giving daily from 1999/01/02 to 2023/12/20. Data is pretty straight forward.\n\n\nShow the code\n# it is important to change to a tsibble for time series analysis. The idea is to have an index that works with time series formulas, and it is also important to make new data frames have the same index format. \n# One of my stumbling block for this hw was not having the same index format for some of the new df that where made so it delay me a lot\n# also, first reading the class samples and the steps done, then possibly do it in excel to understand how my data will look. \n\n\n\ntemps_ts &lt;- rex_temp |&gt;\n  arrange(dates) |&gt; # Sorting oldest to newest\n  dplyr::select(dates, rexburg_airport_high) |&gt;\n  rename(temp_high = rexburg_airport_high) |&gt;\n  mutate(dates = as.Date(dates)) |&gt; # Ensure dates is a Date object\n  as_tsibble(index = dates) # Convert to tsibble object\n\n\nFirst few rows of tsibble follow\n\n\nShow the code\ntemps_ts |&gt;\n  head()\n\n\n# A tsibble: 6 x 2 [1D]\n  dates      temp_high\n  &lt;date&gt;         &lt;int&gt;\n1 1999-01-02        30\n2 1999-01-03        25\n3 1999-01-04        26\n4 1999-01-05        29\n5 1999-01-06        32\n6 1999-01-07        31\n\n\n\n\n\n\n\n\nQuestion 2: Visualization (5 points)\nPlease plot the Rexburg Daily Temperature series choosing the range and frequency to illustrate the data in the most readable format. Use the appropriate axis labels, units, and captions.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Please provide your code here\n\nplot_ly(temps_ts, x = ~dates, y = ~temp_high, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    title = \"Rexburg Daily High Temperature\",\n    xaxis = list(title = \"Day\"),\n    yaxis = list(title = \"Rexburg High Temp\"),\n    plot_bgcolor = \"white\",  # optional: sets background color to white\n    title_x = 0.5  # centers the title\n  )\n\n\n\n\n\n\n\n\n\n\nQuestion 3: Additive Decomposition - Manual Approach (25 points)\nThis exercise will guide you through all the steps to conduct an additive decomposition of the Rexburg Daily Temperature Series. The first step is to aggregate the daily series to a monthly frequency to ease on the calculation. The code below accomplished the task.\n\na) Please use AI to comment and explain the steps of the code below. Replace the code below with the fully explained code.\n\n# Weather data for Rexburg\nmonthly_tsibble &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\") |&gt; \n  # Convert 'dates' to Date format\n  mutate(date2 = ymd(dates)) |&gt;\n  # Extract year and month from 'date2'\n  mutate(year_month = yearmonth(date2)) |&gt;\n  # Group data by 'year_month'\n  group_by(year_month) |&gt;\n  # Calculate mean of 'rexburg_airport_high' for each group\n  summarize(average_daily_high_temp = mean(rexburg_airport_high)) |&gt;\n  # Remove grouping\n  ungroup() |&gt; \n  # Convert data frame to time series tibble\n  as_tsibble(index = year_month)\n\n# Display the resulting tibble\n#view(monthly_tsibble) \n\n\n\nb) Please calculate the centered moving average of the Rexburg Monthly Temperature Series. Plot the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Doing m_hat & s_hat formulas & plotting m_hat\nmonthly_tsibble &lt;- monthly_tsibble |&gt;\n  mutate( # allows to do calculations - doing - m_hat & s_hat\n    m_hat = (\n          (1/2) * lag(average_daily_high_temp, 6)\n          + lag(average_daily_high_temp, 5)\n          + lag(average_daily_high_temp, 4)\n          + lag(average_daily_high_temp, 3)\n          + lag(average_daily_high_temp, 2)\n          + lag(average_daily_high_temp, 1)\n          + average_daily_high_temp\n          + lead(average_daily_high_temp, 1)\n          + lead(average_daily_high_temp, 2)\n          + lead(average_daily_high_temp, 3)\n          + lead(average_daily_high_temp, 4)\n          + lead(average_daily_high_temp, 5)\n          + (1/2) * lead(average_daily_high_temp, 6)\n        ) / 12,\n    s_hat = average_daily_high_temp - m_hat # s_hat calculation\n  )\n\n# Plot the CMA (centred moving average) and original data\nplain_plot &lt;- autoplot(monthly_tsibble, .vars = m_hat) +\n  labs(\n    x = \"Date\",\n    y = \"Rexburg Monthly avg High\",\n    title = \"Rexburg Monthly CMA\"\n  ) +\n  scale_y_continuous(limits = c(min(monthly_tsibble$m_hat, na.rm = TRUE), max(monthly_tsibble$m_hat, na.rm = TRUE))) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Plot the original unemployment data with CMA overlay\nfancy_plot &lt;- autoplot(monthly_tsibble, .vars = average_daily_high_temp) +\n  labs(\n    x = \"Date\",\n    y = \"Rexburg Monthly avg High\",\n    title = \"Rexburg Monthly CMA\"\n  ) +\n  geom_line(aes(x = year_month, y = m_hat), color = \"#D55E00\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine the two plots\nplain_plot\n\n\n\n\n\n\n\n\n\nShow the code\nfancy_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nc) Please calculate the seasonally adjusted means series. Plot the series\nIn this just the monthly additive effect?\n\n\nShow the code\n# Please provide your code here\n# Extracting year and month from the year_month column\n# adding s_bar manually do to joining errors\nmonthly_tsibble_extended &lt;- monthly_tsibble %&gt;%\n  mutate(\n    year = year(year_month),\n    month = month(year_month),\n    s_bar = case_when(\n      month == 1  ~ -28.7455842,\n      month == 2  ~ -24.7455412,\n      month == 3  ~ -11.7605428,\n      month == 4  ~ -0.4256582,\n      month == 5  ~ 9.9075202,\n      month == 6  ~ 19.4142229,\n      month == 7  ~ 30.0205787,\n      month == 8  ~ 28.1817736,\n      month == 9  ~ 17.5450445,\n      month == 10 ~ 1.6857404,\n      month == 11 ~ -14.1466707,\n      month == 12 ~ -26.9308833\n    )\n  )\n\n\n# Initialize an empty list to store the sums for each month\nmonthly_averages &lt;- vector(\"list\", 12)\n\n# Loop through months 1 to 12\nfor (month in 1:12) {\n  # Calculate the sum of s_hat for each month and store in the list\n  monthly_averages[[month]] &lt;- (mean(monthly_tsibble_extended$s_hat[monthly_tsibble_extended$month == month], na.rm = TRUE))\n}\n\n# Optionally, you can convert the list to a named vector for clearer output\nnames(monthly_averages) &lt;- month.name  # Assigning month names for clarity\n\n# Calculate the overall mean of the unadjusted monthly additive component\noverall_mean_of_the_unadjusted_monthly_additive_component &lt;- sum(unlist(monthly_averages)) / 12\n\n# Compute the seasonally adjusted mean for each month\nseasonally_adjusted_mean &lt;- sapply(monthly_averages, function(x) x - overall_mean_of_the_unadjusted_monthly_additive_component)\n\n# Naming the results for the seasonally adjusted means for clarity\nnames(seasonally_adjusted_mean) &lt;- month.name\n\n# Display the overall mean and the seasonally adjusted means\noverall_mean_of_the_unadjusted_monthly_additive_component\n\n\n[1] -0.00853748\n\n\nShow the code\n# Create a data frame for the results\ns_bar_df &lt;- data.frame(\n  month = month.name,\n  s_hat_bar = unlist(monthly_averages),\n  s_bar = unlist(seasonally_adjusted_mean)\n)\n\n# formulas for random and seasonally adjusted x\nadjusted_ts &lt;- monthly_tsibble_extended |&gt;\n  mutate(random = average_daily_high_temp - m_hat - s_bar) |&gt; \n  mutate(seasonally_adjusted_x = average_daily_high_temp - s_bar)\n\n# note difference between s_bar resuts\n# the last mutate, seasonally_adjusted_x is the SEASONALLY ADJUSTED SERIES\n# and just having the s_bar or seasonally_adjusted_mean (12 months only) is the SEASONALLY ADJUSTED MEANS \n\n\nplotting seasonally adjusted mean\n\n\nShow the code\n# plotting seasonally adjusted mean\nfancy_plot &lt;- autoplot(adjusted_ts, .vars = seasonally_adjusted_x) +\n  labs(\n    x = \"Date\",\n    y = \"Seasonally Adjusted Mean\",\n    title = \"Seasonally Adjusted Mean for Each Month\"\n  )\nfancy_plot\n\n\n\n\n\n\n\n\n\nShow the code\n# the seasonally adjusted mean for each month refers to just the 12 months not the adjusted series. but not exact which one we should use.\n\n\n\n\nd) Please calculate the random component. Please plot the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Plot of the random component\nfancy_plot &lt;- autoplot(adjusted_ts, .vars = random) +\n  labs(\n    x = \"Date\",\n    y = \"Random\",\n    title = \"Random Component\"\n  )\nfancy_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4: Additive Decomposition - R’s Decompose (15)\n\na) Please use additive decomposition model described in the Time Series notebook to decompose the Rexburg Monthly Temperature Series. Plot the algorithm’s output.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Please provide your code here\n# Compute the additive decomposition for adjusted_ts\ntemps_decompose_add &lt;- adjusted_ts |&gt;\n  model(feasts::classical_decomposition(average_daily_high_temp,\n          type = \"add\"))  |&gt;\n  components()\n\n# Compute the multiplicative decomposition for adjusted_ts\ntemps_decompose_mult &lt;- adjusted_ts |&gt;\n  model(feasts::classical_decomposition(average_daily_high_temp,\n          type = \"mult\"))  |&gt;\n  components()\n\n# Display 14 rows of both decompositions\n# temps_decompose_mult |&gt;\n#  head(14)\n\ntemps_decompose_add |&gt;\n  head(14)\n\n\n# A dable: 14 x 7 [1M]\n# Key:     .model [1]\n# :        average_daily_high_temp = trend + seasonal + random\n   .model  year_month average_daily_high_t…¹ trend seasonal random season_adjust\n   &lt;chr&gt;        &lt;mth&gt;                  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1 \"feast…   1999 Jan                   30.3  NA    -28.7   NA              59.1\n 2 \"feast…   1999 Feb                   33.7  NA    -24.7   NA              58.5\n 3 \"feast…   1999 Mar                   44.4  NA    -11.8   NA              56.1\n 4 \"feast…   1999 Apr                   52.6  NA     -0.426 NA              53.0\n 5 \"feast…   1999 May                   61.3  NA      9.91  NA              51.4\n 6 \"feast…   1999 Jun                   71.4  NA     19.4   NA              52.0\n 7 \"feast…   1999 Jul                   82.1  56.2   30.0   -4.16           52.1\n 8 \"feast…   1999 Aug                   83.0  56.4   28.2   -1.59           54.8\n 9 \"feast…   1999 Sep                   71.8  56.6   17.5   -2.31           54.3\n10 \"feast…   1999 Oct                   63.0  57.0    1.69   4.25           61.3\n11 \"feast…   1999 Nov                   53.1  57.6  -14.1    9.66           67.2\n12 \"feast…   1999 Dec                   28.2  58.0  -26.9   -2.89           55.1\n13 \"feast…   2000 Jan                   30.3  58.3  -28.7    0.654          59.0\n14 \"feast…   2000 Feb                   37.2  58.6  -24.7    3.31           62.0\n# ℹ abbreviated name: ¹​average_daily_high_temp\n\n\n\n\n\n\nb) You can also decompose without aggregating the data series. The following code completes the additive decomposition with the original data set. Please use AI to comment and explain how to accomplish the task. Replace the code below with the fully explained code.\n\n\nShow the code\n# Weather data for Rexburg\ndaily_tsibble &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\") |&gt;\n  mutate(year_month_day = ymd(dates)) |&gt;  # Convert date strings to Date objects\n  dplyr::select(-imputed, -dates) |&gt;  # Remove 'imputed' and 'dates' columns\n  as_tsibble(index = year_month_day)  # Convert the tibble to a tsibble with dates as the index\n\n# Display the first few rows of the 'daily_tsibble'\ndaily_tsibble %&gt;% head\n\n\n# A tsibble: 6 x 2 [1D]\n  rexburg_airport_high year_month_day\n                 &lt;int&gt; &lt;date&gt;        \n1                   30 1999-01-02    \n2                   25 1999-01-03    \n3                   26 1999-01-04    \n4                   29 1999-01-05    \n5                   32 1999-01-06    \n6                   31 1999-01-07    \n\n\nShow the code\n# Decompose the daily high temperature series into seasonal components\ndaily_decompose &lt;- daily_tsibble  |&gt;\n  model(feasts::classical_decomposition(rexburg_airport_high ~ season(365.25),\n                                        type = \"add\"))  |&gt;\n  components()\n\n# Plot the seasonal component of the decomposition\ndaily_decompose |&gt; autoplot(.vars = seasonal)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5: Seasonally Adjusted Series - Analysis (20 points)\n\na) Justify why we use the additive decomposition model to seasonally adjust the Rexburg Daily Temperature series.\n\n\n\n\n\n\nAnswer\n\n\n\nThe Rexburg Daily Temperature series displays a consistent seasonal pattern throughout the period observed, which is a key for an additive model where seasonal fluctuations are stable and do not vary in magnitude over time. The temperature data does not exhibit a multiplicative trend where seasonal variations would proportionally increase or decrease as the level of the series changes. Instead, the variations around the trend appear consistent over time, suggesting that the additive model, where seasonal effects are assumed to be constant through the series, is more appropriate.\nThe deterministic nature of the trend in the temperature data supports the use of an additive model. This model assumes that the components of the series (trend, seasonality, and randomness) can be linearly added or subtracted to model or decompose the series. Since there is no evidence of increasing variability with temperature values, a characteristic more aligned with multiplicative models. The additive assumption holds more relevance. This allows for clearer interpretation and more accurate seasonal adjustment of the data.\n\n\n\n\nb) You calculated the random component of the series using three different procedures. Are the random component series the same? Are there patterns that are similar across all the random component series? Propose an explanation for the commonalities.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nIn the Classical decomposition, the error component is determined by removing trend and seasonal from the original time series, which is the same as the normal additive random component we do in this example. I’m honestly confused on this part because replicating some of the other random series we did in class, and also doing the multiplicative random graph made almost no difference. The changes were hard to see, but I’m obviously missing the idea of knowing the difference between the three and how we get them\n\n\n\\[\n  \\text{random component} = x_t - \\hat m_t - \\bar s_t\n\\]\nNext is personal notes for the random component variables.\nx is the monthly mean of the Rexburg temperature highs. m_hat is the monthly centered moving average, and s_hat is the seasonally adjusted mean for each month. X takes the mean of all observations for the given month (t). m_hat uses the previous six and next six observations (months), this is done to better capture seasonality in the months, because some seasons can be colder than the same season the previous year. Doing the CMA will smooth out the trend and make the time series analysis more accurate. Not using the CMA (using x_t) can cause the variance to spike during abnormal seasons and can falsely point to better doing a multiplicative model. s_bar takes the mean of all unique months, so with this data, s_bar returns 12 observations, one for each month. s_bar helps in finding how random the seasons are. So when a season is colder than usual, it will be below the s_bar value for that month, and if that month is also colder than the CMA for the given month, then it will result in a higher random value. Having the CMA and s_bar months subtracted from the actual normal mean (x) paints a better picture if that was a normal month of January by taking into account the season and means of all Januarys. In an additive model, the months have a pattern and that’s why we use a boxplot. So the seasonality is deterministic, so you can take January and February. A multiplicative model has either an increasing or decreasing trend, so plugging in those same two months a few years later will not work out. So, in essence, the variance moves out of the pattern over time for multiplicative, which makes it stochastic, but the variance stays deterministic because the variance stays constant or within the same seasonality over time."
  },
  {
    "objectID": "hw/homework_1_4.html#rubric",
    "href": "hw/homework_1_4.html#rubric",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "Rubric",
    "text": "Rubric\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 2: Visualization\nChooses a reasonable manual range for the Rexburg Daily Temperature series, providing a readable plot that captures the essential data trends. Creates a plot with accurate and clear axis labels, appropriate units, and a caption that enhances the understanding of the Rexburg Daily Temperature series.\nAttempts manual range selection, but with significant issues impacting the readability of the plot. The chosen range may obscure important data trends, demonstrating a limited understanding of graphical representation.Fails to include, axis labels, units, or captions, leaving the visual representation and interpretation incomplete.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3a: Monthly Aggregation\nThe code has been updated with comments and clear explanations of what each command and function does. The student shows they understand the intuition behind the procedure.\nThe code has not been updated or the comments and explanation do not provide enough evidence to prove the student understand the code.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3b: Centered Moving Average\nCorrectly calculates the centered moving average. Clearly presents the results with well-labeled axes, titles, and a properly formatted plot. | Incorrectly calculates the centered moving average or omits it entirely. The plot is either missing or poorly presented, lacking clear labels, titles, or proper formatting, making it difficult to interpret the results. |\n\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 3c: Seasonally Adjusted Means Series\nCorrectly calculates the seasonally adjusted means series using an appropriate method. Produces a clear, accurate plot of the seasonally adjusted time series with well-labeled axes and titles. | Incorrect calculation or missing/incorrect plot. Plot lacks essential elements like labels, titles, or fails to represent the seasonally adjusted series. |\n\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3d: Random Component Series\nCorrectly calculates the random component of the series by removing the trend and seasonal components. Produces a clear, accurate plot with well-labeled axes, titles, and proper formatting.\nIncorrectly calculates the random component, omits steps (e.g., does not remove trend/seasonality), or the plot is unclear or missing essential elements like labels and titles.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 4a: Decompose Monthly\nCorrectly applies the additive decomposition model to the Rexburg Monthly Temperature Series. Clearly presents the trend, seasonal, and random components in well-labeled plots, with appropriate titles, axes labels, and formatting.\nFails to correctly apply the additive decomposition model, resulting in incorrect or incomplete separation of the trend, seasonal, and random components. The plot is either missing or poorly presented, lacking proper labels, titles, or clear distinction between components.\n\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 4b: Decompose Daily\nThe code has been updated with comments and clear explanations of what each command and function does. The student shows they understand the intuition behind the procedure.\nThe code has not been updated or the comments and explanation do not provide enough evidence to prove the student understand the code.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 5a: Modeling Justification\nClearly differentiates between the multiplicative and additive model assumptions, and shows how the series best matches the additive model’s assumptions.\nIt’s not clear that the student understands the difference between the additive and multiplicative model or their assumptions.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 5b: Random Component Analysis\nCompares the random components derived from three different procedures, accurately stating whether they are the same or different. Identifies and explains any observed similarities or differences. Proposes a logical, well-reasoned explanation for why similar patterns might exist in the random components, using relevant time series concepts (e.g., common noise factors, model assumptions). | Fails to correctly compare the random components, or provides an unclear or inaccurate comparison.\n| Fails to provide a clear or accurate explanation for the commonalities, or provides irrelevant reasoning. |\n\n\n\n\nTotal Points\n75"
  },
  {
    "objectID": "hw/homework_1_5.html",
    "href": "hw/homework_1_5.html",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "",
    "text": "vessels &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\")"
  },
  {
    "objectID": "hw/homework_1_5.html#data",
    "href": "hw/homework_1_5.html#data",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "",
    "text": "vessels &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\")"
  },
  {
    "objectID": "hw/homework_1_5.html#questions",
    "href": "hw/homework_1_5.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1: Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\na) Vessels Cleared in Foreign Trade for United States\nhttps://fred.stlouisfed.org/series/M03022USM583NNBR\n\n\n\n\n\n\nAnswer\n\n\n\nThe Vessels data gives the monthly number of vessels (in thousands) cleared in Foreign Trade for the United States. The plot below plots every month from January 1901 to September 1940. Base on the plot we can observe the number of vessels cleared each month was increasing over time. We can see that the number of vessels cleared monthly hit a decline in growth during certain years, but the time series is in general increasing over time.\n\n\nShow the code\nvessels_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") |&gt; # ts1\n  mutate(\n    dates = dmy(date),\n    year = lubridate::year(date),\n    months = lubridate::month(date),\n    value = vessels\n  ) |&gt; # ts2\n  dplyr::select(dates, year, months, value)  |&gt; # ts3\n  arrange(dates) |&gt; # ts4\n  mutate(index = tsibble::yearmonth(dates)) |&gt; # ts5\n  as_tsibble(index = index) |&gt; # ts6\n  dplyr::select(index, dates, year, months, value) |&gt; # ts7\n  rename(Vessels = value) # rename value to emphasize data context\nvessels_ts |&gt; #ts8\n  autoplot(.vars = Vessels) +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Seasonally Adjusted Series - Calculation and Plot (10 points)\n\na) Decompose the Vessels Cleared in Foreign Trade series using the multiplicative decomposition model. Show the first 10 rows of results from the decomposition like shown in the Time Series Notebook.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose &lt;- vessels_ts |&gt; # Ts\n  model(feasts::classical_decomposition(Vessels,\n          type = \"mult\"))  |&gt; # make sure to update x value\n  components()\n\n# something wrong with the display_table. has to do w/ common libraries\n# vessels_decompose |&gt;\n#   head(10) |&gt;\n#   display_table()\n\n# so this code takes the prepare data and does the classical_decomposition\n\n\n\n\n\n\nb) Illustrate the original, trend, and the seasonally adjusted series in the same plot. Use the appropriate axis labels, units, and captions.\nColor code: Original = black, Trend = blue, and seasonally adjusted series = orange.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose |&gt;\n  ggplot() +\n  geom_line(data = vessels_decompose, aes(x = index, y = Vessels), color = \"black\") +\n  geom_line(data = vessels_decompose, aes(x = index, y = season_adjust), color = \"#D55E00\") +\n  geom_line(data = vessels_decompose, aes(x = index, y = trend), color = \"#0072B2\") +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates(in thousands)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3: Seasonally Adjusted Series - Analysis (30 points)\n\na) Plot the random component of the multiplicative decomposition from Q2a.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose |&gt;\n  ggplot() +\n  geom_line(data = vessels_decompose, aes(x = index, y = random), color = \"black\")  +\n  labs(\n    x = \"Month\",\n    y = \"Random Component\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for US(in thousands)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Use the additive decomposition method to decompose the Vessels Cleared in Foreign Trade series. Plot the random component.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose_additive &lt;- vessels_ts |&gt; # Ts\n  model(feasts::classical_decomposition(Vessels,\n          type = \"add\"))  |&gt; # make sure to update x value\n  components()\n\nvessels_decompose_additive |&gt;\n  ggplot() +\n  geom_line(data = vessels_decompose_additive, aes(x = index, y = random), color = \"black\")  +\n  labs(\n    x = \"Month\",\n    y = \"Random Component\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for US(in thousands)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nShow the code\n# The random component in the multiplicative decomposition model supports insights giving by the seasonal and trend assumptions. We can research maritime events that could have impacted certain certain years of the vessel data, and reasoning that research using the multiplicative model will make the most sense. \n\n# The random component is taken by subtracting the trend and seasonally adjusted mean from any giving month in the index. By doing this, the random component takes into account seasons where vessel numbers could have been trending twice as much as they were the year before. Other events in the data that happen every year would be captured by the seasonally adjusted mean. For example we can argue that before the Christmas season, the number of vessels cleared would be higher then December and the first few months of the year. By doing this the random component tells us how random any giving month in the index is compared to the rest of the data. We can see in the plot of the multiplicative random component that the randomness is not much, and only stays between 0.8 and 1.2. \n\n# The additive random component plot shows the random component raging anywhere from -780 to 1000. We have months as low as 1659 vessels cleared with the highest month being 8475 vessels cleared. This is like trying to predict the number of vessels for next January will be 2000 (+-800). +-800 is a big difference. In contrast using the multiplicative model, the prediction can be 2000 (+-1). \n\n\n\n\n\n\nc) Please describe the differences between the random component series and use it as part of your justification for why we use the multiplicative decomposition model instead of the additive model to seasonally adjust the series.\n\n\n\n\n\n\nAnswer\n\n\n\nThe multiplicative decomposition model’s random component supports insights given by the seasonal and trend assumptions. We can research maritime events that could have impacted certain years of the vessel data, and reasoning that research using the multiplicative model will make the most sense.\n\nThe random component is taken by subtracting the trend and seasonally adjusted mean from any given month in the index. By doing this, the random component considers seasons where vessel numbers could have been trending twice as much as they were the year before. The seasonally adjusted mean would capture other events in the data that happen every year. For example, we can argue that before the Christmas season, the number of vessels cleared would be higher than in December and the first few months of the year. Subtracting the trend (m_hat) and seasonally adjusted mean from any month in the index will result in the random component being a great metric to determined how random any given month in the index is compared to the rest of the data. We can see in the plot of the multiplicative random component that the randomness is not much and only stays between 0.8 and 1.2.\n\nThe additive random component plot shows the random component ranging anywhere from -780 to 1000. We have months as low as 1659 vessels cleared with the highest month being 8475 vessels cleared. This is like trying to predict the number of vessels for next January will be 2000 (+-800). +-800 is a big difference. In contrast, using the multiplicative model, the prediction can be 2000 (+-1)."
  },
  {
    "objectID": "hw/homework_1_5.html#rubric",
    "href": "hw/homework_1_5.html#rubric",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "Rubric",
    "text": "Rubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student provides a clear and detailed explanation of the data collection process, unit of analysis, and meaning of each observation for the series\nThe student provides a basic explanation of the data context, but some details are missing or unclear.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2a: Multiplicative Decomposition\nCorrectly applies the multiplicative decomposition model. Displays the first ten rows of decomposition results (trend, seasonal, and random components) in a clear, organized format with appropriate labeling.\nIncorrectly applies the decomposition model, produces inaccurate results, or fails to present the first ten rows correctly, or the presentation needs to be clearer and more organized.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2b: Plot\nAccurately illustrates all three series in a single, clear plot where each series is distinguishable. Clearly labels the axes with appropriate units and includes informative captions. All elements are well-presented and properly formatted.\nAttempts to create a plot, but with significant inaccuracies or lack of clarity. The plot may not effectively communicate the results due to labeling or presentation issues.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3a: Multiplicative Random Component Plot\nPlots the random component of the multiplicative decomposition with clear axis labels, appropriate units, and proper formatting for readability.\nLacks proper labeling (e.g., missing axis labels, incorrect units), or presents a poorly formatted and unclear plot.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3b: Additive Random Component Plot\nCorrectly applies the additive decomposition method to decompose the series and accurately plots the random component with clear axis labels, units, and proper formatting.\nFails to correctly apply the decomposition method, inaccurately plots the random component, or provides a plot that lacks proper labeling, units, or formatting.\n\n\n\nMastery (20)\nIncomplete (0)\n\n\nQuestion 3c: Random Component Analysis\nClearly describes the differences between the random components derived from the multiplicative and additive decomposition models, and provides a logical and well-reasoned justification for using the multiplicative model based on these differences.\nThe description of the differences between the random components or provides an unclear, unsupported, or incorrect justification for using the multiplicative model.\n\n\nTotal Points\n50"
  }
]