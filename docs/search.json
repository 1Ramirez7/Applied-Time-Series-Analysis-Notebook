[
  {
    "objectID": "tools/stepsforDate_index_formatting.html",
    "href": "tools/stepsforDate_index_formatting.html",
    "title": "Steps for formatting Date and Creating Index",
    "section": "",
    "text": "Code\n#source(\"common_functions.R\") # should i be using this one?? not a good idea since many of the formulas return different values\n\n# Loading R packages. originally was using common functions but trying not to use\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra, tidyquant\n               )\n\n\nThe following is steps to check in what format the date column is in in a new dataset. Then to convert to DATE format to do time series research\n\nImport Data:\nCheck date Column Type:\nConvert date to Date Format:\n\n\n\nCode\n# the rename is for a different df\n\n# Step 1: Import your data\ndf &lt;- read.csv(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") \n# We'll assume the data has a column called 'date' (replace with the actual column name) and a 'value' column\n\n\n# Step 2: Check the structure of the 'date' column to verify its type\nstr(df$date)  # This will show you if the date column is a character, Date, or something else\n\n\n# Convert 'date' column to Date type if it's in character format \n# 2.1 reomve unwanted columns or\ndf1 &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date) # date is date columm name\n         # mdy(date) mdy is current order of date, lubridate will format to ymd. \n       #dplyr::select(-comments) # remove unwanted columns\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) # rename columns\n\n# 2.1 Convert 'Date' column to yearquarter format\ndf$Date &lt;- lubridate::ymd(df$Date) # df2\n\n\n\n# Verify the 'date' column is now in the correct format (should be Date type)\nstr(df$date)  # Should now return 'date'\n\n\nDoing mutate by getting weekly average\n\n\nCode\n# Set symbol and date range for Apple\nsymbol &lt;- \"AAPL\"\ndate_start &lt;- \"2022-01-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Download the stock data\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Convert to a tsibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(dates = lubridate::ymd(date), value = adjusted) %&gt;%\n  mutate(year_week = yearweek(dates)) |&gt;\n  group_by(year_week) |&gt;\n  summarise(value = mean(value)) |&gt;\n  ungroup() |&gt;\n  as_tsibble(index = year_week)\n\n# Time plot of the daily closing prices\nautoplot(stock_ts, value) +\n  labs(title = \"Time Plot of Apple (AAPL) Daily Closing Prices\",\n       x = \"Date\", y = \"Closing Price (USD)\")\n\n\n\n\n\n\n\n\n\nthe code below was taken from project one, made to plot the time series without doing the monthly mean. The first two lines of code are missing\ndata is for daily data.\n\n\nCode\n# this code is also not complete\n\n\n# the first 2 ts can almost be taken from the previous code.\n# this code was first use but replace by the above code\n\n  dplyr::select(dates, year, months, value)  |&gt; # ts3\n  arrange(dates) |&gt; # ts4\n  mutate(index = tsibble::yearmonth(dates)) |&gt; # ts5\n  as_tsibble(index = index) |&gt; # ts6\n  dplyr::select(index, dates, year, months, value) |&gt; # ts7\n  rename(Vessels = value) # rename value to emphasize data context\nvessels_ts |&gt; #ts8\n  autoplot(.vars = Vessels) +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nworking with two variables and period column\n\n\nCode\n# I think this file is all for samples, and I edit them so now the code is all mix up hence all the eval=false! sucks \n\ndf &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n# str(df$date)\n\n\ndf1 &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date),\n         constructionequip_ord = as.numeric(constructionequip_ord), # make sure numeric for x variables\n         constructionequip_ship = as.numeric(constructionequip_ship)\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) |&gt; # renames columns and converts to numeric\n  select(date, x, y) # re orders and or removes not selected columns\n  \ndf2 &lt;- df1 |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(obs, x, y)\n\n# can the obs code to make multiple df with different columns. \n\n\n\n\ndata has gaps\nTyson notes\n\nfilling with previous variable\n\nThen there is 100% correlation with the previous variable\n\nFilling with average\n\nsome dataâ€™s average, like weather can vary so it can trow off random,\nusing the same variable as last year is an option, it will just mess with the seasonality\n\ntaking the sum of the lag and lead periods, and divide by two to replace missing\n\nits a good one\n\n\n\n\nCode\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\") |&gt;\n    mutate(YM = yearmonth(lubridate::mdy(date)))\ndf &lt;- as_tsibble(df0, index = YM) |&gt;\n  select(ym, cdebt)\ninterval(df) # gives interval: M, D ot Y etc\nhas_gaps(df) # false if none and vice versa True\n\n\n\n\nReading different file formats\n\n\nCode\n# wine_dat &lt;- read_table(\"data/wine.dat\") # resource 3.4.2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Date & Index formatting",
      "Steps for formatting Date and Creating Index"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1_code_notes.html",
    "href": "chapters/chapter_4_lesson_1_code_notes.html",
    "title": "Ch 4.1 Code Notes",
    "section": "",
    "text": "set.seed(1)\nn_points &lt;- 50  # Number of points to use in both parts\nsigma &lt;- 2 # Standard deviation for rnorm\naver &lt;- 0\n\n# Generate the tibble using n_points and sigma\nwd_gaussian &lt;- tibble(\n    index = 1:n_points,\n    y = rnorm(n_points, mean = aver, sd = sigma)\n) |&gt; \n  as_tsibble(index = index) |&gt;\n  mutate(\n  density = dnorm(y, mean = aver, sd = sigma))\n\n\n# Plot the first tibble\nwd_gaussian |&gt; \n    ggplot(aes(x = index, y = y)) + \n    geom_line() +\n    theme_bw() +\n    ggtitle(\"Generated White Noise Series\")\n\n\n\n\n\n\n\n# 2. Calculate and print the mean and variance\nsample_mean &lt;- mean(wd_gaussian$y)\nsample_variance &lt;- var(wd_gaussian$y)\ncat(\"Estimated Mean:\", sample_mean, \"\\n\")\n\nEstimated Mean: 0.2008966 \n\ncat(\"Estimated Variance:\", sample_variance, \"\\n\")\n\nEstimated Variance: 2.764863 \n\n# 3. Plot the Autocorrelation Function (ACF)\nacf_plot &lt;- wd_gaussian |&gt;\n  ACF(y, type = \"correlation\") |&gt;\n  autoplot() +\n  ggtitle(\"Autocorrelation Function of White Noise\") +\n  theme_bw()\n\n\n# Plot the histogram using the data from wd_gaussian\nhist_plot &lt;- wd_gaussian |&gt;\n    ggplot(aes(x = y)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"darkgrey\", bins = 10) +\n    geom_line(aes(x = y, y = density)) +\n    theme_bw() +\n  ggtitle(\"Histogram with Theoretical Normal Density Curve\")\n\n\nacf_plot\n\n\n\n\n\n\n\nhist_plot",
    "crumbs": [
      "Lesson 1",
      "Ch 4.1 Code Notes"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1_code_notes.html#homework-4-question-3.a",
    "href": "chapters/chapter_4_lesson_1_code_notes.html#homework-4-question-3.a",
    "title": "Ch 4.1 Code Notes",
    "section": "homework 4 question 3.a",
    "text": "homework 4 question 3.a\nthis is sample code given by ai that uses an rexp function which we dont use but it does appear once in the book for one of the practice problems. I decided not to use it and show more of the math being done on the code\n\n#| include: false\nset.seed(42)\n\n# Parameters\nlambda &lt;- 1\nn &lt;- 500\n\n# Simulate white noise using the exponential distribution\ns_t &lt;- rexp(n, rate = lambda)\nw_t &lt;- s_t - lambda\n\n# Plot the simulation\nlibrary(ggplot2)\n\ndf &lt;- data.frame(Time = 1:n, White_Noise = w_t)\nggplot(df, aes(x = Time, y = White_Noise)) +\n  geom_line(color = \"blue\") +\n  theme_minimal() +\n  labs(\n    title = \"Simulated White Noise Process Using Exponential Distribution\",\n    x = \"Time\",\n    y = expression(w[t])\n  )",
    "crumbs": [
      "Lesson 1",
      "Ch 4.1 Code Notes"
    ]
  },
  {
    "objectID": "chapters/chapter_2.html",
    "href": "chapters/chapter_2.html",
    "title": "Autocorrelation Concepts",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4_code_notes.html",
    "href": "chapters/chapter_4_lesson_4_code_notes.html",
    "title": "Ch 4.4 Code Notes",
    "section": "",
    "text": "transfer from chapter_4_lesson_4_notes.qmd old repo\nI belive this are notes taken from class.\n\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n# chunk number 2\npacf(temps_ts$change)\n\n\n\n\n\n\n\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n# A tibble: 7 Ã— 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\nTo go over\ngo over the table fromt he Fitting Models (Dynamic Number of Parameters) exercise.\nwrite the ar model for this time series. * What does the table tell us. * estimeate * estimate * statistic * p.value\nKnow how to indentify if a model is stationary or not stationary. Moncayoâ€™s r 6 model is not stationary because the of the .98, but the book model, I think the ar 4 model shows it is stationary. ([1] 1.011 1.755 1.453 1.453). Moncayoâ€™s ar6 shows that it is not stationary. so the plot in the lesson, shows an increasing forecasted trend. Since the model says it is stationary, and the temps has been in an increasing trend since the 1980, the book model plot hasthe forecast coming back down to a mean of zero. This makes sense for a stiationary model because it has a mean of zero.\n\n\n\n Back to top",
    "crumbs": [
      "Lesson 4",
      "Ch 4.4 Code Notes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Time Series Analysis Notebook",
    "section": "",
    "text": "Iâ€™m first going to work out this index file, to have the whole picture then I will branch out base on information here. As I branch out information here will be compress and have links to the lessons and samples."
  },
  {
    "objectID": "index.html#stationary-time-series",
    "href": "index.html#stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Stationary Time Series",
    "text": "Stationary Time Series\n\nSationarity of Linear Models Lesson 5.1\n\nLinear models for time series are non-stationary when they include functions of time.\n\nDifferencing can often transform a non-stationary series with a de terministic trend to a stationary series (4.1).\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences (4.2)."
  },
  {
    "objectID": "index.html#non-stationary-time-series",
    "href": "index.html#non-stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Non-Stationary Time Series",
    "text": "Non-Stationary Time Series\n\nNon-Stationary Time Series Lesson 5.2\n\nA time series with a stochastic trend is non-stationary.\nA time series with a deterministic trend is non-stationary.\nA time series with a seasonal component is non-stationary.\nA time series with a unit root is non-stationary."
  },
  {
    "objectID": "index.html#lesson-4.1-white-noise-and-random-walks",
    "href": "index.html#lesson-4.1-white-noise-and-random-walks",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.1 White Noise and Random Walks",
    "text": "Lesson 4.1 White Noise and Random Walks\n\nStochastic Process 4.1\nStochastic processes or random processes evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\n\nDiscrete White Noise DWN 4.1\n\n4.2: Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\nA time series \\(\\{w_t: t = 1, 2, \\ldots, n\\}\\) is a discrete white noise (DWN) if the variables \\(w_1, w_2, \\ldots, w_n\\) are independent and identically distributed with mean 0.\n\nSecond-Order Properties of DWN\n\nWhen we refer to the second-order properties of a time series, we are talking about its variance and covariance. The variance of a DWN is constant, and the covariance between any two observations is zero\nThe mean is a first-order property, the covariance is a second-order property.\n\n\n\nDiscrete White Noise Process\n\nA DWN process will have the following properties:\n\nThere is a discrete observations.\nThe mean of the observations is zero.\nThe variance of the observations is finite.\nSuccessive observations are uncorrelated.\n\n\n\n\n\nRandom Walk\n\nRandom Walks Lesson 4.1\n\nA random walk is a stochastic process in which the difference between each observation is a white noise process, a non-stationary time series. (see def)\nwt is a dwn and often model as gwn, however wt could be as simple as a coin toss (random walk).\n\n\n\nProperties of Random Walk or walks\nor First-Order Properties of A Random Walk - The mean of a random walk series is 0.\nLook at shinny code for this\n\n\nSecond-Order Properties of a Random Walk\n\nCovariance: \\(cov(x_t,x_{t+k})\\):\nThe covariance between two values of the series depends on ( t ):\n\\[\n\\text{cov}(x_t, x_{t+k}) = t \\sigma^2\n\\]\nCorrelation Function \\(\\rho_k\\):\nThe correlation for lag Â k Â is:\n\\[\n\\rho_k = \\frac{1}{\\sqrt{1 + \\frac{k}{t}}}\n\\]\nNon-Stationarity:\nThe variance of the series increases with ( t ), making the random walk non-stationary.\nCorrelogram Characteristics:\nThe correlogram of a random walk typically shows:\n\nPositive autocorrelations starting near 1.\nA slow decrease as ( k ) increases.\n\n\n\n\n\nGaussian White Noise GWN 4.1\n\nIf the variables are normally distributed, i.e.Â \\(w_i \\sim N(0,\\sigma^2)\\), the DWN is called a Gaussian white noise process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n\n\nWhite Noise Time Series\n\nWhite Noise Time Series Lesson 4.1\n\nA white noise time series is a sequence of random variables that are uncorrelated and have a mean of zero.\nA white noise time series has a constant variance.\nA white noise time series has a constant mean.\nA white noise time series has a constant autocorrelation of zero for all lags except when the lag is zero.\n\n\n\n\nCorrelogram 4.1\n\nCorrelogram Lesson 4.1\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series.\nEach correlogram lag tests for correlation significance, increasing the chance of Type I error, resulting in potentially misleading conclusions about significant relationships.\n\n\n\n\nFitting the White Noise Model\n\n\nBackward Shift Operator\nWe define the backward shift operator or the lag operator, \\(\\mathbf{B}\\), as: \\[\n  \\mathbf{B} x_t = x_{t-1}\n\\] where \\(\\{x_t\\}\\) is any time series.\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\\[\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n\\]\n\nProperties of the Backshift Operator\nThe backwards shift operator is a linear operator. So, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants, then \\[\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n\\] The distributive property also holds. \\[\\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t\n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\\]\n.\n\n\n\nsearch words for lesson 4.1\nGaussian white noise - GWN - discrete white noise - dwn - variance - covariance - correlation - correlogram - Type I error - histogram - backward shift operator - backshift operator"
  },
  {
    "objectID": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "href": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.2 White Noise and Random Walks - Part 2",
    "text": "Lesson 4.2 White Noise and Random Walks - Part 2\n\nDifferecing a Time Series\nWhy do we difference a time series? Differencing a time series can help us to remove the trend and make the series stationary.\n\nComputing the difference between successive terms of a random walk leads to a discrete white noise series.\n\n\\[\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\\]\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences.\n\n\nCorrelograms & Histogram\nWhen do we use a correlogram and what do we look for?\nCorrelogram\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series. The ACF is a measure of the correlation between the time series and a lagged version of itself.\nNotice that the values in the correlogram of the stock prices start at 1 and slowly decay as k increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk.\n\nHistogram\n\nFigure 5 is a histogram of the differences. This is a simple measure of volatility of the stock, or in other words, how much the price changes in a day.\n\n\n\nDifference Operator\n\nDifferencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n\n\n\n\n\n\nDefinition of the Difference Operator\n\n\n\nThe difference operator, \\(\\nabla\\), is defined as:\n\\[\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t\\]\nHigher-order differencing can be denoted\n\\[\\nabla^n x_t = (1-\\mathbf{B})^n x_t\\]\n\n\nThings to do - Do excel workout and link to this. So in the website for this. This lesson will be nother tab. Maybe add the option to download the excel sheet.\nComputing Differences Small group acitivity The difference operator can be helpful in identifying the functional underpinnings of a trend. If a function is linear, then the first differences of equally-spaced values will be constant. If a function is quadratic, then the second differences of equally-spaced values will be constant. If a function is cubic, then the third differences of equally-spaced values will be constant, and so on.\ndifferencing Stock Prices do this group activity\nIntegrated Autoregressive Model do this group activity\nClass Activyt: Random Walk Drift do this class activity"
  },
  {
    "objectID": "index.html#lesson-4.3-autoregressive-ar-models",
    "href": "index.html#lesson-4.3-autoregressive-ar-models",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.3 Autoregressive (AR) Models",
    "text": "Lesson 4.3 Autoregressive (AR) Models\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\n\nProperties of an AR(p) Stochastic Process\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\nExploring AR(1) Models\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\nSecond-Order Properties of an AR(1) Model\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\nCorrelogram of an AR(1) Model\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do\n\nDO group activity: Simulation of an AR(1) process\n\n\n\nPartial Autocorrelation\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\nPartial Autocorrelation Plots of Various AR(p) Processes\nLook at lesson shinny code\n\n\nSationary and Non-Stationary AR Processes\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\n\n\nAbsolute Value in the Complex Plane\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\nQuestions\n\nWhat is an exponential smoothing model?\n\n\n\nSearch for words for lesson 4.3\nexponential smoothing model - polyroot function -"
  },
  {
    "objectID": "index.html#lesson-5.1-unassigned-sections",
    "href": "index.html#lesson-5.1-unassigned-sections",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 5.1 unassigned sections",
    "text": "Lesson 5.1 unassigned sections\n\nGeneralized Least Squares (GLS)\n\nThe autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS â€“ the indominable Generalized Least Squares algorithm!\nGeneralized Least Squares (GLS) Lesson 5.1\n\nGeneralized Least Squares (GLS) is a method for estimating the unknown parameters in a linear regression model.\nGLS is used when the errors in a regression model are correlated.\nGLS is used when the errors in a regression model are heteroskedastic.\nGLS is used when the errors in a regression model are autocorrelated.\nGLS is used when the errors in a regression model are non-normal.\n\n\n\n\nAdditive Seasonal Indicator Variables\n\nadditive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\n\\[\n  x_t = m_t + s_t + z_t\n\\]\nwhere \\[\n  s_t =\n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      â‹®~~~~ & ~~~~~~~~~~~~â‹® \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s\n    \\end{cases}\n\\] and \\(s\\) is the number of seasons in one cycle/period, and \\(n\\) is the number of observations, so \\(t = 1, 2, \\ldots, n\\) and \\(i = 1, 2, \\ldots, s\\), and \\(z_t\\) is the residual error series, which can be autocorrelated.\nIt is important to note that \\(m_t\\) does not need to be a constant. It can be a linear trend:\n\nSeasonal indicator variable\n\nlesson5.1\n\nWe will create a linear model that includes a constant term for each month. This constant monthly term is called a seasonal indicator variable.\nThis name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented.\nIndicator variables are also called dummy varaibles."
  }
]