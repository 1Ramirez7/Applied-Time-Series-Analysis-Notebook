[
  {
    "objectID": "projects/project3.html",
    "href": "projects/project3.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Project 3",
      "Project title here"
    ]
  },
  {
    "objectID": "projects/project1.html",
    "href": "projects/project1.html",
    "title": "Time Series: China Export Commodities",
    "section": "",
    "text": "In today’s global economy, analyzing time series data is essential for predicting trends, understanding seasonality, and identifying unexpected shocks. The objective of this analysis is to explore China’s export commodities over time, using time series analysis techniques to gain insights that can aid in strategic planning and effective decision-making.\n\n\n\nTime series analysis involves collecting and interpreting data recorded over time. By examining China’s export data from early 1990 to the present, we can observe trends, seasonality, and irregular movements that are crucial for understanding the dynamics of international trade.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#introduction",
    "href": "projects/project1.html#introduction",
    "title": "Time Series: China Export Commodities",
    "section": "",
    "text": "In today’s global economy, analyzing time series data is essential for predicting trends, understanding seasonality, and identifying unexpected shocks. The objective of this analysis is to explore China’s export commodities over time, using time series analysis techniques to gain insights that can aid in strategic planning and effective decision-making.\n\n\n\nTime series analysis involves collecting and interpreting data recorded over time. By examining China’s export data from early 1990 to the present, we can observe trends, seasonality, and irregular movements that are crucial for understanding the dynamics of international trade.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#data-preparation",
    "href": "projects/project1.html#data-preparation",
    "title": "Time Series: China Export Commodities",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nShow the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, data.table, plotly)\n\n# Data Import and Preparation\ncexports_ts &lt;- rio::import(\"../data/CHNXTEXVA01NCMLM.csv\") |&gt; \n  mutate(\n    CHNXTEXVA01NCMLM = as.numeric(gsub(\",\", \"\", CHNXTEXVA01NCMLM)), # Remove commas and convert to numeric\n    dates = mdy(date),\n    year = lubridate::year(dates), \n    month = lubridate::month(dates), \n    value = as.numeric(CHNXTEXVA01NCMLM)\n  ) |&gt;\n  dplyr::select(dates, year, month, value)  |&gt;\n  arrange(dates) |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index) |&gt;\n  dplyr::select(index, dates, year, month, value) |&gt;\n  rename(exports_commodities = value) |&gt;\n  mutate(exports_commodities = exports_commodities / 1000)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#chinas-export-trade-commodities-time-series-plot",
    "href": "projects/project1.html#chinas-export-trade-commodities-time-series-plot",
    "title": "Time Series: China Export Commodities",
    "section": "China’s Export Trade Commodities Time Series Plot",
    "text": "China’s Export Trade Commodities Time Series Plot\n\n\nShow the code\nplain_plot &lt;- ggplot(cexports_ts, aes(x = dates, y = exports_commodities)) +\n  geom_rect(aes(xmin = as.Date(\"2008-01-01\"), xmax = as.Date(\"2010-01-01\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_rect(aes(xmin = as.Date(\"2020-02-01\"), xmax = as.Date(\"2020-04-01\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_line() +\n  geom_rect(aes(xmin = as.Date(\"2015-04-10\"), xmax = as.Date(\"2015-04-28\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_line() +\n  labs(x = \"\", y = \"Exports Commodities (Billions)\", title = \"Fig 1 - China: Exports Commodities Time Series\") +\n  scale_y_continuous(limits = range(cexports_ts$exports_commodities, na.rm = TRUE)) +\n    scale_x_yearmonth(\n    labels = scales::label_date(\"%Y %b\"),  # Format labels as \"Jan 2020\"\n    breaks = \"36 months\"                    # Show labels every 8 months\n  ) + \n  theme(plot.title = element_text(hjust = 0.5))\n\nplain_plot\n\n\n\n\n\nFig 1 - China’s Exports Commodities Time Series\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig1_exports_commodities_plot.png\", plot = plain_plot, width = 12, height = 6, dpi = 300)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#time-series-decomposition",
    "href": "projects/project1.html#time-series-decomposition",
    "title": "Time Series: China Export Commodities",
    "section": "Time Series Decomposition",
    "text": "Time Series Decomposition\n\nUnderstanding Decomposition\nTime series decomposition breaks down data into three components: trend, seasonality, and irregularities. This helps us understand underlying patterns and make better forecasts.\n\n\nWhy Use a Multiplicative Decomposition?\nThe upward trend in China’s export data suggests that fluctuations in seasonality and irregularities increase in magnitude as exports grow. That’s why we use a multiplicative classical decomposition model, where the data is broken down into:\n\nTrend Component: Captures the long-term growth pattern. Here, we observe steady growth in exports.\nSeasonal Component: Identifies repeating patterns within a year. Exports peak in December (possibly driven by holiday demand, especially in the US) and dip to their lowest in February (likely due to the Chinese New Year and post-holiday slowdowns).\nRandom Component: Captures what’s left after accounting for the trend and seasonal components. It represents what the model cannot explain. These unexpected changes often correlate with macroeconomic events like the 2008 Global Financial Crisis, but not all irregular variations can be directly linked to specific events.\n\n\n\nMultiplicative Classical Decomposition Model\n\n\nShow the code\nchina_decompose_mult &lt;- cexports_ts |&gt;\n  model(feasts::classical_decomposition(exports_commodities, type = \"mult\"))  |&gt;\n  components()\n\ndecompose_plot &lt;- autoplot(china_decompose_mult) +\n  ggtitle(\"Fig 2 - Classical Multiplicative Decomposition of China's Exports\") +\n  labs(subtitle = \"Export commodities = trend (billions) * seasonal * random\")\n\ndecompose_plot\n\n\n\n\n\nFig 2 - Classical Multiplicative Decomposition of China’s Exports\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig2_mult_decomposition.png\", plot = decompose_plot, width = 12, height = 6, dpi = 300)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#seasonal-component",
    "href": "projects/project1.html#seasonal-component",
    "title": "Time Series: China Export Commodities",
    "section": "Seasonal Component",
    "text": "Seasonal Component\n\n\nShow the code\nplot_decomp_seasonal &lt;- ggplot(china_decompose_mult %&gt;% filter(index &gt;= yearmonth('2019 Dec') & index &lt;= yearmonth('2021 Jan')), aes(x = index, y = seasonal)) +\n  geom_line(group = 1, color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Fig 3 - China: Exports Commodities Seasonal Component\",\n       x = \"Month\",\n       y = \"Seasonal\") +\n  scale_x_yearmonth(\n    labels = scales::label_date(\"%b\"),  \n    breaks = \"1 months\"                    \n  )  + \n  theme(plot.title = element_text(hjust = 0.5))\n\nplot_decomp_seasonal\n\n\n\n\n\nFig 3 - China’s Exports Commodities Seasonal Component\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig3_seasonal_component.png\", plot = plot_decomp_seasonal, width = 12, height = 6, dpi = 300)\n\n\n\nRandom Component and Macroeconomic Events\nThese unexpected changes often correlate with macroeconomic events like the 2008 Global Financial Crisis. For example, in response to a slowdown (random shock), China increased export tax rebates in March 2015 to stimulate exports, showing how responses can occur after the initial shock. This randomness highlights inherent uncertainties—macroeconomic disruptions may explain some shocks, but many remain unpredictable. Understanding the random component helps us see where patterns could align—but also where uncertainties remain.\n\n\nShow the code\nhighlight_months &lt;- c(\"1992 Oct\", \"1994 Jan\", \"1994 Apr\", \"1994 Sep\", \n                      \"1995 Dec\", \"1997 Jul\", \"1998 Jan\", \"1999 Apr\", \n                      \"1999 Nov\", \"2001 Dec\", \"2003 Apr\", \"2004 Jul\", \n                      \"2005 Jul\", \"2008 Sep\", \"2008 Nov\", \"2009 Jun\", \n                      \"2010 May\", \"2010 Dec\", \"2015 Mar\", \"2017 Jan\", \n                      \"2018 Jul\", \"2019 Oct\", \"2019 Dec\", \"2020 Jan\", \n                      \"2020 Mar\")\n\nhighlight_dates &lt;- tsibble::yearmonth(highlight_months)\n\nplot_random_component &lt;- ggplot(china_decompose_mult %&gt;% \n                                  filter(index &gt;= yearmonth('1992 Jul') & \n                                         index &lt;= yearmonth('2023 Dec')), \n                                aes(x = index, y = random)) +\n  geom_line(group = 1, color = \"blue\") +\n  geom_point(data = china_decompose_mult %&gt;% filter(index %in% highlight_dates),\n             aes(x = index, y = random),\n             shape = 24, color = \"red\", fill = \"red\", size = 4) + \n  labs(title = \"Fig 4 - Exports Commodities Random Component vs Macroeconomic Events\",\n       x = \"\",\n       y = \"Random\") +\n  scale_x_yearmonth(\n    labels = scales::label_date(\"%Y %b\"),  \n    breaks = \"36 months\"                    \n  )\n\nplot_random_component\n\n\n\n\n\nFig 4 - Exports Commodities Random Component vs Macroeconomic Events\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig4_random_vs_macroeventns.png\", plot = plot_random_component, width = 12, height = 6, dpi = 300)\n\n\n\n\nWhy a Multiplicative Decomposition Model Fits\nThe multiplicative model is particularly suitable because the amplitude of seasonal changes and irregular fluctuations increases along with the trend. In China’s export data, as exports grow, the seasonal variations and irregularities also become more pronounced. A multiplicative model accounts for this proportionality, making it appropriate for capturing the dynamics of the data.\n\n\nWhy an Additive Decomposition Model Would Not Work\nAn additive model assumes that the size of seasonal and irregular variations remains constant regardless of the trend. However, the data shows that fluctuations grow as the trend grows. Therefore, an additive model would fail to accurately represent the increasing variability linked to higher export levels.\n\n\nAdditive Classical Decomposition Model Plot\n\n\nShow the code\nchina_decompose_add &lt;- cexports_ts |&gt;\n  model(feasts::classical_decomposition(exports_commodities, type = \"add\"))  |&gt;\n  components()\n\nautoplot(china_decompose_add) +\n  ggtitle(\"Fig 5 - Classical Additive Decomposition of China's Exports\")\n\n\n\n\n\nFig 5 - Classical Additive Decomposition of China’s Exports",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#key-takeaways",
    "href": "projects/project1.html#key-takeaways",
    "title": "Time Series: China Export Commodities",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSeasonal Patterns: Understanding these can help companies optimize their supply chains, especially during high-demand months.\nEconomic Resilience: The trend shows that China’s export sector has consistently been increasing over time.\nStrategic Planning: By anticipating seasonal lows (like in February) and leveraging peaks (in December), businesses can better align their operations with demand cycles.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#recommendations",
    "href": "projects/project1.html#recommendations",
    "title": "Time Series: China Export Commodities",
    "section": "Recommendations",
    "text": "Recommendations\nGiven the observed seasonality, trend, and irregular components of China’s export data, it is important to prepare for upcoming political events that could potentially lead to a temporary fall in exports. However, based on historical patterns, China has demonstrated resilience and the ability to recover fairly quickly from these disruptions. Therefore, we recommend the following actionable items:\n\nDiversify Supply Chains: Identify and establish relationships with alternative suppliers and markets to reduce dependence on China during potential disruptions.\nIncrease Inventory Buffer: Build up inventory reserves during periods of high export activity to prepare for potential downturns caused by political events.\nMonitor Key Indicators: Closely monitor geopolitical developments and economic indicators to respond proactively to any signals of impending disruptions.\nEstablish Contingency Plans: Develop and maintain contingency plans to address short-term declines in supply, including logistical adjustments and finding new suppliers.\nCommunicate with Partners: Maintain open communication with Chinese suppliers and partners for early warnings of potential issues.\nLeverage Seasonal Patterns: Use the identified seasonal peaks to strategically schedule production and export needs, ensuring that reserves are built up during times of predictable high export activity.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#conclusion",
    "href": "projects/project1.html#conclusion",
    "title": "Time Series: China Export Commodities",
    "section": "Conclusion",
    "text": "Conclusion\nThe decomposition of China’s export data reveals a strong upward trend, along with predictable seasonal fluctuations and irregular disruptions. While upcoming political events may lead to short-term declines, historical data shows that these are typically followed by rapid recovery. By understanding these trends, seasonality, and random components, businesses can better prepare for potential disruptions without significant long-term impacts.\nThe key insight is that while risks are present, proactive preparation can mitigate their effects. By leveraging strategic planning, maintaining communication, and building contingency buffers, companies can continue to operate smoothly and minimize disruptions. This approach ensures resilience in the face of political uncertainties, relying on the observed adaptability of China’s export capabilities.\nWhat’s Next? I’ll continue refining the analysis and exploring potential forecast models to enhance predictive capabilities.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Time Series Analysis Notebook",
    "section": "",
    "text": "I’m first going to work out this index file, to have the whole picture then I will branch out base on information here. As I branch out information here will be compress and have links to the lessons and samples."
  },
  {
    "objectID": "index.html#stationary-time-series",
    "href": "index.html#stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Stationary Time Series",
    "text": "Stationary Time Series\n\nSationarity of Linear Models Lesson 5.1\n\nLinear models for time series are non-stationary when they include functions of time.\n\nDifferencing can often transform a non-stationary series with a de terministic trend to a stationary series (4.1).\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences (4.2)."
  },
  {
    "objectID": "index.html#non-stationary-time-series",
    "href": "index.html#non-stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Non-Stationary Time Series",
    "text": "Non-Stationary Time Series\n\nNon-Stationary Time Series Lesson 5.2\n\nA time series with a stochastic trend is non-stationary.\nA time series with a deterministic trend is non-stationary.\nA time series with a seasonal component is non-stationary.\nA time series with a unit root is non-stationary."
  },
  {
    "objectID": "index.html#lesson-4.1-white-noise-and-random-walks",
    "href": "index.html#lesson-4.1-white-noise-and-random-walks",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.1 White Noise and Random Walks",
    "text": "Lesson 4.1 White Noise and Random Walks\n\nStochastic Process 4.1\nStochastic processes or random processes evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\n\nDiscrete White Noise DWN 4.1\n\n4.2: Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\nA time series \\(\\{w_t: t = 1, 2, \\ldots, n\\}\\) is a discrete white noise (DWN) if the variables \\(w_1, w_2, \\ldots, w_n\\) are independent and identically distributed with mean 0.\n\nSecond-Order Properties of DWN\n\nWhen we refer to the second-order properties of a time series, we are talking about its variance and covariance. The variance of a DWN is constant, and the covariance between any two observations is zero\nThe mean is a first-order property, the covariance is a second-order property.\n\n\n\nDiscrete White Noise Process\n\nA DWN process will have the following properties:\n\nThere is a discrete observations.\nThe mean of the observations is zero.\nThe variance of the observations is finite.\nSuccessive observations are uncorrelated.\n\n\n\n\n\nRandom Walk\n\nRandom Walks Lesson 4.1\n\nA random walk is a stochastic process in which the difference between each observation is a white noise process, a non-stationary time series. (see def)\nwt is a dwn and often model as gwn, however wt could be as simple as a coin toss (random walk).\n\n\n\nProperties of Random Walk or walks\nor First-Order Properties of A Random Walk - The mean of a random walk series is 0.\nLook at shinny code for this\n\n\nSecond-Order Properties of a Random Walk\n\nCovariance: \\(cov(x_t,x_{t+k})\\):\nThe covariance between two values of the series depends on ( t ):\n\\[\n\\text{cov}(x_t, x_{t+k}) = t \\sigma^2\n\\]\nCorrelation Function \\(\\rho_k\\):\nThe correlation for lag  k  is:\n\\[\n\\rho_k = \\frac{1}{\\sqrt{1 + \\frac{k}{t}}}\n\\]\nNon-Stationarity:\nThe variance of the series increases with ( t ), making the random walk non-stationary.\nCorrelogram Characteristics:\nThe correlogram of a random walk typically shows:\n\nPositive autocorrelations starting near 1.\nA slow decrease as ( k ) increases.\n\n\n\n\n\nGaussian White Noise GWN 4.1\n\nIf the variables are normally distributed, i.e. \\(w_i \\sim N(0,\\sigma^2)\\), the DWN is called a Gaussian white noise process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n\n\nWhite Noise Time Series\n\nWhite Noise Time Series Lesson 4.1\n\nA white noise time series is a sequence of random variables that are uncorrelated and have a mean of zero.\nA white noise time series has a constant variance.\nA white noise time series has a constant mean.\nA white noise time series has a constant autocorrelation of zero for all lags except when the lag is zero.\n\n\n\n\nCorrelogram 4.1\n\nCorrelogram Lesson 4.1\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series.\nEach correlogram lag tests for correlation significance, increasing the chance of Type I error, resulting in potentially misleading conclusions about significant relationships.\n\n\n\n\nFitting the White Noise Model\n\n\nBackward Shift Operator\nWe define the backward shift operator or the lag operator, \\(\\mathbf{B}\\), as: \\[\n  \\mathbf{B} x_t = x_{t-1}\n\\] where \\(\\{x_t\\}\\) is any time series.\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\\[\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n\\]\n\nProperties of the Backshift Operator\nThe backwards shift operator is a linear operator. So, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants, then \\[\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n\\] The distributive property also holds. \\[\\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t\n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\\]\n.\n\n\n\nsearch words for lesson 4.1\nGaussian white noise - GWN - discrete white noise - dwn - variance - covariance - correlation - correlogram - Type I error - histogram - backward shift operator - backshift operator"
  },
  {
    "objectID": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "href": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.2 White Noise and Random Walks - Part 2",
    "text": "Lesson 4.2 White Noise and Random Walks - Part 2\n\nDifferecing a Time Series\nWhy do we difference a time series? Differencing a time series can help us to remove the trend and make the series stationary.\n\nComputing the difference between successive terms of a random walk leads to a discrete white noise series.\n\n\\[\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\\]\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences.\n\n\nCorrelograms & Histogram\nWhen do we use a correlogram and what do we look for?\nCorrelogram\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series. The ACF is a measure of the correlation between the time series and a lagged version of itself.\nNotice that the values in the correlogram of the stock prices start at 1 and slowly decay as k increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk.\n\nHistogram\n\nFigure 5 is a histogram of the differences. This is a simple measure of volatility of the stock, or in other words, how much the price changes in a day.\n\n\n\nDifference Operator\n\nDifferencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n\n\n\n\n\n\nDefinition of the Difference Operator\n\n\n\nThe difference operator, \\(\\nabla\\), is defined as:\n\\[\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t\\]\nHigher-order differencing can be denoted\n\\[\\nabla^n x_t = (1-\\mathbf{B})^n x_t\\]\n\n\nThings to do - Do excel workout and link to this. So in the website for this. This lesson will be nother tab. Maybe add the option to download the excel sheet.\nComputing Differences Small group acitivity The difference operator can be helpful in identifying the functional underpinnings of a trend. If a function is linear, then the first differences of equally-spaced values will be constant. If a function is quadratic, then the second differences of equally-spaced values will be constant. If a function is cubic, then the third differences of equally-spaced values will be constant, and so on.\ndifferencing Stock Prices do this group activity\nIntegrated Autoregressive Model do this group activity\nClass Activyt: Random Walk Drift do this class activity"
  },
  {
    "objectID": "index.html#lesson-5.1-unassigned-sections",
    "href": "index.html#lesson-5.1-unassigned-sections",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 5.1 unassigned sections",
    "text": "Lesson 5.1 unassigned sections\n\nGeneralized Least Squares (GLS)\n\nThe autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS – the indominable Generalized Least Squares algorithm!\nGeneralized Least Squares (GLS) Lesson 5.1\n\nGeneralized Least Squares (GLS) is a method for estimating the unknown parameters in a linear regression model.\nGLS is used when the errors in a regression model are correlated.\nGLS is used when the errors in a regression model are heteroskedastic.\nGLS is used when the errors in a regression model are autocorrelated.\nGLS is used when the errors in a regression model are non-normal.\n\n\n\n\nAdditive Seasonal Indicator Variables\n\nadditive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\n\\[\n  x_t = m_t + s_t + z_t\n\\]\nwhere \\[\n  s_t =\n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      ⋮~~~~ & ~~~~~~~~~~~~⋮ \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s\n    \\end{cases}\n\\] and \\(s\\) is the number of seasons in one cycle/period, and \\(n\\) is the number of observations, so \\(t = 1, 2, \\ldots, n\\) and \\(i = 1, 2, \\ldots, s\\), and \\(z_t\\) is the residual error series, which can be autocorrelated.\nIt is important to note that \\(m_t\\) does not need to be a constant. It can be a linear trend:\n\nSeasonal indicator variable\n\nlesson5.1\n\nWe will create a linear model that includes a constant term for each month. This constant monthly term is called a seasonal indicator variable.\nThis name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented.\nIndicator variables are also called dummy varaibles."
  },
  {
    "objectID": "chapters/chapter_5.html",
    "href": "chapters/chapter_5.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 5. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_2.html",
    "href": "chapters/chapter_4_lesson_2.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4.html",
    "href": "chapters/chapter_4.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_3.html",
    "href": "chapters/chapter_3.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_3_overview_code_draft.html",
    "href": "chapters/chapter_3_overview_code_draft.html",
    "title": "Chapter 3 r code examples and practice",
    "section": "",
    "text": "This qmd is made to summarize chapter 3 and have models for the chapter to view.\n\nI have a goal to make the code not be reliant on adjusting variable names and titles.\n\nThis will require having an initial r chunk that assigns variable names and dataset.\nI will still have multiple df.\n\nThis will be a bit hard and the models will have to be pretty general because some data sets required different approaches\n\nI think maybe I can also assign if its yearmonth, yearquater etc, but that will require so many if statements.\n\nIdk how I’m going to approach this\n\n\n\n\n\nCode\n# Assign your column names to variables\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n\ndates &lt;- \"date\"\nx_col &lt;- \"constructionequip_ord\"\ny_col &lt;- \"constructionequip_ship\"\n\n\n\n# Assign plot labels\nx_label &lt;- \"Month\"\ny_label &lt;- \"New Orders & Value of Equip\"\nplot_title &lt;- \"Time Series of Construction Equip: New Orders & Equipment\"\n\n\n\n\nCode\ndf &lt;- df0 |&gt;\n  mutate(\n    date = lubridate::mdy(.data[[dates]]),\n    x = as.numeric(.data[[x_col]]), # Convert and rename to x\n    y = as.numeric(.data[[y_col]])  # Convert and rename to y\n  ) |&gt;\n  select(date, x, y) \n\n\n\n\nCode\ndf1 &lt;- df |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(date, obs, x, y)\n\ndfx &lt;- df |&gt; # lone df for variable x = ord\n  mutate(obs = row_number()) |&gt; \n  select(date, x)\n\ndfy &lt;- df |&gt; # lone df for y = ship\n  mutate(obs = row_number()) |&gt; \n  select(date, y)\n\n\ntest\n\n\nCode\n# this is code for hw 3-1\n\n# this code is same as the dfx2 & dfy2 code\ndf1 &lt;- df1 |&gt;\n  mutate(index = tsibble::yearmonth(date)) |&gt; # 3.1\n  as_tsibble(index = index) |&gt;\n  select(index, date, x, y)\n\n\n\nautoplot(df1, .vars = x) +\n  geom_line(data = df1, aes(x = index, y = y), color = \"#E69F00\") +\n  labs(\n    x = x_label,\n    y = y_label,\n    title = plot_title\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "r code Models draft",
      "Chapter 3 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1.html",
    "href": "chapters/chapter_4_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4_overview_code_draft.html",
    "href": "chapters/chapter_4_overview_code_draft.html",
    "title": "Chapter 4 r code examples and practice",
    "section": "",
    "text": "spacer\n\n\nCode\n# 1\n\n\nfile_path &lt;- \"../data/white_noise.parquet\"\n\n# ../data goes back one level. ../../data goes back two folders\n\n\n\n\nCode\n# straight from lesson code\n# This code was used to create the white noise data file\n\n# Set random seed\nset.seed(10)\n\n# Specify means and standard deviation\nn &lt;- 2500                           # number of points\nwhite_noise_sigma &lt;- rnorm(1, 5, 1) # choose a random standard deviation\n\n# Simulate normal data\n# data.frame(x = rnorm(n, 0, white_noise_sigma)) |&gt;\n#   rio::export(\"data/white_noise.parquettest\") # uncomment this two lines to run\n\n\n\n\nCode\n# 2\n\n\n# White noise data\ndf0 &lt;- rio::import(file_path)\n\n\nname changes\nchunk 3: white_noise_df = df0\nchunk 4: x = v1\n\n\nCode\n# 3\n# words b/ code: The first 250 points in this time....\n\ndf1 &lt;- df0 |&gt; # this code updates names, but now t = x. and x is y in latter codes so keep that in mind.\n  mutate(t = 1:nrow(df0)) |&gt;\n  rename(x = t, v1 = x) # rename this, original code had those names for x and y. now just x and y\n\ndf1 |&gt; # original, but this code does not rename or change df. \n  head(250) |&gt;  \n  ggplot(aes(x = x, y = v1)) + \n    geom_line() +\n    theme_bw() +\n    labs(\n      x = \"Time\",\n      y = \"Values\",\n      title = \"First 250 Values of a Gaussian White Noise Time Series\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nCode\n  acf(df1$v1, type = \"covariance\") # doing this in class to get to the density plot below\n\n\n\n\n\n\n\n\n\nCode\n  acf(df1$v1, type = \"correlation\") # use this acf samples from previous lesson. \n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# 4.1 - 4\n# words before code: Here is a histogram of the 2500 values from....\n\n# this x is the variable, but not necessarily the x axis variable. x is y here \ndf1 |&gt;\n  mutate(density = dnorm(v1, mean(df1$v1), sd(df1$v1))) |&gt;\n  ggplot(aes(x = v1)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n    geom_line(aes(x = v1, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values or variable 1\",\n      y = \"Frequency\",\n      title = \"Histogram of Values from a Gaussian White Noise Process\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nCode\n# left off in this histogram code because the differences between the y and x names is confusing. I need to just hard edit the code to clearly define the variable and avoid the name changes. I just need to make sure I take note of it.\n\n\nRandom Walk Cumulative Sum\nname changes\ny = v2\nthe x in this code refers to the x - axis values. This code is meant for when the x axis values are number of observations (eg. 1-60). Dates can maybe work, but anything else can cause troubles.\n\n\nCode\n# 4.1 - 5\n# sample code to simulate a random walk\n# words b/ code: Complete steps 2 and 3 a total of\n\n# set.seed(7)\n\ndf2 &lt;- df1 |&gt;\n  # mutate(w = ifelse(row_number() == 1, 0, sample(c(-1,1), size = 60, replace = TRUE))) |&gt; # generates coin flips, but no longer needed for chapter model\n  mutate(v2 = cumsum(v1)) # creates cumulitve v2 column\n\nggplot(data=df2, aes(x=x, y=v2)) +\n  # geom_point(data = df2, aes(x=x, y=v2), size = 0.01) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  # scale_x_continuous(limits = c(0,60), # limits to only 60 obs\n  #                    breaks = seq(0, 60, by = 5),\n  #                    minor_breaks = seq(0, 60, 1)) +\n  # scale_y_continuous(limits = c(-20,20),\n  #                    breaks = seq(-20, 20, by = 5),\n  #                    minor_breaks = seq(-20, 20, 1)) +\n  labs(\n      x = \"Toss Number\",\n      y = expression(paste(\"$x_t$\")),\n      title = \"Cumulative Results of Coin Tosses\" # cum results (v2) of v1. \n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(colour = \"black\")\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# 4.1 - 6 \n# words b/ code: be a time series with the following values.\n\nset.seed(6)\nn &lt;- 8\nd_operator &lt;- data.frame(t = c(1:n), x = sample(1:15, n, replace = FALSE)) |&gt;\n  mutate(diff = t - n) # what is this code doing. right now is its doing t - n. n is always the set 8, and t is just the number of observations. so if n is 8, then the first t- n is -7, second is -6 and so on, but this is just using the number/date assigned to the actual variable. its like comparing hot days when only using the data, but not the temperature. so what is this code doing exactly????\n\n#cat( paste( paste0(\"$x_{t\", ifelse(d_operator$t==n,\"\",d_operator$t-n), \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\ncat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\n# Computes the value of the \"power_on_d\"^th difference from x_n\nd_value &lt;- function(power_on_d = 0) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(diff == -power_on_d) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n\nts_val &lt;- function(t_value) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(t == t_value) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n# this code below was the last r chunk for lesson 4-1, but it is not needed since it is in this r chunk. This r chunk is set not to evaluate for class. \n\n#cat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) )\n\n\nspacer\nThis is solution to backwards shift operator so formula for this code is done in a previous r chunk.",
    "crumbs": [
      "r code Models draft",
      "Chapter 4 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_4_overview_code_draft.html#visualizing-white-noise",
    "href": "chapters/chapter_4_overview_code_draft.html#visualizing-white-noise",
    "title": "Chapter 4 r code examples and practice",
    "section": "",
    "text": "spacer\n\n\nCode\n# 1\n\n\nfile_path &lt;- \"../data/white_noise.parquet\"\n\n# ../data goes back one level. ../../data goes back two folders\n\n\n\n\nCode\n# straight from lesson code\n# This code was used to create the white noise data file\n\n# Set random seed\nset.seed(10)\n\n# Specify means and standard deviation\nn &lt;- 2500                           # number of points\nwhite_noise_sigma &lt;- rnorm(1, 5, 1) # choose a random standard deviation\n\n# Simulate normal data\n# data.frame(x = rnorm(n, 0, white_noise_sigma)) |&gt;\n#   rio::export(\"data/white_noise.parquettest\") # uncomment this two lines to run\n\n\n\n\nCode\n# 2\n\n\n# White noise data\ndf0 &lt;- rio::import(file_path)\n\n\nname changes\nchunk 3: white_noise_df = df0\nchunk 4: x = v1\n\n\nCode\n# 3\n# words b/ code: The first 250 points in this time....\n\ndf1 &lt;- df0 |&gt; # this code updates names, but now t = x. and x is y in latter codes so keep that in mind.\n  mutate(t = 1:nrow(df0)) |&gt;\n  rename(x = t, v1 = x) # rename this, original code had those names for x and y. now just x and y\n\ndf1 |&gt; # original, but this code does not rename or change df. \n  head(250) |&gt;  \n  ggplot(aes(x = x, y = v1)) + \n    geom_line() +\n    theme_bw() +\n    labs(\n      x = \"Time\",\n      y = \"Values\",\n      title = \"First 250 Values of a Gaussian White Noise Time Series\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nCode\n  acf(df1$v1, type = \"covariance\") # doing this in class to get to the density plot below\n\n\n\n\n\n\n\n\n\nCode\n  acf(df1$v1, type = \"correlation\") # use this acf samples from previous lesson. \n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# 4.1 - 4\n# words before code: Here is a histogram of the 2500 values from....\n\n# this x is the variable, but not necessarily the x axis variable. x is y here \ndf1 |&gt;\n  mutate(density = dnorm(v1, mean(df1$v1), sd(df1$v1))) |&gt;\n  ggplot(aes(x = v1)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n    geom_line(aes(x = v1, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values or variable 1\",\n      y = \"Frequency\",\n      title = \"Histogram of Values from a Gaussian White Noise Process\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nCode\n# left off in this histogram code because the differences between the y and x names is confusing. I need to just hard edit the code to clearly define the variable and avoid the name changes. I just need to make sure I take note of it.\n\n\nRandom Walk Cumulative Sum\nname changes\ny = v2\nthe x in this code refers to the x - axis values. This code is meant for when the x axis values are number of observations (eg. 1-60). Dates can maybe work, but anything else can cause troubles.\n\n\nCode\n# 4.1 - 5\n# sample code to simulate a random walk\n# words b/ code: Complete steps 2 and 3 a total of\n\n# set.seed(7)\n\ndf2 &lt;- df1 |&gt;\n  # mutate(w = ifelse(row_number() == 1, 0, sample(c(-1,1), size = 60, replace = TRUE))) |&gt; # generates coin flips, but no longer needed for chapter model\n  mutate(v2 = cumsum(v1)) # creates cumulitve v2 column\n\nggplot(data=df2, aes(x=x, y=v2)) +\n  # geom_point(data = df2, aes(x=x, y=v2), size = 0.01) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  # scale_x_continuous(limits = c(0,60), # limits to only 60 obs\n  #                    breaks = seq(0, 60, by = 5),\n  #                    minor_breaks = seq(0, 60, 1)) +\n  # scale_y_continuous(limits = c(-20,20),\n  #                    breaks = seq(-20, 20, by = 5),\n  #                    minor_breaks = seq(-20, 20, 1)) +\n  labs(\n      x = \"Toss Number\",\n      y = expression(paste(\"$x_t$\")),\n      title = \"Cumulative Results of Coin Tosses\" # cum results (v2) of v1. \n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(colour = \"black\")\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# 4.1 - 6 \n# words b/ code: be a time series with the following values.\n\nset.seed(6)\nn &lt;- 8\nd_operator &lt;- data.frame(t = c(1:n), x = sample(1:15, n, replace = FALSE)) |&gt;\n  mutate(diff = t - n) # what is this code doing. right now is its doing t - n. n is always the set 8, and t is just the number of observations. so if n is 8, then the first t- n is -7, second is -6 and so on, but this is just using the number/date assigned to the actual variable. its like comparing hot days when only using the data, but not the temperature. so what is this code doing exactly????\n\n#cat( paste( paste0(\"$x_{t\", ifelse(d_operator$t==n,\"\",d_operator$t-n), \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\ncat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\n# Computes the value of the \"power_on_d\"^th difference from x_n\nd_value &lt;- function(power_on_d = 0) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(diff == -power_on_d) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n\nts_val &lt;- function(t_value) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(t == t_value) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n# this code below was the last r chunk for lesson 4-1, but it is not needed since it is in this r chunk. This r chunk is set not to evaluate for class. \n\n#cat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) )\n\n\nspacer\nThis is solution to backwards shift operator so formula for this code is done in a previous r chunk.",
    "crumbs": [
      "r code Models draft",
      "Chapter 4 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_5_lesson_1.html",
    "href": "chapters/chapter_5_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Applied Time Series Projects",
    "section": "",
    "text": "Applied Time Series Research\n\nChina Export Commodities Time Series Analysis:\n\nApplied time series techniques to analyze trends, seasonality, and irregularities in China’s export commodity data from 1990 to the present.\nConducted a classical multiplicative decomposition to distinguish long-term growth patterns, seasonal cycles, and random shocks influenced by macroeconomic events.\nProvided actionable insights into optimizing trade strategies by leveraging seasonal trends and mitigating risks from unpredictable disruptions.\n\n\n\n\nTime Series Reserach on China’s Export Commodities\n\n\nConsumer Credit Data Analysis:\n\nExamined consumer credit data from the G.19 Statistical Release, focusing on outstanding credit extended to individuals for household and personal expenditures.\nDifferentiated between revolving credit (e.g., credit card loans) and nonrevolving credit (e.g., motor vehicle and education loans) to identify distinct patterns and trends.\nAnalyzed the interest rates and terms of credit, including new car loans, personal loans, and credit card plans at commercial banks, and discussed the implications of historical data series.\nProvided insights into consumer borrowing behaviors and the potential impact of macroeconomic changes on credit markets.\n\n\n\nProject 2: Consumer Credit Data Analysis\n\n\nProject 3: Coming Soon\n\n\nproject 3\n\n\nConsumer Credit Data Analysis:\n\nExamined consumer credit data from the G.19 Statistical Release, focusing on outstanding credit extended to individuals for household and personal expenditures.\nDifferentiated between revolving credit (e.g., credit card loans) and nonrevolving credit (e.g., motor vehicle and education loans) to identify distinct patterns and trends.\nAnalyzed the interest rates and terms of credit, including new car loans, personal loans, and credit card plans at commercial banks, and discussed the implications of historical data series.\nProvided insights into consumer borrowing behaviors and the potential impact of macroeconomic changes on credit markets.\n\n\n\nProject 2: Consumer Credit Data Analysis\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Applied Time Series Projects"
    ]
  },
  {
    "objectID": "projects/project2.html",
    "href": "projects/project2.html",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "",
    "text": "Part I. What Consumer Credit Data Are Available on the G.19 Statistical Release, “Consumer Credit,” and How Are These Data Calculated?\nThe G.19 Statistical Release, “Consumer Credit,” reports outstanding credit extended to individuals for household, family, and other personal expenditures, excluding loans secured by real estate. Total consumer credit comprises two major types: revolving and nonrevolving. Revolving credit plans may be unsecured or secured by collateral and allow a consumer to borrow up to a prearranged limit and repay the debt in one or more installments. Credit card loans comprise most of revolving consumer credit measured in the G.19, but other types, such as prearranged overdraft plans, are also included. Nonrevolving credit is closed-end credit extended to consumers that is repaid on a prearranged repayment schedule and may be secured or unsecured. To borrow additional funds, the consumer must enter into an additional contract with the lender. Consumer motor vehicle and education loans comprise the majority of nonrevolving credit, but other loan types, such as boat loans, recreational vehicle loans, and personal loans, are also included.\nThe G.19 also reports selected terms of credit, including interest rates on new car loans, personal loans, and credit card plans at commercial banks. Historically, the G.19 also included series that measure the terms of credit for motor vehicle loans at finance companies. In the first quarter of 2011, publication of these series was temporarily suspended because of the deterioration of their statistical foundation. The statistical foundation is in the process of being improved, and publication will resume as soon as possible.\n\n\nCode\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n    mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n   select(ym, OCC, OCC_MoM) |&gt;\n   mutate(OCC = OCC / 1000) |&gt;\n   slice_head(prop = 1) # tail: last 10%. head: first or oldest\n#   slice((n() * 0.5):(n() * 0.6)) # select from 50% to 60%\n# interval(df) # gives interval: M, D ot Y etc\n# has_gaps(df) # false if none and vice versa True\n\n\np_cdebt &lt;- df |&gt; # fig 1\n  autoplot(OCC) +\n  labs(\n    x = \"Month\",\n    y = \"OCC\",\n    title = \"Fig 1 - Time Series: OCC\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np_cdebt_tail &lt;- df |&gt; # fig 1.1\n  slice_tail(prop = .1) |&gt;\n  autoplot(OCC) +\n  labs(\n    x = \"Month\",\n    y = \"OCC\",\n    title = \"fig 1.1 Time Series: OCC 2017-Present\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\np_cdebt_change &lt;- df |&gt;  # fig 1.2\n  autoplot(OCC_MoM) +\n  labs(\n    x = \"Month\",\n    y = \"MoM OCC\",\n    title = \"Fig 1.2 - Time Series: OCC_MoM\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\np_cdebt | p_cdebt_tail\n\n\n\n\n\n\n\n\n\n\n\nCode\n# fig 1.2\np_cdebt_change\n\n\n\n\n\n\n\n\n\nFigure 1 shows the time series of the Outstanding consumer debt. Figure 1.1 shows the same time series but form 2017 to lattest data. Figure 1.3 is the month over month(MoM) percent change in Outstanding Consumer Credit.\n\nNoticible\n\nThe percent change has more volatility, specially in the early years of the time series, something that the time series plot does not catch due to the scale of the y axis.",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#notes",
    "href": "projects/project2.html#notes",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "Notes",
    "text": "Notes\nI was looking to see what can cause the spikes in the Random component, and maybe incentives to take out loans? I have data on student loans totals so I can see if there was an actual increase.\nai: April 2021: Biden requested that Education Secretary Miguel Cardona review his executive authority to cancel student loan debt unilaterally. This move suggested he was considering executive action, although no specific details or amounts were confirmed at that time.\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD # Exponential Smoothing\n\n\nCode\n# mirror to resource code 3.4.1 exponential smoothing\n\nfig_3_4_1_B1 &lt;- autoplot(df) +\n    labs(x = \"ym\", y = \"OCC\")\n\n\nPlot variable not specified, automatically selected `.vars = OCC`\n\n\nCode\ndf1 &lt;- df |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Additive = ETS(OCC ~\n        trend(\"M\", alpha = 0.7849282, beta = 0.1282016) +\n        error(\"M\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\nSSE_1 &lt;- sum(components(df1)$remainder^2, na.rm = T)\n\nfig_3_4_1_B2 &lt;- augment(df1) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndf2 &lt;- df |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Additive = ETS(OCC ~\n        trend(\"M\", alpha = 0.7849282, beta = 0.1282016) +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\nSSE_2 &lt;-  sum(components(df2)$remainder^2, na.rm = T)\n\nfig_3_4_1_B1\n\n\n\n\n\n\n\n\n\nCode\nSSE_1\n\n\n[1] 0.1694887\n\n\nCode\nfig_3_4_1_B2\n\n\n\n\n\n\n\n\n\nCode\nSSE_2\n\n\n[1] 115763.8\n\n\nai notes:\n\nEvaluating Model Fit The sum of squared residuals (SSE) is a common metric to evaluate how well the model fits the data. Lower SSE values indicate that the model’s predictions are close to the actual data points. In exponential smoothing models, you might compare SSEs across different model configurations (e.g., different smoothing parameters, different trend or seasonal components) to select the model that minimizes error.\nOptimization and Model Selection When fitting ETS models, you often need to optimize parameters such as smoothing constants ( 𝛼 α, 𝛽 β, and 𝛾 γ). The optimization criterion “amse” (average mean squared error) minimizes the mean squared error, indirectly influencing SSE. Comparing SSEs between models (df1 and df2 in your example) can help you determine if adjusting a parameter (e.g., 𝛼 α from 0.1429622 to 0.2) results in a better model fit.\nDiagnostics and Model Improvement SSE can be a diagnostic tool to assess whether your chosen model structure (e.g., additive trend and additive seasonality) is suitable for your data. High residual sums (or unexplained variance) may suggest that the model lacks a component (like an interaction between trend and seasonality) or is mis-specified.\nStatistical Properties of the Residuals For an ETS model to be considered a good fit, residuals should ideally be white noise—meaning they are uncorrelated, normally distributed, and have zero mean. Analyzing the sum of squared residuals, along with residual plots, autocorrelation, and partial autocorrelation functions, provides insights into whether the residuals meet these assumptions.\n\nIn short, the SSE (sum of squared residuals) is a key metric in exponential smoothing to measure and compare model performance, guide parameter optimization, and assess the statistical quality of the model’s fit to time series data.\nspacer\n\n\nCode\n# mirror to resource code 3.4.2 \n\ndf342 &lt;- df \n\n# autoplot(df342) +\n#     labs(\n#         x = \"ym\",\n#         y = \"OCC\")\ndf342_2 &lt;- df342 |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"M\") +\n        error(\"M\") +\n        season(\"M\"), \n        opt_crit = \"amse\", nmse = 1))\n# `report(df342_2)\n# tidy(df342_2)\n# sum(components(df342_2)$remainder^2, na.rm = T)\n# accuracy(df342_2)$RMSE\n# sd(df342$OCC)\n# autoplot(components(df342_2))\n# augment(df342_2) |&gt;\n#     ggplot(aes(x = ym, y = OCC)) +\n#     geom_line() +\n#     geom_line(aes(y = .fitted, color = \"Fitted\")) +\n#     labs(color = \"\")\n\nreport(df342_2)\n\n\nSeries: OCC \nModel: ETS(M,M,M) \n  Smoothing parameters:\n    alpha = 0.7719996 \n    beta  = 0.07304847 \n    gamma = 0.2274828 \n\n  Initial states:\n     l[0]     b[0]     s[0]    s[-1]     s[-2]     s[-3]     s[-4]    s[-5]\n 56.69474 0.919114 1.084212 1.148118 0.9343098 0.9815824 0.9831539 1.010902\n     s[-6]     s[-7]    s[-8]     s[-9]   s[-10]    s[-11]\n 0.9273472 0.8630946 1.243133 0.9753093 1.026207 0.8226304\n\n  sigma^2:  0.0012\n\n     AIC     AICc      BIC \n10223.21 10224.01 10302.48 \n\n\nI tried doing NO seasonality (N) for the Season component but I was running into an error.",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#code-takes-for-ever-to-run",
    "href": "projects/project2.html#code-takes-for-ever-to-run",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "3.4.3 code takes for ever to run!!!!!!!!!!!!!!!!!!",
    "text": "3.4.3 code takes for ever to run!!!!!!!!!!!!!!!!!!\n\n\nCode\n# mirror to resource code 3.4.3\n\ndf343_0 &lt;-  df |&gt;\n  slice_head(prop = .94)\n\ndf343 &lt;-  df343_0 |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\np_343 &lt;- augment(df343) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\np_343_forecast &lt;- df343 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343) |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2020 Nov\"))) +\n  scale_color_discrete(name = \"\")\n\np_343\n\n\n\n\n\n\n\n\n\nCode\np_343_forecast\n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using dygraph for a more interactive time series\n#works great, but it does not use as.tsibble and converts to xts in order to work with dygraphs. \n# I think xts will cause errors with my data, need to what differes xts to as.tsibble.\n\ndf343_0 &lt;- df |&gt;\n  slice_head(prop = .94)\n\n# Fit the model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~\n    trend(\"A\") +\n    error(\"A\") +\n    season(\"A\"),\n    opt_crit = \"amse\", nmse = 1))\n\n# Generate fitted values\nfitted_data &lt;- augment(df343)\n\n# Convert the data to xts format for dygraph\nocc_xts &lt;- xts(fitted_data$OCC, order.by = as.Date(fitted_data$ym))\nfitted_xts &lt;- xts(fitted_data$.fitted, order.by = as.Date(fitted_data$ym))\n\n# Create a dygraph with actual and fitted values\ndygraph(cbind(Actual = occ_xts, Fitted = fitted_xts)) %&gt;%\n  dySeries(\"Actual\", label = \"Actual\") %&gt;%\n  dySeries(\"Fitted\", label = \"Fitted\") %&gt;%\n  dyOptions(colors = c(\"blue\", \"red\")) %&gt;%\n  dyRangeSelector() %&gt;%\n  dyLegend(show = \"always\")\n\n\n\n\n\n\nspacer\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using plotly  for a more interactive time series, but this code did not look great but leaving here for sample\n\n\n# Load required libraries\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, plotly)\n\n# Import data and convert to tsibble\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n  mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n  select(ym, OCC, OCC_MoM)\n\n# Create the initial slice of the dataset\ndf343_0 &lt;- df |&gt; slice_head(prop = .94)\n\n# Fit the ETS model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~ trend(\"A\") + error(\"A\") + season(\"A\"), opt_crit = \"amse\", nmse = 1))\n\n# Create the first interactive plot using Plotly\np_343 &lt;- augment(df343) |&gt;\n  ggplot(aes(x = ym, y = OCC)) +\n  geom_line() +\n  geom_line(aes(y = .fitted, color = \"Fitted\")) +\n  labs(color = \"\")\n\n# Convert ggplot object to plotly for interactivity\np_343_plotly &lt;- ggplotly(p_343)\np_343_plotly\n\n# Forecast and create the second interactive plot using Plotly\np_343_forecast &lt;- df343 |&gt;\n  forecast(h = \"2 years\") |&gt;\n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343) |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2020 Nov\"))) +\n  scale_color_discrete(name = \"\")\n\n# Convert the forecast plot to plotly\np_343_forecast_plotly &lt;- ggplotly(p_343_forecast)\np_343_forecast_plotly\n\n\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using plotly  for a more interactive time series\n# it is almost good but I need the y scale to autornage according to the area the plot is zooming in. \n# Load required libraries\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, plotly)\n\n# Import data and convert to tsibble\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n  mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n  select(ym, OCC, OCC_MoM)\n\n# Create the initial slice of the dataset\ndf343_0 &lt;- df |&gt; slice_head(prop = .94)\n\n# Fit the ETS model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~ trend(\"A\") + error(\"A\") + season(\"A\"), opt_crit = \"amse\", nmse = 1))\n\n# Generate fitted values\nfitted_data &lt;- augment(df343)\n\n# Prepare the data for Plotly\ndf_plot &lt;- fitted_data |&gt;\n  mutate(Date = as.Date(ym))\n\n# Create an interactive Plotly plot for actual and fitted values with dynamic y-axis\np_343_plotly &lt;- plot_ly(df_plot, x = ~Date) %&gt;%\n  add_lines(y = ~OCC, name = \"Actual\", line = list(color = 'blue')) %&gt;%\n  add_lines(y = ~.fitted, name = \"Fitted\", line = list(color = 'red')) %&gt;%\n  layout(\n    title = \"Actual vs Fitted Values\",\n    xaxis = list(\n      title = \"Date\",\n      rangeslider = list(visible = TRUE)\n    ),\n    yaxis = list(\n      title = \"OCC\",\n      autorange = TRUE\n    ),\n    legend = list(orientation = \"h\", x = 0.1, y = -0.2)\n  )\n\n# Display the interactive plot with dynamic y-axis scaling\np_343_plotly",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#using-full-df-and-doing-24-26-forecast",
    "href": "projects/project2.html#using-full-df-and-doing-24-26-forecast",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "using full df and doing 24-26 forecast",
    "text": "using full df and doing 24-26 forecast\n\n\nCode\n# mirror to resource code 3.4.3 w/ 2024-2026 forecast\n\n\ndf343_1_0 &lt;-  df |&gt;\n  slice_head(prop = 1)\n\ndf343_2 &lt;-  df343_1_0 |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\np_343_2 &lt;- augment(df343_2) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\np_343_2_forecast &lt;- df343_2 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2021 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343_2) |&gt; filter(ym &gt;= yearmonth(\"2021 Jan\") & ym &lt;= yearmonth(\"2024 Aug\"))) +\n  scale_color_discrete(name = \"\")\n\n\np_343_2\n\n\n\n\n\n\n\n\n\nCode\np_343_2_forecast\n\n\n\n\n\n\n\n\n\nspacer\nwould the season but ‘M’ because it is not as volatile as the time series goes on. Like in the beginning OCC_MoM has a good amount of volatility, so the season is properly going down over time? * Okay I did cut the first 20 years of thge df so what will happen if I include the whole dataset.\n======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; 41d67e11b64162a043bcde7f5be9624fcaf874ad",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  }
]