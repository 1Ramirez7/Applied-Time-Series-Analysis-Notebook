[
  {
    "objectID": "tools_help_ideas.html",
    "href": "tools_help_ideas.html",
    "title": "Tools, Resources and Help Ideas",
    "section": "",
    "text": "This page is to include pontential ideas to do for resources, tools, and ideas for time series",
    "crumbs": [
      "Tools, Help & Ideas",
      "Tools, Resources and Help Ideas"
    ]
  },
  {
    "objectID": "tools_help_ideas.html#footnotes",
    "href": "tools_help_ideas.html#footnotes",
    "title": "Tools, Resources and Help Ideas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhite noise consists of independent, identically distributed variables with mean 0, finite constant variance, and no correlation, representing purely random fluctuations over time.↩︎",
    "crumbs": [
      "Tools, Help & Ideas",
      "Tools, Resources and Help Ideas"
    ]
  },
  {
    "objectID": "tools/math_formulas.html",
    "href": "tools/math_formulas.html",
    "title": "Formulas",
    "section": "",
    "text": "2.1\n\\[\n\\gamma(x, y) = E[(x - \\mu_x)(y - \\mu_y)]\n\\]\n2.2\n\\[\nCov(x, y) = \\sum(x_i - \\bar{x})(y_i - \\bar{y}) / (n - 1)\n\\]\n2.3\nmean((x - mean(x))*(y - mean(y))) \\(\\to\\) \\(E[(x - \\mu_x)(y - \\mu_y)]\\)\n2.4\n\\[\np(x, y) = \\frac{E[(x - \\mu_x)(y - \\mu_y)]}{\\sigma_x\\sigma_y} = \\frac{\\gamma(x, y)}{\\sigma_x\\sigma_y}\n\\]\n2.5\n\\[\nCor(x, y) = \\frac{Cov(x, y)}{sd(x)sd(y)}\n\\]\n2.6\n\\[\n\\mu(t) = E[x_t]\n\\]\n2.7\n\\[\n\\bar{x} =  \\sum_{t=1}^n \\frac{x_t}{n}\n\\]\n2.8\n\\[\n\\lim_{ \\to \\infty} \\frac{\\sum x_t}{n} = \\mu\n\\]\n2.9\n\\[\n\\sigma^2(t) = E[(x_t - \\mu)^2]\n\\]\n2.10\n\\[\nVar(x) = \\frac{\\sum (x_t - \\bar{x})^2}{n - 1}\n\\]\ne2.11\n\\[\n\\gamma_k = E[(x_t - \\mu)(x_{t+k} - \\mu)]\n\\]\n2.12\n\\[\n\\rho_k = \\frac{\\gamma_k}{\\sigma^2}\n\\]\n2.13\n\\[\nc_k = \\frac{1}{n} \\sum_{t=1}^{n-k} (x_t - \\bar{x})(x_{t+k} - \\bar{x})\n\\]\n2.14\n\\[\nr_k = \\frac{c_k}{c_0}\n\\]\n2.15\nIn subsequent chapters, second-order properties for several time series models are derived using the result shown in Equation (2.15). Let x1,x2,…,xn and y1, y2,…,ym be random variables. Then\n\\[\nCov( \\sum_{i=1}^{n} x_i, \\sum_{j=1}^{m} y_j ) = \\sum_{i=1}^{n} \\sum_{j=1}^{m} Cov(x_i , y_j)\n\\]\nwhere \\(Cov(x,y)\\) is the covariance between a pair of random variables \\(x\\) and \\(y\\). The result tells us that the covariance of two sums of variables is the sum of all possible covariance pairs of the variables. Note that the special case of \\(n = m\\) and \\(x_i = y_i\\) (i = 1,…,n) occurs in subsequent chapters for a time series {\\(x_t\\)}.The proof of Equation (2.15) is left to Exercise 5a\nAutoregressive (AR) Model\n4.3 lesson (book reference 4.15)\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nExploring AR(1) Models Lesson 4.3.\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\nSecond-Order Properties of an AR(1) Model Lesson 4.3.\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\nCharacterisitc Equation\nLesson 4.3\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nLesson 1.3: Vocabulary and Nomenclature activity\n\n\n\nNomenclature Matching\n\n\n\n\n\n\n\n8. Discrete observations of a time series, taken at times \\(1, 2, \\ldots, n\\).\n\\(\\{x_t\\}\\)\n\n\n9. Number of observations of a time series\n\\(n\\)\n\n\n10. Lead time\n\\(k\\)\n\n\n11. The trend as observed at time \\(t\\)\n\\(m_t\\)\n\n\n12. The seasonal effect, as observed at time \\(t\\)\n\\(s_t\\)\n\n\n13. The error term (a sequence of correlated random variables with mean zero), as observed at time \\(t\\)\n\\(z_t\\)\n\n\n14. Centered moving average for obsrvations made monthly\n\\(\\hat m_t\\)\n\n\n15. Estimate of monthly additive effect\n\\(\\hat s_t = x_t - \\hat m_t\\)\n\n\n16. Estimate of monthly multiplicative effect\n\\(\\hat s_t = \\dfrac{x_t}{\\hat m_t}\\)\n\n\n\nAdditional Nomenclature Matching\n\n\n\n\n\n\n\n17. Forecast made at time \\(t\\) for a future value \\(k\\) time units in the future\n\\(\\hat x_{t+k \\mid t}\\)\n\n\n18. Additive decomposition model\n\\(x_t = m_t + s_t + z_t\\)\n\n\n19. Additive decomposition model after taking the logarithm\n\\(\\log(x_t) = m_t + s_t + z_t\\)\n\n\n20. Multiplicative decomposition model\n\\(x_t = m_t \\cdot s_t + z_t\\)\n\n\n21. Seasonally adjusted mean for the month corresponding to time \\(t\\)\n\\(\\bar s_t\\)\n\n\n22. Seasonal adjusted series (additive seasonal effect)\n\\(x_t - \\bar s_t\\)\n\n\n23. Seasonal adjusted series (multiplicative seasonal effect)\n\\(\\frac{x_t}{\\bar s_t}\\)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Math Formulas",
      "Formulas"
    ]
  },
  {
    "objectID": "tools/git_yml_term.html",
    "href": "tools/git_yml_term.html",
    "title": "Git and Terminal Commands",
    "section": "",
    "text": "console\nrenders qmd file, but does not show up in viewer\nquarto::quarto_render(“folder/your_file.qmd”)\n\n\nQuarto yml code (indiviual)\nThis is yml code for qmd files.\n\n\ntemrinal\nif cache needs to be clear\nquarto clean\nor delete cache folder\n\n\n\n\n Back to top",
    "crumbs": [
      "Tools, Help & Ideas",
      "Git and Terminal Commands"
    ]
  },
  {
    "objectID": "projects/project2.html",
    "href": "projects/project2.html",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "",
    "text": "Part I. What Consumer Credit Data Are Available on the G.19 Statistical Release, “Consumer Credit,” and How Are These Data Calculated?\nThe G.19 Statistical Release, “Consumer Credit,” reports outstanding credit extended to individuals for household, family, and other personal expenditures, excluding loans secured by real estate. Total consumer credit comprises two major types: revolving and nonrevolving. Revolving credit plans may be unsecured or secured by collateral and allow a consumer to borrow up to a prearranged limit and repay the debt in one or more installments. Credit card loans comprise most of revolving consumer credit measured in the G.19, but other types, such as prearranged overdraft plans, are also included. Nonrevolving credit is closed-end credit extended to consumers that is repaid on a prearranged repayment schedule and may be secured or unsecured. To borrow additional funds, the consumer must enter into an additional contract with the lender. Consumer motor vehicle and education loans comprise the majority of nonrevolving credit, but other loan types, such as boat loans, recreational vehicle loans, and personal loans, are also included.\nThe G.19 also reports selected terms of credit, including interest rates on new car loans, personal loans, and credit card plans at commercial banks. Historically, the G.19 also included series that measure the terms of credit for motor vehicle loans at finance companies. In the first quarter of 2011, publication of these series was temporarily suspended because of the deterioration of their statistical foundation. The statistical foundation is in the process of being improved, and publication will resume as soon as possible.\n\n\nCode\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n    mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n   select(ym, OCC, OCC_MoM) |&gt;\n   mutate(OCC = OCC / 1000) |&gt;\n   slice_head(prop = 1) # tail: last 10%. head: first or oldest\n#   slice((n() * 0.5):(n() * 0.6)) # select from 50% to 60%\n# interval(df) # gives interval: M, D ot Y etc\n# has_gaps(df) # false if none and vice versa True\n\n\np_cdebt &lt;- df |&gt; # fig 1\n  autoplot(OCC) +\n  labs(\n    x = \"Month\",\n    y = \"OCC\",\n    title = \"Fig 1 - Time Series: OCC\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np_cdebt_tail &lt;- df |&gt; # fig 1.1\n  slice_tail(prop = .1) |&gt;\n  autoplot(OCC) +\n  labs(\n    x = \"Month\",\n    y = \"OCC\",\n    title = \"fig 1.1 Time Series: OCC 2017-Present\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\np_cdebt_change &lt;- df |&gt;  # fig 1.2\n  autoplot(OCC_MoM) +\n  labs(\n    x = \"Month\",\n    y = \"MoM OCC\",\n    title = \"Fig 1.2 - Time Series: OCC_MoM\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\np_cdebt | p_cdebt_tail\n\n\n\n\n\n\n\n\n\n\n\nCode\n# fig 1.2\np_cdebt_change\n\n\n\n\n\n\n\n\n\nFigure 1 shows the time series of the Outstanding consumer debt. Figure 1.1 shows the same time series but form 2017 to lattest data. Figure 1.3 is the month over month(MoM) percent change in Outstanding Consumer Credit.\n\nNoticible\n\nThe percent change has more volatility, specially in the early years of the time series, something that the time series plot does not catch due to the scale of the y axis.",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#notes",
    "href": "projects/project2.html#notes",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "Notes",
    "text": "Notes\nI was looking to see what can cause the spikes in the Random component, and maybe incentives to take out loans? I have data on student loans totals so I can see if there was an actual increase.\nai: April 2021: Biden requested that Education Secretary Miguel Cardona review his executive authority to cancel student loan debt unilaterally. This move suggested he was considering executive action, although no specific details or amounts were confirmed at that time.\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD # Exponential Smoothing\n\n\nCode\n# mirror to resource code 3.4.1 exponential smoothing\n\nfig_3_4_1_B1 &lt;- autoplot(df) +\n    labs(x = \"ym\", y = \"OCC\")\n\n\nPlot variable not specified, automatically selected `.vars = OCC`\n\n\nCode\ndf1 &lt;- df |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Additive = ETS(OCC ~\n        trend(\"M\", alpha = 0.7849282, beta = 0.1282016) +\n        error(\"M\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\nSSE_1 &lt;- sum(components(df1)$remainder^2, na.rm = T)\n\nfig_3_4_1_B2 &lt;- augment(df1) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndf2 &lt;- df |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Additive = ETS(OCC ~\n        trend(\"M\", alpha = 0.7849282, beta = 0.1282016) +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\nSSE_2 &lt;-  sum(components(df2)$remainder^2, na.rm = T)\n\nfig_3_4_1_B1\n\n\n\n\n\n\n\n\n\nCode\nSSE_1\n\n\n[1] 0.1694887\n\n\nCode\nfig_3_4_1_B2\n\n\n\n\n\n\n\n\n\nCode\nSSE_2\n\n\n[1] 115763.8\n\n\nai notes:\n\nEvaluating Model Fit The sum of squared residuals (SSE) is a common metric to evaluate how well the model fits the data. Lower SSE values indicate that the model’s predictions are close to the actual data points. In exponential smoothing models, you might compare SSEs across different model configurations (e.g., different smoothing parameters, different trend or seasonal components) to select the model that minimizes error.\nOptimization and Model Selection When fitting ETS models, you often need to optimize parameters such as smoothing constants ( 𝛼 α, 𝛽 β, and 𝛾 γ). The optimization criterion “amse” (average mean squared error) minimizes the mean squared error, indirectly influencing SSE. Comparing SSEs between models (df1 and df2 in your example) can help you determine if adjusting a parameter (e.g., 𝛼 α from 0.1429622 to 0.2) results in a better model fit.\nDiagnostics and Model Improvement SSE can be a diagnostic tool to assess whether your chosen model structure (e.g., additive trend and additive seasonality) is suitable for your data. High residual sums (or unexplained variance) may suggest that the model lacks a component (like an interaction between trend and seasonality) or is mis-specified.\nStatistical Properties of the Residuals For an ETS model to be considered a good fit, residuals should ideally be white noise—meaning they are uncorrelated, normally distributed, and have zero mean. Analyzing the sum of squared residuals, along with residual plots, autocorrelation, and partial autocorrelation functions, provides insights into whether the residuals meet these assumptions.\n\nIn short, the SSE (sum of squared residuals) is a key metric in exponential smoothing to measure and compare model performance, guide parameter optimization, and assess the statistical quality of the model’s fit to time series data.\nspacer\n\n\nCode\n# mirror to resource code 3.4.2 \n\ndf342 &lt;- df \n\n# autoplot(df342) +\n#     labs(\n#         x = \"ym\",\n#         y = \"OCC\")\ndf342_2 &lt;- df342 |&gt;\n    slice_tail(prop = .8) |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"M\") +\n        error(\"M\") +\n        season(\"M\"), \n        opt_crit = \"amse\", nmse = 1))\n# `report(df342_2)\n# tidy(df342_2)\n# sum(components(df342_2)$remainder^2, na.rm = T)\n# accuracy(df342_2)$RMSE\n# sd(df342$OCC)\n# autoplot(components(df342_2))\n# augment(df342_2) |&gt;\n#     ggplot(aes(x = ym, y = OCC)) +\n#     geom_line() +\n#     geom_line(aes(y = .fitted, color = \"Fitted\")) +\n#     labs(color = \"\")\n\nreport(df342_2)\n\n\nSeries: OCC \nModel: ETS(M,M,M) \n  Smoothing parameters:\n    alpha = 0.7719996 \n    beta  = 0.07304847 \n    gamma = 0.2274828 \n\n  Initial states:\n     l[0]     b[0]     s[0]    s[-1]     s[-2]     s[-3]     s[-4]    s[-5]\n 56.69474 0.919114 1.084212 1.148118 0.9343098 0.9815824 0.9831539 1.010902\n     s[-6]     s[-7]    s[-8]     s[-9]   s[-10]    s[-11]\n 0.9273472 0.8630946 1.243133 0.9753093 1.026207 0.8226304\n\n  sigma^2:  0.0012\n\n     AIC     AICc      BIC \n10223.21 10224.01 10302.48 \n\n\nI tried doing NO seasonality (N) for the Season component but I was running into an error.",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#code-takes-for-ever-to-run",
    "href": "projects/project2.html#code-takes-for-ever-to-run",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "3.4.3 code takes for ever to run!!!!!!!!!!!!!!!!!!",
    "text": "3.4.3 code takes for ever to run!!!!!!!!!!!!!!!!!!\n\n\nCode\n# mirror to resource code 3.4.3\n\ndf343_0 &lt;-  df |&gt;\n  slice_head(prop = .94)\n\ndf343 &lt;-  df343_0 |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\np_343 &lt;- augment(df343) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\np_343_forecast &lt;- df343 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343) |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2020 Nov\"))) +\n  scale_color_discrete(name = \"\")\n\np_343\n\n\n\n\n\n\n\n\n\nCode\np_343_forecast\n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using dygraph for a more interactive time series\n#works great, but it does not use as.tsibble and converts to xts in order to work with dygraphs. \n# I think xts will cause errors with my data, need to what differes xts to as.tsibble.\n\ndf343_0 &lt;- df |&gt;\n  slice_head(prop = .94)\n\n# Fit the model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~\n    trend(\"A\") +\n    error(\"A\") +\n    season(\"A\"),\n    opt_crit = \"amse\", nmse = 1))\n\n# Generate fitted values\nfitted_data &lt;- augment(df343)\n\n# Convert the data to xts format for dygraph\nocc_xts &lt;- xts(fitted_data$OCC, order.by = as.Date(fitted_data$ym))\nfitted_xts &lt;- xts(fitted_data$.fitted, order.by = as.Date(fitted_data$ym))\n\n# Create a dygraph with actual and fitted values\ndygraph(cbind(Actual = occ_xts, Fitted = fitted_xts)) %&gt;%\n  dySeries(\"Actual\", label = \"Actual\") %&gt;%\n  dySeries(\"Fitted\", label = \"Fitted\") %&gt;%\n  dyOptions(colors = c(\"blue\", \"red\")) %&gt;%\n  dyRangeSelector() %&gt;%\n  dyLegend(show = \"always\")\n\n\n\n\n\n\nspacer\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using plotly  for a more interactive time series, but this code did not look great but leaving here for sample\n\n\n# Load required libraries\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, plotly)\n\n# Import data and convert to tsibble\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n  mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n  select(ym, OCC, OCC_MoM)\n\n# Create the initial slice of the dataset\ndf343_0 &lt;- df |&gt; slice_head(prop = .94)\n\n# Fit the ETS model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~ trend(\"A\") + error(\"A\") + season(\"A\"), opt_crit = \"amse\", nmse = 1))\n\n# Create the first interactive plot using Plotly\np_343 &lt;- augment(df343) |&gt;\n  ggplot(aes(x = ym, y = OCC)) +\n  geom_line() +\n  geom_line(aes(y = .fitted, color = \"Fitted\")) +\n  labs(color = \"\")\n\n# Convert ggplot object to plotly for interactivity\np_343_plotly &lt;- ggplotly(p_343)\np_343_plotly\n\n# Forecast and create the second interactive plot using Plotly\np_343_forecast &lt;- df343 |&gt;\n  forecast(h = \"2 years\") |&gt;\n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343) |&gt; filter(ym &gt;= yearmonth(\"2017 Jan\") & ym &lt;= yearmonth(\"2020 Nov\"))) +\n  scale_color_discrete(name = \"\")\n\n# Convert the forecast plot to plotly\np_343_forecast_plotly &lt;- ggplotly(p_343_forecast)\np_343_forecast_plotly\n\n\nspacer\n\n\nCode\n# this cell is mirror of 3.4.3 and it is using plotly  for a more interactive time series\n# it is almost good but I need the y scale to autornage according to the area the plot is zooming in. \n# Load required libraries\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, plotly)\n\n# Import data and convert to tsibble\ncredit &lt;- rio::import(\"../data/debt.xlsx\") |&gt;\n  mutate(ym = yearmonth(lubridate::ym(date)))\ndf &lt;- as_tsibble(credit, index = ym) |&gt;\n  select(ym, OCC, OCC_MoM)\n\n# Create the initial slice of the dataset\ndf343_0 &lt;- df |&gt; slice_head(prop = .94)\n\n# Fit the ETS model\ndf343 &lt;- df343_0 |&gt;\n  model(Multiplicative = ETS(OCC ~ trend(\"A\") + error(\"A\") + season(\"A\"), opt_crit = \"amse\", nmse = 1))\n\n# Generate fitted values\nfitted_data &lt;- augment(df343)\n\n# Prepare the data for Plotly\ndf_plot &lt;- fitted_data |&gt;\n  mutate(Date = as.Date(ym))\n\n# Create an interactive Plotly plot for actual and fitted values with dynamic y-axis\np_343_plotly &lt;- plot_ly(df_plot, x = ~Date) %&gt;%\n  add_lines(y = ~OCC, name = \"Actual\", line = list(color = 'blue')) %&gt;%\n  add_lines(y = ~.fitted, name = \"Fitted\", line = list(color = 'red')) %&gt;%\n  layout(\n    title = \"Actual vs Fitted Values\",\n    xaxis = list(\n      title = \"Date\",\n      rangeslider = list(visible = TRUE)\n    ),\n    yaxis = list(\n      title = \"OCC\",\n      autorange = TRUE\n    ),\n    legend = list(orientation = \"h\", x = 0.1, y = -0.2)\n  )\n\n# Display the interactive plot with dynamic y-axis scaling\np_343_plotly",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects/project2.html#using-full-df-and-doing-24-26-forecast",
    "href": "projects/project2.html#using-full-df-and-doing-24-26-forecast",
    "title": "Time Series Project 2: Consumer Credit",
    "section": "using full df and doing 24-26 forecast",
    "text": "using full df and doing 24-26 forecast\n\n\nCode\n# mirror to resource code 3.4.3 w/ 2024-2026 forecast\n\n\ndf343_1_0 &lt;-  df |&gt;\n  slice_head(prop = 1)\n\ndf343_2 &lt;-  df343_1_0 |&gt;\n    model(Multiplicative = ETS(OCC ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\np_343_2 &lt;- augment(df343_2) |&gt;\n    ggplot(aes(x = ym, y = OCC)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\np_343_2_forecast &lt;- df343_2 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df |&gt; filter(ym &gt;= yearmonth(\"2021 Jan\") & ym &lt;= yearmonth(\"2024 Aug\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df343_2) |&gt; filter(ym &gt;= yearmonth(\"2021 Jan\") & ym &lt;= yearmonth(\"2024 Aug\"))) +\n  scale_color_discrete(name = \"\")\n\n\np_343_2\n\n\n\n\n\n\n\n\n\nCode\np_343_2_forecast\n\n\n\n\n\n\n\n\n\nspacer\nwould the season but ‘M’ because it is not as volatile as the time series goes on. Like in the beginning OCC_MoM has a good amount of volatility, so the season is properly going down over time? * Okay I did cut the first 20 years of thge df so what will happen if I include the whole dataset.\n======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; 41d67e11b64162a043bcde7f5be9624fcaf874ad",
    "crumbs": [
      "Project 2",
      "Time Series Project 2: Consumer Credit"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Applied Time Series Projects",
    "section": "",
    "text": "Applied Time Series Research\n\nChina Export Commodities Time Series Analysis:\n\nApplied time series techniques to analyze trends, seasonality, and irregularities in China’s export commodity data from 1990 to the present.\nConducted a classical multiplicative decomposition to distinguish long-term growth patterns, seasonal cycles, and random shocks influenced by macroeconomic events.\nProvided actionable insights into optimizing trade strategies by leveraging seasonal trends and mitigating risks from unpredictable disruptions.\n\n\n\n\nTime Series Reserach on China’s Export Commodities\n\n\nConsumer Credit Data Analysis:\n\nExamined consumer credit data from the G.19 Statistical Release, focusing on outstanding credit extended to individuals for household and personal expenditures.\nDifferentiated between revolving credit (e.g., credit card loans) and nonrevolving credit (e.g., motor vehicle and education loans) to identify distinct patterns and trends.\nAnalyzed the interest rates and terms of credit, including new car loans, personal loans, and credit card plans at commercial banks, and discussed the implications of historical data series.\nProvided insights into consumer borrowing behaviors and the potential impact of macroeconomic changes on credit markets.\n\n\n\nProject 2: Consumer Credit Data Analysis\n\n\nProject 3: Coming Soon\n\n\nproject 3\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Applied Time Series Projects"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Time Series Analysis Notebook",
    "section": "",
    "text": "I’m first going to work out this index file, to have the whole picture then I will branch out base on information here. As I branch out information here will be compress and have links to the lessons and samples."
  },
  {
    "objectID": "index.html#ar1-model-lesson-4.3",
    "href": "index.html#ar1-model-lesson-4.3",
    "title": "Applied Time Series Analysis Notebook",
    "section": "AR(1) model lesson 4.3",
    "text": "AR(1) model lesson 4.3\n\\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "index.html#stationary-time-series",
    "href": "index.html#stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Stationary Time Series",
    "text": "Stationary Time Series\n\nSationarity of Linear Models Lesson 5.1\n\nLinear models for time series are non-stationary when they include functions of time.\n\nDifferencing can often transform a non-stationary series with a de terministic trend to a stationary series (4.1).\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences (4.2)."
  },
  {
    "objectID": "index.html#non-stationary-time-series",
    "href": "index.html#non-stationary-time-series",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Non-Stationary Time Series",
    "text": "Non-Stationary Time Series\n\nNon-Stationary Time Series Lesson 5.2\n\nA time series with a stochastic trend is non-stationary.\nA time series with a deterministic trend is non-stationary.\nA time series with a seasonal component is non-stationary.\nA time series with a unit root is non-stationary."
  },
  {
    "objectID": "index.html#lesson-4.1-white-noise-and-random-walks",
    "href": "index.html#lesson-4.1-white-noise-and-random-walks",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.1 White Noise and Random Walks",
    "text": "Lesson 4.1 White Noise and Random Walks\n\nStochastic Process 4.1\nStochastic processes or random processes evolve over time. They are characterized by the fact that the future values of the process cannot be predicted based on past values. The random walk is a classic example of a stochastic process.\n\n\nDiscrete White Noise DWN 4.1\n\n4.2: Computing the difference between successive terms of a random walk leads to a discrete white noise series.\n\nA time series \\(\\{w_t: t = 1, 2, \\ldots, n\\}\\) is a discrete white noise (DWN) if the variables \\(w_1, w_2, \\ldots, w_n\\) are independent and identically distributed with mean 0.\n\nSecond-Order Properties of DWN\n\nWhen we refer to the second-order properties of a time series, we are talking about its variance and covariance. The variance of a DWN is constant, and the covariance between any two observations is zero\nThe mean is a first-order property, the covariance is a second-order property.\n\n\n\nDiscrete White Noise Process\n\nA DWN process will have the following properties:\n\nThere is a discrete observations.\nThe mean of the observations is zero.\nThe variance of the observations is finite.\nSuccessive observations are uncorrelated.\n\n\n\n\n\nRandom Walk\n\nRandom Walks Lesson 4.1\n\nA random walk is a stochastic process in which the difference between each observation is a white noise process, a non-stationary time series. (see def)\nwt is a dwn and often model as gwn, however wt could be as simple as a coin toss (random walk).\n\n\n\nProperties of Random Walk or walks\nor First-Order Properties of A Random Walk - The mean of a random walk series is 0.\nLook at shinny code for this\n\n\nSecond-Order Properties of a Random Walk\n\nCovariance: \\(cov(x_t,x_{t+k})\\):\nThe covariance between two values of the series depends on ( t ):\n\\[\n\\text{cov}(x_t, x_{t+k}) = t \\sigma^2\n\\]\nCorrelation Function \\(\\rho_k\\):\nThe correlation for lag  k  is:\n\\[\n\\rho_k = \\frac{1}{\\sqrt{1 + \\frac{k}{t}}}\n\\]\nNon-Stationarity:\nThe variance of the series increases with ( t ), making the random walk non-stationary.\nCorrelogram Characteristics:\nThe correlogram of a random walk typically shows:\n\nPositive autocorrelations starting near 1.\nA slow decrease as ( k ) increases.\n\n\n\n\n\nGaussian White Noise GWN 4.1\n\nIf the variables are normally distributed, i.e. \\(w_i \\sim N(0,\\sigma^2)\\), the DWN is called a Gaussian white noise process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n\n\nWhite Noise Time Series\n\nWhite Noise Time Series Lesson 4.1\n\nA white noise time series is a sequence of random variables that are uncorrelated and have a mean of zero.\nA white noise time series has a constant variance.\nA white noise time series has a constant mean.\nA white noise time series has a constant autocorrelation of zero for all lags except when the lag is zero.\n\n\n\n\nCorrelogram 4.1\n\nCorrelogram Lesson 4.1\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series.\nEach correlogram lag tests for correlation significance, increasing the chance of Type I error, resulting in potentially misleading conclusions about significant relationships.\n\n\n\n\nFitting the White Noise Model\n\n\nBackward Shift Operator\nWe define the backward shift operator or the lag operator, \\(\\mathbf{B}\\), as: \\[\n  \\mathbf{B} x_t = x_{t-1}\n\\] where \\(\\{x_t\\}\\) is any time series.\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\\[\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n\\]\n\nProperties of the Backshift Operator\nThe backwards shift operator is a linear operator. So, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants, then \\[\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n\\] The distributive property also holds. \\[\\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t\n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\\]\n.\n\n\n\nsearch words for lesson 4.1\nGaussian white noise - GWN - discrete white noise - dwn - variance - covariance - correlation - correlogram - Type I error - histogram - backward shift operator - backshift operator"
  },
  {
    "objectID": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "href": "index.html#lesson-4.2-white-noise-and-random-walks---part-2",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.2 White Noise and Random Walks - Part 2",
    "text": "Lesson 4.2 White Noise and Random Walks - Part 2\n\nDifferecing a Time Series\nWhy do we difference a time series? Differencing a time series can help us to remove the trend and make the series stationary.\n\nComputing the difference between successive terms of a random walk leads to a discrete white noise series.\n\n\\[\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\\]\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences.\n\n\nCorrelograms & Histogram\nWhen do we use a correlogram and what do we look for?\nCorrelogram\n\nA correlogram is a plot of the autocorrelation function (ACF) of a time series. The ACF is a measure of the correlation between the time series and a lagged version of itself.\nNotice that the values in the correlogram of the stock prices start at 1 and slowly decay as k increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk.\n\nHistogram\n\nFigure 5 is a histogram of the differences. This is a simple measure of volatility of the stock, or in other words, how much the price changes in a day.\n\n\n\nDifference Operator\n\nDifferencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n\n\n\n\n\n\nDefinition of the Difference Operator\n\n\n\nThe difference operator, \\(\\nabla\\), is defined as:\n\\[\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t\\]\nHigher-order differencing can be denoted\n\\[\\nabla^n x_t = (1-\\mathbf{B})^n x_t\\]\n\n\nThings to do - Do excel workout and link to this. So in the website for this. This lesson will be nother tab. Maybe add the option to download the excel sheet.\nComputing Differences Small group acitivity The difference operator can be helpful in identifying the functional underpinnings of a trend. If a function is linear, then the first differences of equally-spaced values will be constant. If a function is quadratic, then the second differences of equally-spaced values will be constant. If a function is cubic, then the third differences of equally-spaced values will be constant, and so on.\ndifferencing Stock Prices do this group activity\nIntegrated Autoregressive Model do this group activity\nClass Activyt: Random Walk Drift do this class activity"
  },
  {
    "objectID": "index.html#lesson-4.3-autoregressive-ar-models",
    "href": "index.html#lesson-4.3-autoregressive-ar-models",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 4.3 Autoregressive (AR) Models",
    "text": "Lesson 4.3 Autoregressive (AR) Models\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\n\nProperties of an AR(p) Stochastic Process\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\nExploring AR(1) Models\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\nSecond-Order Properties of an AR(1) Model\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\nCorrelogram of an AR(1) Model\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do\n\nDO group activity: Simulation of an AR(1) process\n\n\n\nPartial Autocorrelation\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\nPartial Autocorrelation Plots of Various AR(p) Processes\nLook at lesson shinny code\n\n\nSationary and Non-Stationary AR Processes\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1. - to Prove that ( y_t ) is stationary. (see hw 4.3 and lesson 4.3) (see hw4.3 Q2.C)\n\n\n\n\nAbsolute Value in the Complex Plane\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\nQuestions\n\nWhat is an exponential smoothing model?\n\n\n\nSearch for words for lesson 4.3\nexponential smoothing model - polyroot function -"
  },
  {
    "objectID": "index.html#lesson-5.1-unassigned-sections",
    "href": "index.html#lesson-5.1-unassigned-sections",
    "title": "Applied Time Series Analysis Notebook",
    "section": "Lesson 5.1 unassigned sections",
    "text": "Lesson 5.1 unassigned sections\n\nGeneralized Least Squares (GLS)\n\nThe autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS – the indominable Generalized Least Squares algorithm!\nGeneralized Least Squares (GLS) Lesson 5.1\n\nGeneralized Least Squares (GLS) is a method for estimating the unknown parameters in a linear regression model.\nGLS is used when the errors in a regression model are correlated.\nGLS is used when the errors in a regression model are heteroskedastic.\nGLS is used when the errors in a regression model are autocorrelated.\nGLS is used when the errors in a regression model are non-normal.\n\n\n\n\nAdditive Seasonal Indicator Variables\n\nadditive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\n\\[\n  x_t = m_t + s_t + z_t\n\\]\nwhere \\[\n  s_t =\n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      ⋮~~~~ & ~~~~~~~~~~~~⋮ \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s\n    \\end{cases}\n\\] and \\(s\\) is the number of seasons in one cycle/period, and \\(n\\) is the number of observations, so \\(t = 1, 2, \\ldots, n\\) and \\(i = 1, 2, \\ldots, s\\), and \\(z_t\\) is the residual error series, which can be autocorrelated.\nIt is important to note that \\(m_t\\) does not need to be a constant. It can be a linear trend:\n\nSeasonal indicator variable\n\nlesson5.1\n\nWe will create a linear model that includes a constant term for each month. This constant monthly term is called a seasonal indicator variable.\nThis name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented.\nIndicator variables are also called dummy varaibles."
  },
  {
    "objectID": "hw/homework_4_2.html",
    "href": "hw/homework_4_2.html",
    "title": "Time Series Homework: Chapter 4 Lesson 2",
    "section": "",
    "text": "Modify the code used to get the prices of McDonald’s stock to download closing stock prices for a different publicly-traded company over a time period of your choice.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Set symbol and date range for Apple\nsymbol &lt;- \"AAPL\"\ndate_start &lt;- \"2022-01-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Download the stock data\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Convert to a tsibble\nstock_ts &lt;- stock_df |&gt;\n  mutate(dates = lubridate::ymd(date), value = adjusted) |&gt;\n  mutate(year_week = yearweek(dates)) |&gt;\n  group_by(year_week) |&gt;\n  summarise(value = mean(value)) |&gt;\n  ungroup() |&gt;\n  as_tsibble(index = year_week)\n\n# Time plot of the daily closing prices\nautoplot(stock_ts, value) +\n  labs(title = \"Time Plot of Apple (AAPL) Daily Closing Prices\",\n       x = \"Date\", y = \"Closing Price (USD)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Correlogram of the stock prices\nACF(stock_ts, value) |&gt;\n  autoplot() +\n  labs(title = \"Correlogram of AAPL Stock Prices\",\n       x = \"Lag\", y = \"ACF\")\n\n\n\n\n\n\n\n\nYes, there is evidence that daily closing stock prices follows a random walk. We can see how the correlogram starts at 1 and slowly decreases as k increases. This shows that there is no significant autocorrelation in the differenced values. This what to expect from a random walk.\n.\nSome thoughts: Interestingly I can see how under some circumstances the closing price for Apple can be an increasing trend and just using the basic classical decomposition we will determine the time series for AAPL’s closing prices is a deterministic trend. Now if we apply some linear model with seasonal variables, and somehow manage to add an inflation variable. We can probably make a longer time series for AAPL not seem like an increasing trend as their price matches inflation increase overtime. This can give a more fitted model to determine AAPL time series and provide more accurate forecast model. Obviously AAPL is a successful company so their price will still see some sort of increasing trend.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Calculate the first difference\nstock_ts &lt;- stock_ts %&gt;%\n  mutate(diff = value - lag(value))\n\n# Time plot of the first differences\nautoplot(stock_ts, diff, na.rm = TRUE) +\n  labs(title = \"Time Plot of First Differences of AAPL Stock Prices\",\n       x = \"Date\", y = \"First Difference of Closing Price\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Correlogram of the differenced series \n# Part d: Correlogram of the first differences\nACF(stock_ts, diff, na.rm = TRUE) %&gt;%\n  autoplot() +\n  labs(title = \"Correlogram of First Differences of AAPL Stock Prices\",\n       x = \"Lag\", y = \"ACF\")\n\n\n\n\n\n\n\n# Histogram of the differences\nstock_ts %&gt;%\n  ggplot(aes(x = diff)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, color = \"black\", fill = \"lightblue\") +\n  geom_density(color = \"red\", linewidth = 1) +\n  labs(title = \"Histogram of Differences in AAPL Stock Prices\",\n       x = \"Difference\", y = \"Density\")\n\n\n\n\n\n\n\n# Variance of the differences\nvariance_diff &lt;- var(stock_ts$diff, na.rm = TRUE)\nvariance_diff\n\n[1] 24.42079\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nThe first differences of the AAPL stock prices show no significant auto correlations in the correlogram, and the histogram resembles a normal distribution with a density curve. This indicates that the first-differences series may follow a white noise process, as there are no patterns or significant lags in the correlogram.\n\n\n\n\n\n\nUsing the daily closing stock prices series of the previous question. Can you find evidence that there is drift in the series.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Part a: Calculate mean and standard deviation\nmean_diff &lt;- mean(stock_ts$diff, na.rm = TRUE)\nsd_diff &lt;- sd(stock_ts$diff, na.rm = TRUE)\n\n# Print the results\nmean_diff\n\n[1] 0.1826733\n\nsd_diff\n\n[1] 4.94174\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Part b: Conduct a t-test to see if the mean is significantly different from zero\nt_test_result &lt;- t.test(stock_ts$diff, mu = 0, na.rm = TRUE)\n\n# Display the results of the t-test\nt_test_result\n\n\n    One Sample t-test\n\ndata:  stock_ts$diff\nt = 0.37516, df = 102, p-value = 0.7083\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.7831384  1.1484849\nsample estimates:\nmean of x \n0.1826733 \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nThe mean difference is 0.18 and this is the estimate of the drift paramenter of the drift parameter. The standard deviation of the 102 differences is 4.94. An approximate 95% confidence interval for the drift parameter is [-0.78, 1.15] which includes 0. This means that the drift parameter is not significantly different from 0. This means that there is no justification to adding an exponentially weighted slope vs adding a drift parameter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (5)\nIncomplete (0)\n\n\nQuestion 1a: Series Plot\nResponses create a time plot of the daily closing stock prices, effectively representing the data over time. The plot is well-constructed, with clear labeling of the axes, a title, and appropriate formatting to enhance readability. Proficient submissions ensure that the time plot accurately reflects the temporal trends and patterns present in the daily closing stock prices, providing a clear visual representation of the data.\nThe plot may lack clarity or proper presentation, making it difficult to interpret the trends or patterns in the data. Additionally, they may fail to include necessary elements such as axis labels, a title, or appropriate formatting, hindering the readability of the plot. Overall, their representation of the data may be incomplete or insufficient, indicating a need for improvement in data visualization skills.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1b: Series Correlation Analysis\nResponses demonstrate a clear understanding of the concept of a random walk and its implications for autocorrelation patterns.They provide clear explanations supported by statistical evidence from the correlogram, discussing how the observed autocorrelation patterns align with the characteristics of a random walk.\nStudents demonstrate a limited understanding of the concept of a random walk or fail to connect the observed autocorrelation patterns to its implications. Their analysis may lack depth or coherence, providing vague or incorrect explanations for the presence or absence of evidence supporting a random walk. Additionally, they may overlook key features or patterns in the correlogram, hindering their ability to draw meaningful conclusions about the nature of the time series data.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 1c: First Difference Plot\nResponses create a time plot of the first-differences daily closing stock prices, effectively representing the data over time. The plot is well-constructed, with clear labeling of the axes, a title, and appropriate formatting to enhance readability. Proficient submissions ensure that the time plot accurately reflects the temporal trends and patterns present in the daily closing stock prices, providing a clear visual representation of the data.\nThe plot may lack clarity or proper presentation, making it difficult to interpret the trends or patterns in the data. Additionally, they may fail to include necessary elements such as axis labels, a title, or appropriate formatting, hindering the readability of the plot. Overall, their representation of the data may be incomplete or insufficient, indicating a need for improvement in data visualization skills.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 1d: First Difference Properties\nStudents create two charts as specified: a correlogram of the first-difference of daily closing stock prices and a histogram of the difference in stock prices with the corresponding normal density superimposed. The correlogram effectively examines the autocorrelation structure of the differenced series, providing insights into the stationarity and serial dependence of the data. The histogram and superimposed normal density accurately represent the distribution of the differences in stock prices, allowing for visual comparison between the empirical distribution and the theoretical normal distribution.\nStudents encounter difficulties in understanding the concept of differencing or density estimation, leading to inaccuracies in the visual representation of the data. Their analysis of the correlogram and histogram may lack depth or coherence, providing vague or incorrect interpretations of the autocorrelation structure or distribution of the differences in stock prices. Additionally, they may fail to effectively superimpose the normal density on the histogram or overlook key features or patterns in the visualizations.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1e: Model Identification\nStudent interpret the autocorrelation patterns in the correlogram, focusing on the presence or absence of significant autocorrelation at lag 1 and higher lags. They interpret the shape and centering of the histogram, considering whether it resembles a normal distribution, discussing how the observed patterns align with the expectations for a random walk process.\nStudents provide incomplete or superficial interpretations of the autocorrelation patterns or histogram shape, failing to connect them to the characteristics of a random walk process. They overlook key features or patterns in the visualizations, indicating a limited understanding of the underlying concepts.\n\n\n\nMastery (0)\nIncomplete (0)\n\n\nQuestion 2a: Summary Statistics\nStudents accurately calculate the mean and standard deviation of the first-difference daily closing stock prices series, with well-commented code.\nSubmission have calculation errors or lack sufficient comments in the code, making it hard to follow. They might struggle to provide accurate values or clear explanations, indicating a need for improvement in both mathematical and coding skills.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2b: Evidence of Drift\nStudents offer statistical evidence supporting the necessity of including a drift component in the random walk model. They conduct a hypothesis test correctly\nThe hypothesis test is built incorrectly, or the evidence collecter is not sufficient to make a statistical statement.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2c: Interpretation\nStudents interpret the hypothesis test correctly and use the correct language to describe their results\nResponses lack clear statistical evidence or fail to effectively justify the inclusion of a drift component in the random walk model. They might offer vague or unsupported assertions about the presence of a trend in the data without conducting appropriate statistical analysis. The interpretation of the confidence interval is incorrect\n\n\nTotal Points\n50",
    "crumbs": [
      "Lesson 2",
      "Time Series Homework: Chapter 4 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_4_2.html#questions",
    "href": "hw/homework_4_2.html#questions",
    "title": "Time Series Homework: Chapter 4 Lesson 2",
    "section": "",
    "text": "Modify the code used to get the prices of McDonald’s stock to download closing stock prices for a different publicly-traded company over a time period of your choice.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Set symbol and date range for Apple\nsymbol &lt;- \"AAPL\"\ndate_start &lt;- \"2022-01-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Download the stock data\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Convert to a tsibble\nstock_ts &lt;- stock_df |&gt;\n  mutate(dates = lubridate::ymd(date), value = adjusted) |&gt;\n  mutate(year_week = yearweek(dates)) |&gt;\n  group_by(year_week) |&gt;\n  summarise(value = mean(value)) |&gt;\n  ungroup() |&gt;\n  as_tsibble(index = year_week)\n\n# Time plot of the daily closing prices\nautoplot(stock_ts, value) +\n  labs(title = \"Time Plot of Apple (AAPL) Daily Closing Prices\",\n       x = \"Date\", y = \"Closing Price (USD)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Correlogram of the stock prices\nACF(stock_ts, value) |&gt;\n  autoplot() +\n  labs(title = \"Correlogram of AAPL Stock Prices\",\n       x = \"Lag\", y = \"ACF\")\n\n\n\n\n\n\n\n\nYes, there is evidence that daily closing stock prices follows a random walk. We can see how the correlogram starts at 1 and slowly decreases as k increases. This shows that there is no significant autocorrelation in the differenced values. This what to expect from a random walk.\n.\nSome thoughts: Interestingly I can see how under some circumstances the closing price for Apple can be an increasing trend and just using the basic classical decomposition we will determine the time series for AAPL’s closing prices is a deterministic trend. Now if we apply some linear model with seasonal variables, and somehow manage to add an inflation variable. We can probably make a longer time series for AAPL not seem like an increasing trend as their price matches inflation increase overtime. This can give a more fitted model to determine AAPL time series and provide more accurate forecast model. Obviously AAPL is a successful company so their price will still see some sort of increasing trend.\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Calculate the first difference\nstock_ts &lt;- stock_ts %&gt;%\n  mutate(diff = value - lag(value))\n\n# Time plot of the first differences\nautoplot(stock_ts, diff, na.rm = TRUE) +\n  labs(title = \"Time Plot of First Differences of AAPL Stock Prices\",\n       x = \"Date\", y = \"First Difference of Closing Price\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Correlogram of the differenced series \n# Part d: Correlogram of the first differences\nACF(stock_ts, diff, na.rm = TRUE) %&gt;%\n  autoplot() +\n  labs(title = \"Correlogram of First Differences of AAPL Stock Prices\",\n       x = \"Lag\", y = \"ACF\")\n\n\n\n\n\n\n\n# Histogram of the differences\nstock_ts %&gt;%\n  ggplot(aes(x = diff)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, color = \"black\", fill = \"lightblue\") +\n  geom_density(color = \"red\", linewidth = 1) +\n  labs(title = \"Histogram of Differences in AAPL Stock Prices\",\n       x = \"Difference\", y = \"Density\")\n\n\n\n\n\n\n\n# Variance of the differences\nvariance_diff &lt;- var(stock_ts$diff, na.rm = TRUE)\nvariance_diff\n\n[1] 24.42079\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nThe first differences of the AAPL stock prices show no significant auto correlations in the correlogram, and the histogram resembles a normal distribution with a density curve. This indicates that the first-differences series may follow a white noise process, as there are no patterns or significant lags in the correlogram.\n\n\n\n\n\n\nUsing the daily closing stock prices series of the previous question. Can you find evidence that there is drift in the series.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Part a: Calculate mean and standard deviation\nmean_diff &lt;- mean(stock_ts$diff, na.rm = TRUE)\nsd_diff &lt;- sd(stock_ts$diff, na.rm = TRUE)\n\n# Print the results\nmean_diff\n\n[1] 0.1826733\n\nsd_diff\n\n[1] 4.94174\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Part b: Conduct a t-test to see if the mean is significantly different from zero\nt_test_result &lt;- t.test(stock_ts$diff, mu = 0, na.rm = TRUE)\n\n# Display the results of the t-test\nt_test_result\n\n\n    One Sample t-test\n\ndata:  stock_ts$diff\nt = 0.37516, df = 102, p-value = 0.7083\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.7831384  1.1484849\nsample estimates:\nmean of x \n0.1826733 \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nThe mean difference is 0.18 and this is the estimate of the drift paramenter of the drift parameter. The standard deviation of the 102 differences is 4.94. An approximate 95% confidence interval for the drift parameter is [-0.78, 1.15] which includes 0. This means that the drift parameter is not significantly different from 0. This means that there is no justification to adding an exponentially weighted slope vs adding a drift parameter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (5)\nIncomplete (0)\n\n\nQuestion 1a: Series Plot\nResponses create a time plot of the daily closing stock prices, effectively representing the data over time. The plot is well-constructed, with clear labeling of the axes, a title, and appropriate formatting to enhance readability. Proficient submissions ensure that the time plot accurately reflects the temporal trends and patterns present in the daily closing stock prices, providing a clear visual representation of the data.\nThe plot may lack clarity or proper presentation, making it difficult to interpret the trends or patterns in the data. Additionally, they may fail to include necessary elements such as axis labels, a title, or appropriate formatting, hindering the readability of the plot. Overall, their representation of the data may be incomplete or insufficient, indicating a need for improvement in data visualization skills.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1b: Series Correlation Analysis\nResponses demonstrate a clear understanding of the concept of a random walk and its implications for autocorrelation patterns.They provide clear explanations supported by statistical evidence from the correlogram, discussing how the observed autocorrelation patterns align with the characteristics of a random walk.\nStudents demonstrate a limited understanding of the concept of a random walk or fail to connect the observed autocorrelation patterns to its implications. Their analysis may lack depth or coherence, providing vague or incorrect explanations for the presence or absence of evidence supporting a random walk. Additionally, they may overlook key features or patterns in the correlogram, hindering their ability to draw meaningful conclusions about the nature of the time series data.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 1c: First Difference Plot\nResponses create a time plot of the first-differences daily closing stock prices, effectively representing the data over time. The plot is well-constructed, with clear labeling of the axes, a title, and appropriate formatting to enhance readability. Proficient submissions ensure that the time plot accurately reflects the temporal trends and patterns present in the daily closing stock prices, providing a clear visual representation of the data.\nThe plot may lack clarity or proper presentation, making it difficult to interpret the trends or patterns in the data. Additionally, they may fail to include necessary elements such as axis labels, a title, or appropriate formatting, hindering the readability of the plot. Overall, their representation of the data may be incomplete or insufficient, indicating a need for improvement in data visualization skills.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 1d: First Difference Properties\nStudents create two charts as specified: a correlogram of the first-difference of daily closing stock prices and a histogram of the difference in stock prices with the corresponding normal density superimposed. The correlogram effectively examines the autocorrelation structure of the differenced series, providing insights into the stationarity and serial dependence of the data. The histogram and superimposed normal density accurately represent the distribution of the differences in stock prices, allowing for visual comparison between the empirical distribution and the theoretical normal distribution.\nStudents encounter difficulties in understanding the concept of differencing or density estimation, leading to inaccuracies in the visual representation of the data. Their analysis of the correlogram and histogram may lack depth or coherence, providing vague or incorrect interpretations of the autocorrelation structure or distribution of the differences in stock prices. Additionally, they may fail to effectively superimpose the normal density on the histogram or overlook key features or patterns in the visualizations.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1e: Model Identification\nStudent interpret the autocorrelation patterns in the correlogram, focusing on the presence or absence of significant autocorrelation at lag 1 and higher lags. They interpret the shape and centering of the histogram, considering whether it resembles a normal distribution, discussing how the observed patterns align with the expectations for a random walk process.\nStudents provide incomplete or superficial interpretations of the autocorrelation patterns or histogram shape, failing to connect them to the characteristics of a random walk process. They overlook key features or patterns in the visualizations, indicating a limited understanding of the underlying concepts.\n\n\n\nMastery (0)\nIncomplete (0)\n\n\nQuestion 2a: Summary Statistics\nStudents accurately calculate the mean and standard deviation of the first-difference daily closing stock prices series, with well-commented code.\nSubmission have calculation errors or lack sufficient comments in the code, making it hard to follow. They might struggle to provide accurate values or clear explanations, indicating a need for improvement in both mathematical and coding skills.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2b: Evidence of Drift\nStudents offer statistical evidence supporting the necessity of including a drift component in the random walk model. They conduct a hypothesis test correctly\nThe hypothesis test is built incorrectly, or the evidence collecter is not sufficient to make a statistical statement.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2c: Interpretation\nStudents interpret the hypothesis test correctly and use the correct language to describe their results\nResponses lack clear statistical evidence or fail to effectively justify the inclusion of a drift component in the random walk model. They might offer vague or unsupported assertions about the presence of a trend in the data without conducting appropriate statistical analysis. The interpretation of the confidence interval is incorrect\n\n\nTotal Points\n50",
    "crumbs": [
      "Lesson 2",
      "Time Series Homework: Chapter 4 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_3_4.html",
    "href": "hw/homework_3_4.html",
    "title": "Time Series Homework: Chapter 3 Lesson 4",
    "section": "",
    "text": "Did not include last 3 code chunks due to classical decomposition forecast models take a long time to run.",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_3_4.html#data",
    "href": "hw/homework_3_4.html#data",
    "title": "Time Series Homework: Chapter 3 Lesson 4",
    "section": "Data",
    "text": "Data\n\n# avgkwhr &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/USENERGYPRICE.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_3_4.html#questions",
    "href": "hw/homework_3_4.html#questions",
    "title": "Time Series Homework: Chapter 3 Lesson 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\nAverage Price of Electricity per Kilowatt-Hour in U.S.: City Average\nhttps://fred.stlouisfed.org/series/APU000072610\n\n\n\n\n\n\nAnswer\n\n\n\n\navgkwhr &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/USENERGYPRICE.csv\") |&gt;\n    mutate(ymonth = yearmonth(lubridate::mdy(date)),\n           usenergyprice = as.numeric(usenergyprice)\n           ) |&gt;\n  select(ymonth, usenergyprice)\ndf &lt;- as_tsibble(avgkwhr, index = ymonth)\n#interval(df)\n#sum(is.na(df$usenergyprice))\n#has_gaps(df)\n\n\n# Fill single NA values by averaging the lag and lead values\ndf &lt;- df |&gt;\n  mutate(usenergyprice = if_else(\n    is.na(usenergyprice) & !is.na(lag(usenergyprice)) & !is.na(lead(usenergyprice)),\n    (lag(usenergyprice) + lead(usenergyprice)) / 2,\n    usenergyprice\n  ))\nsum(is.na(df$usenergyprice)) # check for NA\n\n[1] 0\n\nts_add &lt;- df |&gt; # classical ADD\n  model(feasts::classical_decomposition(usenergyprice,\n          type = \"add\"))  |&gt;\n  components()\nautoplot(ts_add)\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nts_mult &lt;- df |&gt; # classical MULT\n  model(feasts::classical_decomposition(usenergyprice,\n          type = \"mult\"))  |&gt;\n  components()\nautoplot(ts_mult)\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2 - US Average Price of Electricity: Additive Holt-Winters Forecasting (25 points)\n\na) Please use the Holt-Winters smoothing method to the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\nautoplot(df) +\n    labs(x = \"Months\", y = \"usenergyprice\")\n\nPlot variable not specified, automatically selected `.vars = usenergyprice`\n\n\n\n\n\n\n\n\ndf1 &lt;- df |&gt;\n    model(Additive = ETS(usenergyprice ~\n        trend(\"A\", alpha = 0.1429622, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\nsum(components(df1)$remainder^2, na.rm = T)\n\n[1] 0.006873411\n\naugment(df1) |&gt;\n    ggplot(aes(x = ymonth, y = usenergyprice)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\n\n\n\n\n\n\n\ndf2 &lt;- df |&gt;\n    model(Additive = ETS(usenergyprice ~\n        trend(\"A\", alpha = 0.2, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\nsum(components(df2)$remainder^2, na.rm = T)\n\n[1] 0.00605785\n\n\n\n\n\n\nb) What parameters values did you choose for \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). Justify your choice.\n\n\n\n\n\n\nAnswer\n\n\n\nFor the Holt-Winters smoothing method, I selected α=0.2, β=0, and γ is not included as we are using an additive model without seasonality. The α value of 0.2 was chosen to ensure a moderate response to recent changes in electricity prices, capturing short-term fluctuations without overfitting. The absence of β and γ reflects a simple trend-focused model, aligning with the observed pattern and reducing potential overcomplications from unnecessary seasonal components.\n\n\n\n\nc) Please plot the Holt-Winters forecast of the series for the next 12 months superimposed against the original series. Please see Figure 7 in Chapter 3: Lesson 3\n\n\n\n\n\n\nAnswer\n\n\n\n\n# remove code to make as_tsibble and use previous as_tsibble df\ndf6 &lt;- df |&gt;\n    model(Multiplicative = ETS(usenergyprice ~\n        trend(\"M\") +\n        error(\"M\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\n\naugment(df6) |&gt;\n    ggplot(aes(x = ymonth, y = usenergyprice)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndf6 |&gt;\n  forecast(h = \"1 years\") |&gt; \n  autoplot(avgkwhr |&gt; filter(ymonth &gt;= yearmonth(\"2019 Nov\") & ymonth &lt;= yearmonth(\"2019 Dec\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df6) |&gt; filter(ymonth &gt;= yearmonth(\"2016 Jan\") & ymonth &lt;= yearmonth(\"2019 Dec\"))) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\nd) Is the trend in the US Average Price of Electricity series deterministic or stochastic? What is the basis of your evaluation?\n\n\n\n\n\n\nAnswer\n\n\n\nThe trend in the US Average Price of Electricity series appears to be stochastic, as evidenced by fluctuations in the series that do not follow a fixed, predictable pattern over time. A deterministic trend would show a consistent, smooth direction (either up or down), whereas here, statistical tests or decompositions (such as classical decomposition) likely reveal a component of randomness affecting the long-term trend. This indicates that external factors, such as policy changes or economic fluctuations, likely influence the series in a stochastic manner.\n\n\n\n\n\nQuestion 3 - Real US Average Price of Electricity: Additive Holt-Winters Forecasting (25 points)\nThe upward trend of the series is mostly due to inflation, the generalized increase in prices throughout the economy. One way to quantify inflation is to use a price index, like the Personal Consumption Expenditures Deflator (PCE).The series HERE shows that prices in the US have climbed steadily over the last 60 years.Because energy is an important part of the economy, it’s likely that energy prices have followed a similar pattern. Adjusting a series with nominal prices, like the price of electricity, to real prices that account for inflation is simple, divide the original series by the price index. The data set imported below is the real price of electricity, which is the US Average Price of Electricity divided by the PCE index excluding food and energy prices (PCEPILFE). Repeat steps a) to d) for the updated series.\n\na) Please use the Holt-Winters smoothing method to the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\nreal_avgkwhr &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/USENERGYPRICE_Real.csv\") |&gt;\n    mutate(yquarter = yearquarter(lubridate::mdy(date)),\n           realprices = as.numeric(realprices)\n           ) |&gt;\n  select(yquarter, realprices)\ndfr &lt;- as_tsibble(real_avgkwhr, index = yquarter)\ninterval(dfr)\nsum(is.na(dfr$realprices))\nhas_gaps(dfr)\n\n\n# Fill single NA values by averaging the lag and lead values\n# dfr &lt;- dfr |&gt;\n#   mutate(realprices = if_else(\n#     is.na(realprices) & !is.na(lag(realprices)) & !is.na(lead(realprices)),\n#     (lag(realprices) + lead(realprices)) / 2,\n#     realprices\n#   ))\n# sum(is.na(dfr$realprices)) # check for NA\n\n\n\n# tsr_add &lt;- dfr |&gt; # classical ADD\n#   model(feasts::classical_decomposition(realprices,\n#           type = \"add\"))  |&gt;\n#   components()\n# autoplot(tsr_add)\n\n\n# tsr_mult &lt;- dfr |&gt; # classical MULT\n#   model(feasts::classical_decomposition(realprices,\n#           type = \"mult\"))  |&gt;\n#   components()\n# autoplot(tsr_mult)\n\n\n\n\nautoplot(dfr) +\n    labs(x = \"Quarters\", y = \"realprices\")\ndfr1 &lt;- dfr |&gt;\n    model(Additive = ETS(realprices ~\n        trend(\"A\", alpha = 0.1429622, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\nsum(components(dfr1)$remainder^2, na.rm = T)\naugment(dfr1) |&gt;\n    ggplot(aes(x = yquarter, y = realprices)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndfr2 &lt;- dfr |&gt;\n    model(Additive = ETS(realprices ~\n        trend(\"A\", alpha = 0.2, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\nsum(components(dfr2)$remainder^2, na.rm = T)\n\n\n\n\n\nb) What parameters values did you choose for \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). Justify your choice.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nc) Please plot the Holt-Winters forecast of the series for the next 12 months superimposed against the original series. Please see Figure 7 in Chapter 3: Lesson 3\n\n\n\n\n\n\nAnswer\n\n\n\n\n# remove code to make as_tsibble and use previous as_tsibble dfr\n\n\n\n\ndfr6 &lt;- dfr |&gt;\n    model(Multiplicative = ETS(realprices ~\n        trend(\"M\") +\n        error(\"M\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\n\naugment(dfr6) |&gt;\n    ggplot(aes(x = yquarter, y = realprices)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndfr6 |&gt;\n  forecast(h = \"1 years\") |&gt; \n  autoplot(real_avgkwhr |&gt; filter(yquarter &gt;= yearquarter(\"2018 Q2\") & yquarter &lt;= yearquarter(\"2023 Q4\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(dfr6) |&gt; filter(yquarter &gt;= yearquarter(\"2018 Q2\") & yquarter &lt;= yearquarter(\"2023 Q4\"))) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\nd) Is the trend in the US Average Real Price of Electricity series deterministic or stochastic? What is the basis of your evaluation?\n\n\n\n\n\n\nAnswer",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_3_2.html",
    "href": "hw/homework_3_2.html",
    "title": "Time Series Homework: Chapter 3 Lesson 2",
    "section": "",
    "text": "Code\nbuild_dat &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/mortality_us.xlsx\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_3_2.html#data",
    "href": "hw/homework_3_2.html#data",
    "title": "Time Series Homework: Chapter 3 Lesson 2",
    "section": "",
    "text": "Code\nbuild_dat &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/mortality_us.xlsx\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_3_2.html#questions",
    "href": "hw/homework_3_2.html#questions",
    "title": "Time Series Homework: Chapter 3 Lesson 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\nUS Mortality\nhttps://wonder.cdc.gov/wonder/help/cmf.html#\nNote: The data is self-explanatory, don’t get lost in the documentation page.\n\n\n\n\n\n\nAnswer\n\n\n\nOverview data\n\n\nCode\npacman::p_load(\"tsibble\", \"fable\", \"feasts\",\n    \"tsibbledata\", \"fable.prophet\", \"tidyverse\", \"patchwork\")\nbuild_dat &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/mortality_us.xlsx\") |&gt;\n    rename(\n        mort = \"mort_rate_per_100,000\",\n        index = year  # rename `year` to `index`\n    ) |&gt;\n    mutate(\n        mort = as.numeric(mort),\n        deaths = as.numeric(deaths)\n    ) |&gt;\n    as_tsibble(index = index)  # set `index` as the tsibble index\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nI ask chatgpt or help plotting this do to differences in the y scale.\nChatgpt: In this ggplot, mort and deaths are plotted on dual y-axes to allow simultaneous visualization despite differing scales. deaths is linearly scaled to align with mort values on the primary y-axis, while a secondary y-axis reverses this transformation to reflect deaths on its natural scale. This approach allows for direct visual comparison of trends and seasonalities in both variables over a common time index, facilitating insight into potential covariation and relative dynamics across time.\n\n\nCode\n# Calculate the scaled deaths for the secondary axis\nmax_mort &lt;- max(build_dat$mort, na.rm = TRUE)\nmax_deaths &lt;- max(build_dat$deaths, na.rm = TRUE)\nbuild_dat &lt;- build_dat %&gt;%\n    mutate(scaled_deaths = deaths / max_deaths * max_mort)\n\n# Plot with dual y-axes\nmort_series_plot &lt;- ggplot(build_dat, aes(x = index)) +\n    geom_line(aes(y = mort, color = \"Mortality\")) +\n    geom_line(aes(y = scaled_deaths, color = \"Deaths\")) +  # Use scaled deaths for plotting\n    scale_y_continuous(\n        name = \"Mortality\",\n        sec.axis = sec_axis(~ . * max_deaths / max_mort, name = \"Deaths\")  # Secondary axis transformation\n    ) +\n    scale_color_manual(values = c(\"Mortality\" = \"blue\", \"Deaths\" = \"red\")) +\n    theme(legend.position = \"bottom\") +\n    labs(color = \"Legend\")\nmort_series_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nnext part\n\n\nCode\nacf_appr &lt;- ACF(build_dat, y = mort) |&gt; autoplot() +\n    labs(title = \"mort\")\nacf_act &lt;- ACF(build_dat, y = deaths) |&gt; autoplot() +\n    labs(title = \"deaths\")\njoint_ccf_plot &lt;- build_dat |&gt;\n  CCF(y = mort, x = deaths) |&gt; autoplot() +\n  labs(title = \"CCF Plot\")\n(acf_appr + acf_act) / joint_ccf_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\nCCF(build_dat, mort, deaths)\n\n\n# A tsibble: 29 x 2 [1Y]\n        lag    ccf\n   &lt;cf_lag&gt;  &lt;dbl&gt;\n 1     -14Y 0.0883\n 2     -13Y 0.0799\n 3     -12Y 0.0586\n 4     -11Y 0.0452\n 5     -10Y 0.0448\n 6      -9Y 0.0457\n 7      -8Y 0.0467\n 8      -7Y 0.0463\n 9      -6Y 0.0409\n10      -5Y 0.0253\n# ℹ 19 more rows\n\n\nnext part\nI ran into a no season error when doing the classical decomposition, this is probably why Bro. Moncayo ask us how we can do a classical decomposition without the tred\nhere is chatgpt’s notes\nThe error occurs because the feasts::classical_decomposition() function requires multiple observations per period to detect seasonality, but with only one observation per year, no seasonal pattern can be extracted, resulting in a null_mdl object that causes further processing errors.\n\n\nCode\n# Fit trend models to the data\napp_model &lt;- build_dat %&gt;%\n    model(trend_model = TSLM(mort ~ trend()))\n\nact_model &lt;- build_dat %&gt;%\n    model(trend_model = TSLM(deaths ~ trend()))\n\n# Extract the fitted values and residuals using augment()\napp_decompose &lt;- app_model %&gt;%\n    augment() %&gt;%\n    select(index, mort, trend = .fitted, remainder = .resid)\n\nact_decompose &lt;- act_model %&gt;%\n    augment() %&gt;%\n    select(index, deaths, trend = .fitted, remainder = .resid)\n\n# Plot the ACF of the remainder components\napp_random &lt;- ACF(app_decompose, remainder) %&gt;% autoplot() +\n    labs(title = \"ACF of Mortality Remainder\")\n\nact_random &lt;- ACF(act_decompose, remainder) %&gt;% autoplot() +\n    labs(title = \"ACF of Deaths Remainder\")\n\n# Merge the remainder components for cross-correlation analysis\nrandom_decompose &lt;- app_decompose %&gt;%\n    select(index, random_app = remainder) %&gt;%\n    left_join(\n        act_decompose %&gt;% select(index, random_act = remainder),\n        by = \"index\"\n    )\n\n# Plot the cross-correlation function between the remainder components\njoint_ccf_random &lt;- random_decompose %&gt;%\n    CCF(y = random_app, x = random_act) %&gt;%\n    autoplot() +\n    labs(title = \"CCF between Mortality and Deaths Remainders\")\n\n# Display the plots\nlibrary(patchwork)\n(app_random + act_random) / joint_ccf_random\n\n\n\n\n\n\n\n\n\nnotes on the error with seasonalities for this dataset:The error arose because the classical decomposition necessitates multiple observations within each seasonal cycle, which isn’t feasible with annual data devoid of seasonality; we resolved this by applying a time series linear model (TSLM) to capture the trend component and utilized augment() to extract residuals for subsequent analysis. By shifting from a decomposition method requiring seasonality to a trend-focused model, we circumvented the limitations imposed by the data’s temporal granularity.\n\n\n\n\n\n\n\nQuestion 2 - US Mortality Rate: Visualization (30 points)\n\na) Please plot the US Mortality Rate series (mortality per 100,000).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Load required libraries\npacman::p_load(\"tsibble\", \"fable\", \"feasts\",\n    \"tsibbledata\", \"fable.prophet\", \"tidyverse\", \"patchwork\")\n\n# Import and rename the data\nmotor_dat &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/mortality_us.xlsx\") |&gt;\n    rename(\n        mort = \"mort_rate_per_100,000\",\n        index = year  # rename `year` to `index`\n    ) |&gt;\n    mutate(\n        mort = as.numeric(mort),\n        deaths = as.numeric(deaths)\n    ) |&gt;\n    as_tsibble(index = index)\n\n# Task 1: Plot the US Mortality Rate series (mortality per 100,000)\nautoplot(motor_dat, mort) +\n    labs(x = \"Year\", y = \"Mortality Rate per 100,000\") +\n    ggtitle(\"US Mortality Rate Over Time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Use the exponential smoothing method to model the US Mortality Rate series. Use the smoothing parameter that R calculates by minimizing the SS1PE. Add it to the plot in 2a\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Task 2: Exponential Smoothing with R's Optimized Smoothing Parameter\n# Assuming R’s alpha (0.1429622) was calculated to minimize SS1PE\nmotor_dat1 &lt;- motor_dat |&gt;\n    model(Optimized = ETS(mort ~\n        trend(\"A\", alpha = 0.1429622, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\n\n# Plot the optimized model\naugment(motor_dat1) |&gt;\n    ggplot(aes(x = index, y = mort)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted - Optimized Alpha\")) +\n    labs(color = \"\") +\n    ggtitle(\"Exponential Smoothing with Optimized Alpha\")\n\n\n\n\n\n\n\n\n\nCode\n# Task 3: Exponential Smoothing with α = 0.2\nmotor_dat2 &lt;- motor_dat |&gt;\n    model(FixedAlpha0_2 = ETS(mort ~\n        trend(\"A\", alpha = 0.2, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\n\n# Plot the α = 0.2 model alongside the previous\naugment(motor_dat2) |&gt;\n    ggplot(aes(x = index, y = mort)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted - Alpha = 0.2\")) +\n    labs(color = \"\") +\n    ggtitle(\"Exponential Smoothing with Alpha = 0.2\")\n\n\n\n\n\n\n\n\n\nCode\n# Task 4: Exponential Smoothing with α = 1/n (where n is the number of observations)\nalpha_value &lt;- 1 / nrow(motor_dat)\nmotor_dat3 &lt;- motor_dat |&gt;\n    model(FixedAlpha1_n = ETS(mort ~\n        trend(\"A\", alpha = alpha_value, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\n\n# Plot the α = 1/n model along with previous models\naugment(motor_dat3) |&gt;\n    ggplot(aes(x = index, y = mort)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = paste(\"Fitted - Alpha =\", round(alpha_value, 4)))) +\n    labs(color = \"\") +\n    ggtitle(paste(\"Exponential Smoothing with Alpha =\", round(alpha_value, 4)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nc) Please repeat the modeling above but instead choose a smoothing parameter value \\(\\alpha=0.2\\). Add it to the plot in 2a\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Task 3: Exponential Smoothing with α = 0.2\nmotor_dat2 &lt;- motor_dat |&gt;\n    model(FixedAlpha0_2 = ETS(mort ~\n        trend(\"A\", alpha = 0.2, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\n\n# Plot the α = 0.2 model alongside the previous\naugment(motor_dat2) |&gt;\n    ggplot(aes(x = index, y = mort)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted - Alpha = 0.2\")) +\n    labs(color = \"\") +\n    ggtitle(\"Exponential Smoothing with Alpha = 0.2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nd) Please repeat the modeling above but instead choose a smoothing parameter value \\(\\alpha=\\frac{1}{n}\\).Add it to the plot in 2a\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Task 4: Exponential Smoothing with α = 1/n (where n is the number of observations)\nalpha_value &lt;- 1 / nrow(motor_dat)\nmotor_dat3 &lt;- motor_dat |&gt;\n    model(FixedAlpha1_n = ETS(mort ~\n        trend(\"A\", alpha = alpha_value, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\n\n# Plot the α = 1/n model along with previous models\naugment(motor_dat3) |&gt;\n    ggplot(aes(x = index, y = mort)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = paste(\"Fitted - Alpha =\", round(alpha_value, 4)))) +\n    labs(color = \"\") +\n    ggtitle(paste(\"Exponential Smoothing with Alpha =\", round(alpha_value, 4)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nd) Which of the smoothing parameter values you tried before work best for the series? Justify your answer.\n\n\n\n\n\n\nAnswer\n\n\n\nThe smoothing parameter that provided the best fit was the optimized α value (0.1429622), as it minimized the Sum of Squared One-Step Prediction Errors (SS1PE). In comparison, fixed α values like 0.2 and α= 1/n​ either over-smoothed or under-smoothed the series, making the optimized α most effective for accurate modeling of the mortality rate series.\n\n\n\n\n\nQuestion 5 - US Mortality Rate: Excess Mortality (30 points)\nThe jump at the last two years of the US Mortality Rate series is clearly the effect that Covid-19 had on mortality across the US.\n\na) Please calculate the excess mortality rate during 2020 and 2021 using the smoothing parameter values you employed in the previous question. Your results should be displayed in a table that is professionally formatted.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Task 1: Calculate the Excess Mortality for 2020 and 2021\n\n# Extract observed data for 2020 and 2021\nobserved_2020_2021 &lt;- motor_dat |&gt; filter(index &gt;= 2020)\n\n# Calculate fitted (expected) values for each model\nfitted_optimized &lt;- augment(motor_dat1) |&gt; filter(index &gt;= 2020) |&gt; pull(.fitted)\nfitted_alpha_0_2 &lt;- augment(motor_dat2) |&gt; filter(index &gt;= 2020) |&gt; pull(.fitted)\nfitted_alpha_1_n &lt;- augment(motor_dat3) |&gt; filter(index &gt;= 2020) |&gt; pull(.fitted)\n\n# Calculate excess mortality by comparing actual rates with fitted (expected) rates\nexcess_optimized &lt;- observed_2020_2021$mort - fitted_optimized\nexcess_alpha_0_2 &lt;- observed_2020_2021$mort - fitted_alpha_0_2\nexcess_alpha_1_n &lt;- observed_2020_2021$mort - fitted_alpha_1_n\n\n# Prepare a formatted table to display the results\nexcess_mortality_table &lt;- tibble(\n    Year = observed_2020_2021$index,\n    Observed_Mortality = observed_2020_2021$mort,\n    Excess_Optimized = excess_optimized,\n    Excess_Alpha_0_2 = excess_alpha_0_2,\n    Excess_Alpha_1_n = excess_alpha_1_n\n)\n\n# Display the formatted table for review\nexcess_mortality_table_formatted &lt;- excess_mortality_table |&gt;\n    rename(\n        \"Year\" = Year,\n        \"Observed Mortality Rate\" = Observed_Mortality,\n        \"Excess Mortality (Optimized Alpha)\" = Excess_Optimized,\n        \"Excess Mortality (Alpha = 0.2)\" = Excess_Alpha_0_2,\n        \"Excess Mortality (Alpha = 1/n)\" = Excess_Alpha_1_n\n    )\n\n# Print the formatted table\nexcess_mortality_table_formatted\n\n\n# A tibble: 2 × 5\n   Year `Observed Mortality Rate` Excess Mortality (Opt…¹ Excess Mortality (Al…²\n  &lt;dbl&gt;                     &lt;dbl&gt;                   &lt;dbl&gt;                  &lt;dbl&gt;\n1  2020                     1027                     189.                   182.\n2  2021                     1044.                    179.                   162.\n# ℹ abbreviated names: ¹​`Excess Mortality (Optimized Alpha)`,\n#   ²​`Excess Mortality (Alpha = 0.2)`\n# ℹ 1 more variable: `Excess Mortality (Alpha = 1/n)` &lt;dbl&gt;\n\n\n\n\n\n\nb) What is the meaning of excess mortality as you calculated it? Please explain it to a general audience, as if you were being interviewed in a national news show.\n\n\n\n\n\n\nAnswer\n\n\n\nExcess mortality refers to the number of deaths occurring above what would normally be expected in a given period based on historical trends. For example, if we typically expect around 800 deaths per 100,000 people each year, but we observe 1,000 per 100,000 during 2020 and 2021, the excess mortality would be 200 per 100,000. This difference often reflects the impact of unusual events or crises, such as the Covid-19 pandemic, which led to an increase in mortality rates beyond what we would normally see.\n\n\n\n\nc) Which excess mortality rate estimate do you think is correct? Some commentators would suggest you are choosing the parameters to defend your political goals, not to conduct a scientific analysis. Please comment on the difficulty of choosing parameter values to estimate the death toll of Covid-19.\n\n\n\n\n\n\nAnswer\n\n\n\nEstimating excess mortality is challenging because it requires selecting parameters that balance responsiveness to sudden changes (like a pandemic) with stability over time. In our analysis, we used three different parameter values: one optimized based on historical trends, one set at a moderate value (α = 0.2), and one based on a conservative approach (α = 1/n) that assumes slow change.\nEach choice reflects different assumptions about how mortality rates are likely to change over time. An optimized parameter aims to minimize errors but may overly fit past data, while a fixed parameter like 0.2 provides moderate sensitivity. The most conservative option, 1/n, smooths the data more aggressively, underemphasizing sudden increases.\nChoosing parameters isn’t about political bias but about finding a balance that reflects reality as closely as possible. It is, however, a difficult task: too much responsiveness may exaggerate changes, while too little may underestimate significant shifts.\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2a: Mortality Plot\nThe student accurately plots the US Mortality Rate series in R, ensuring clear labeling and a descriptive title for easy interpretation. The visualization effectively presents the data with distinguishable points or lines and appropriate formatting. Additionally, the R code is well-commented, providing clear explanations of each step and maintaining readability.\nThe student encounters challenges in plotting the US Mortality Rate series in R. The plot may lack essential labels or a descriptive title, making it difficult to interpret. Additionally, the visualization might be unclear or cluttered, and the R code may lack sufficient comments, hindering understanding of the process. Overall, improvement is needed in effectively plotting time series data in R.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2b: Smoothing SS1PE\nStudents use R to compute exponential smoothing for modeling the US Mortality Rate series. Their R code is well-commented, providing clear explanations for each step of the process, ensuring transparency in the computational process.\nStudents may encounter challenges in implementing exponential smoothing in R, resulting in incomplete or ineffective computations. Their R code might lack sufficient comments, hindering clarity in understanding the computational process.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2c: Smoothing a=0.2\nStudents employ R to repeat the modeling of the US Mortality Rate series using a specified smoothing parameter value of alpha=0.2. Their R code is well-commented, providing clear explanations for each step of the process, ensuring transparency in the computational process.\nStudents may encounter challenges in implementing exponential smoothing in R, resulting in incomplete or ineffective computations. Their R code might lack sufficient comments, hindering clarity in understanding the computational process.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2d: Smoothing a=1/n\nStudents employ R to repeat the modeling of the US Mortality Rate series using a specified smoothing parameter value of alpha=1/n. Their R code is well-commented, providing clear explanations for each step of the process, ensuring transparency in the computational process.\nStudents may encounter challenges in implementing exponential smoothing in R, resulting in incomplete or ineffective computations. Their R code might lack sufficient comments, hindering clarity in understanding the computational process.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2d: Evaluation of Parameter Choice\nStudents justify their choice of parameter in the context of the underlying factors affecting US Mortality Rates evident in the data. The students evidence their understanding of the implications of the values in the smoothing parameter. Students show they have done some background research into the data to answer the question.\nStudents fail to adequately justify their choice of parameter in relation to the underlying factors affecting US Mortality Rates evident in the data. They may lack evidence of understanding the implications of the values in the smoothing parameter or fail to demonstrate how these implications relate to the context of the data. Additionally, they may show limited evidence of background research into the data to support their justification, indicating a lack of depth in their analysis.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3a: Excess MortalityTable\nStudents accurately calculate the excess mortality rate during 2020 and 2021 using the smoothing parameter values employed in the previous question. They present their results in table, clearly displaying the excess mortality rate for each year alongside the corresponding smoothing parameter values. The table is well-labeled and easy to interpret.\nStudents demonstrate inaccuracies in calculating the excess mortality rate during 2020 and 2021. Their presentation of the results in a table may lack clarity and professionalism, with issues such as unclear labeling, inconsistent formatting, or difficulty in interpreting the information provided. Additionally, they may overlook important details or fail to include all necessary information in the table, making it challenging for readers to understand the table.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3b: Excess Mortality\nExplanations effectively convey the meaning of excess mortality to a general audience, avoiding technical terms and providing a clear, accessible description. They define excess mortality as the number of deaths observed during a specific period compared to what would be expected based on historical data.\nResponses may struggle to explain excess mortality clearly to a general audience, potentially using technical language or lacking coherence. They may fail to provide relatable examples or context, making it difficult for the audience to understand the concept and its significance.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3c: Evaluation of assumptions used for inference\nResponses address the challenge of selecting parameter values to make inference in time series. They provide a comprehensive analysis, considering factors like modeling assumptions, and methodological variations that influence parameter selection. Explanations highlight the need of transparent reporting to ensure robust and reliable estimates in professional discourse.\nBelow expectations, responses may lack depth or clarity in addressing the challenge of selecting parameter values for making inference in time series. They may overlook key factors influencing parameter selection, such as data quality or specific characteristics of the time series data. Additionally, they may not effectively consider the impact of modeling assumptions or methodological variations on parameter selection. Furthermore, they may fail to emphasize the importance of transparent reporting in ensuring the reliability and validity of estimates, potentially resulting in a lack of confidence in the conclusions drawn from the analysis.\n\n\nTotal Points\n70",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_2_3.html",
    "href": "hw/homework_2_3.html",
    "title": "Time Series Homework: Chapter 2 Lesson 3",
    "section": "",
    "text": "Code\nind_prod &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/ind_prod_us.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_2_3.html#data",
    "href": "hw/homework_2_3.html#data",
    "title": "Time Series Homework: Chapter 2 Lesson 3",
    "section": "",
    "text": "Code\nind_prod &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/ind_prod_us.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_2_3.html#questions",
    "href": "hw/homework_2_3.html#questions",
    "title": "Time Series Homework: Chapter 2 Lesson 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\nTotal US Industrial Production Index\nhttps://fred.stlouisfed.org/series/IPB50001N\n\n\n\n\n\n\nAnswer\n\n\n\nThe Federal Reserve’s monthly index of industrial production measures the real output of the U.S. industrial sector, which includes manufacturing, mining, and utilities, relative to a 2017 base of 100. The data is reported monthly in index form (not seasonally adjusted) and tracks changes in production activity over time, capturing fluctuations in economic performance. Each observation reflects the current level of industrial output compared to the base year, indicating growth or contraction in these sectors.\n\n\n\n\n\nQuestion 2 - Total US Industrial Production: Correlogram (20 points)\n\na) Please plot a correlogram of the US Industrial Production Index\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\nacf(ind_prod$ind_prod_indx, plot=TRUE, type = \"correlation\", lag.max = 100)\n\n\n\n\n\n\n\n\n\nCode\n# correlogram table .head(10)\nacf_values &lt;- acf(ind_prod$ind_prod_indx, plot = FALSE, type = \"correlation\", lag.max = 100)\nacf_table &lt;- data.frame(Lag = acf_values$lag[1:10], ACF = acf_values$acf[1:10])\n\nacf_table\n\n\n   Lag       ACF\n1    0 1.0000000\n2    1 0.9977744\n3    2 0.9958954\n4    3 0.9940528\n5    4 0.9921186\n6    5 0.9902334\n7    6 0.9883675\n8    7 0.9863674\n9    8 0.9843885\n10   9 0.9825565\n\n\n\n\n\n\nb) Please identify evidence of any trend or seasonal component using the correlogram. Please justify your findings.\n\n\n\n\n\n\nAnswer\n\n\n\nBased on the correlogram of the US Industrial Production Index, there is evidence of a trend due to the slow decline in autocorrelation values across many lags, indicating persistent relationships over time. The high autocorrelation values gradually decrease, which is typical of a trending series. However, there is no clear evidence of a seasonal pattern, as the autocorrelation function (ACF) does not show repeating spikes at regular intervals.\n\n\n\n\n\nQuestion 3 - Total US Industrial Production: Decomposition (10 points)\n\na) Please plot a decomposition of the US Industrial Production Index series. Include the original series, trend, seasonal variation, and random component.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\nind_prod$date &lt;- lubridate::mdy(ind_prod$date)\nind_prod$date &lt;- yearmonth(ind_prod$date)\nind_prod$ind_prod_indx &lt;- as.numeric(ind_prod$ind_prod_indx)\n\nind_tsbl &lt;- as_tsibble(ind_prod, key = NULL, index = date, regular = TRUE)\n# interval(ind_tsbl)\n# has_gaps(ind_tsbl)\n\nind_decom_add &lt;- ind_tsbl |&gt;\n  model(feasts::classical_decomposition(ind_prod_indx,\n          type = \"add\"))  |&gt;\n  components()\nautoplot(ind_decom_add)\n\n\n\n\n\n\n\n\n\nCode\nind_decom_mult &lt;- ind_tsbl |&gt;\n  model(feasts::classical_decomposition(ind_prod_indx,\n          type = \"mult\"))  |&gt;\n  components()\nautoplot(ind_decom_mult)\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Justify your choice of decomposition model (additive vs multiplicative)\n\n\n\n\n\n\nAnswer\n\n\n\nThe time series data is increasing over time, so based on what I have learned so far, we need to use the Multiplicative decomposition model. The seasonally adjusted X needs to account for seasonalities within the year and the overall average over time, so the seasonally adjusted X should do a good job of portraying the trend if there is one. The trend is rising over time, so we have a trend. The variation in the seasonal component between the additive decomposition is larger than the multiplicative decomposition, which we can see in the small bar to the left of the plot. Thus, the multiplicative decomposition does a better job of capturing the seasonal component. It is the same case for the other three components. So, the multiplicative model does a better job of capturing the trend, seasonal, and random components.\n\nThe random component is noticeable because the multiplicative model has a range between 0.8 and 1.25 and, for the most part, stays between the random values of 0.95 and 1.05, so this model does a great job of identifying the ups and downs in this time series. The higher random components in the multiplicative decomposition can be linked to major world events and economic events, like major U.S. and/or world wars, the financial crisis of 2008, and COVID.\nThe Additive decomposition model has random values between -15 and 5. The random component gets larger and larger over time, so it doesn’t capture anything that can be linked to abnormal situations. The add random component fails to captured reasonable world and economic events that can have their effect in this time series. It does not make sense because it starts to go haywire in the late 1990s, and the random component continues to increase thereafter.\n\nThe seasonal component charts below show us a more stable season within the time series, showing a better picture of the seasonal component for both classical decompositions. The additive model assumes a constant seasonal effect, leading it to oscillate around zero hence the large seasonal variations. In the multiplicative model, the seasonal effect varies proportionally with the level of trend, which is increasing, and the additive model fails to capture this.\n\n\nCode\n# Filter to retain only the first 12 months of the seasonal component\nind_decom_add_12_seasonal &lt;- ind_decom_add |&gt; filter(row_number() &lt;= 12) |&gt; select(date, seasonal)\nind_decom_mult_12_seasonal &lt;- ind_decom_mult |&gt; filter(row_number() &lt;= 12) |&gt; select(date, seasonal)\n# Plot additive seasonal component with only the first 12 months\nplot_add_seasonal_12 &lt;- ggplot(ind_decom_add_12_seasonal, aes(x = date, y = seasonal)) +\n  geom_line() +\n  ggtitle(\"Add Decomposition - Seasonal Component (First 12 Months)\") +\n  ylab(\"Seasonal Component\") +\n  xlab(\"Month\")\n# Plot multiplicative seasonal component with only the first 12 months\nplot_mult_seasonal_12 &lt;- ggplot(ind_decom_mult_12_seasonal, aes(x = date, y = seasonal)) +\n  geom_line() +\n  ggtitle(\"Mult Decomposition - Seasonal Component (First 12 Months)\") +\n  ylab(\"Seasonal Component\") +\n  xlab(\"Month\")\n\n# Combine both plots: Add on top, Multi on bottom\nplot_add_seasonal_12 / plot_mult_seasonal_12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4 - Total US Industrial Production: Stationary Series (20 points)\n\na) Please plot a correlogram of the random component of the US Industrial Production Index\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\nacf(ind_decom_add$random |&gt; na.omit(), plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Please interpret the correlogram of the random component of the US Industrial Production Index. Include descriptions of the statistical and practical significance of the results. Be careful to justify the cases when a statistically significant correlation is not practically significant.\n\n\n\n\n\n\nAnswer\n\n\n\nThe correlogram of the random component of the US Industrial Production Index indicates some statistically significant autocorrelations at certain lags, but these may not hold practical significance, as the random component should ideally exhibit minimal pattern. Although the presence of statistically significant correlations could suggest non-random influences, it is important to carefully assess whether these correlations correspond to meaningful economic fluctuations or are simply due to noise. Correlation doesn’t always equal causation so I just need to clarify with Brother Moncayo on how to properly answer this question.\n\n\n\n\n\nQuestion 5 - US Industrial Production Index: Introspection (20 points)\n\na) Why is it important to remove trend and seasonal variation before plotting and analyzing correlograms?\n\n\n\n\n\n\nAnswer\n\n\n\nRemoving trend and seasonal variation is essential to reveal the true autocorrelation structure of the time series, as trends and seasonal patterns can introduce misleadingly high or low correlations at various lags. By eliminating these components, the correlogram becomes a more accurate tool for identifying short-term relationships within the data. This step is crucial for meeting the stationarity assumption needed for reliable time series analysis.\n\n\n\n\nb) Please speculate on the importance of autocorrelation analysis of the random component of time series data on its modeling and investigation?\n\n\n\n\n\n\nAnswer\n\n\n\nAnalyzing the autocorrelation of the random component helps to understand the underlying fluctuations and irregularities in the time series, highlighting the small-scale dynamics that may not be apparent from the main trend or seasonal patterns. This analysis can reveal subtle patterns or dependencies within the residuals, indicating if there are any remaining structures, such as minor trends or cyclical behavior, that could impact the model’s accuracy. It provides deeper insights into the data’s behavior, aiding in refining the model and ensuring it adequately captures all significant features.\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 2a: Correlogram\nThe student plots a correlogram of the time series requested. The plot accurately displays autocorrelation values at various lags. If code is well-commented, providing clarity on the plotting process. The labels, title, and legends are appropriate and match the quality of the illustrations in the Time Series notebook.\nThe student attempts to plot a correlogram of the time series requested but encounters significant errors or lacks clarity in their plot. If code is used, it may lack sufficient commenting or coherence, making it challenging to understand the plotting process. Overall, the plot may lack detail or accuracy, highlighting areas for improvement in time series visualization skills.\n\n\n\n\nMastery (15)\nIncomplete (0)\n\n\n\nQuestion 2b: Interpretation\nThe student effectively interprets the correlogram to identify evidence of trend or seasonal components in the time series. Their description matches the textbook description in page 37.\nThe student attempts to interpret the correlogram but encounters errors or lacks clarity in their analysis. There may be inaccuracies in interpreting autocorrelation values or misinterpretation of the findings, indicating a limited understanding of correlogram analysis techniques. Overall, the justification for findings may lack depth or accuracy.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3a: Decomposition\nThe student plots a decomposition of the US Industrial Production Index series, including the original series, trend, seasonal variation, and random component. The code is well-commented, providing clarity on the decomposition process. The labels, title, and legends are appropriate and enhance the understanding of the plot, matching the quality of illustrations in the Time Series notebook.\nThe student attempts to plot a decomposition of the US Industrial Production Index series but encounters significant errors or lacks clarity in their plot. The code lacks sufficient commenting or coherence, making it challenging to understand the decomposition process. Overall, the plot may lack detail or accuracy.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3b: Modeling Justification\nProvides a well-reasoned justification for choosing either the additive or multiplicative decomposition model, clearly explaining how the data’s characteristics (e.g., seasonality, trend) influence the choice. | Fails to provide a clear or logical justification, or the explanation is incorrect or unsupported by the data’s characteristics.\n\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 4a: Correlogram of random component\nThe student plots a correlogram of the time series requested. The plot accurately displays autocorrelation values at various lags. If code is well-commented, providing clarity on the plotting process. The labels, title, and legends are appropriate and match the quality of the illustrations in the Time Series notebook.\nThe student attempts to plot a correlogram of the time series requested but encounters significant errors or lacks clarity in their plot. If code is used, it may lack sufficient commenting or coherence, making it challenging to understand the plotting process. Overall, the plot may lack detail or accuracy, highlighting areas for improvement in time series visualization skills.\n\n\n\n\nMastery (15)\nIncomplete (0)\n\n\n\nQuestion 4b: Interpretation\nClearly interprets the correlogram, explaining the statistical significance of correlations and addressing practical significance. Provides well-reasoned justification for when statistically significant correlations are not practically important. | Fails to interpret the correlogram accurately, does not explain statistical or practical significance clearly, or provides weak justification for distinguishing between statistical and practical significance. |\n\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 5a: Introspection\nThe student explains the importance of removing trend and seasonal variation before analyzing correlograms, showing an understanding of stationarity assumptions in time series analysis. They recognize that trend and seasonality violate these assumptions, potentially distorting autocorrelation patterns.\nThe student attempts to explain the importance of removing trend and seasonal variation before analyzing correlograms but may struggle with clarity or accuracy. Their understanding of stationarity assumptions in time series analysis might be limited, leading to inconsistencies or inaccuracies. Overall, their explanation may lack depth, indicating areas for improvement in understanding preprocessing steps and stationarity assumptions.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 5b: Introspection\nThe student effectively speculates on the importance of autocorrelation analysis of the random component in time series data for modeling, investigation, and forecasting. Their discussion shows understanding of the topics we have already covered in class. The submission shows effort.\nOverall, their explanation may lack depth,\nor clarity.\n\n\n\n\n\n\n\n\n\nTotal Points\n80",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_2_1.html",
    "href": "hw/homework_2_1.html",
    "title": "Time Series Homework: Chapter 2 Lesson 1",
    "section": "",
    "text": "Code\nokuns &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/outputgap_and_cyclical_unemp.xlsx\") # okuns1\n\ngs_night &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/nightstand-sweat.csv\") # night1",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_2_1.html#data",
    "href": "hw/homework_2_1.html#data",
    "title": "Time Series Homework: Chapter 2 Lesson 1",
    "section": "",
    "text": "Code\nokuns &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/outputgap_and_cyclical_unemp.xlsx\") # okuns1\n\ngs_night &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/nightstand-sweat.csv\") # night1",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_2_1.html#questions",
    "href": "hw/homework_2_1.html#questions",
    "title": "Time Series Homework: Chapter 2 Lesson 1",
    "section": "Questions",
    "text": "Questions\nDoing example for Investment Society\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\na) Output Gap\nhttps://chat.openai.com/share/122aaad9-2be6-43ec-b58a-e1858305b401\n\n\n\n\n\n\nAnswer\n\n\n\nThe output gap is the difference between actual GDP and potential GDP, measured as a percentage of potential GDP. Potential GDP is the level of output that an economy can sustain over the long term without generating inflationary pressures. Data on the output gap is often collected by government institutions like central banks (the Federal Reserve or IMF), and each observation reflects how far an economy’s production deviates from its long-term sustainable level.\n\n\n\n\nb) Cyclical Unemployment\nhttps://chat.openai.com/share/7d6bf187-41d0-42c3-98bc-d02ea1bd5b80\n\n\n\n\n\n\nAnswer\n\n\n\nCyclical unemployment refers to unemployment that results from economic downturns or recessions, where demand for goods and services decreases, leading to job losses. Data on cyclical unemployment is usually derived by subtracting the natural rate of unemployment (which accounts for frictional and structural unemployment) from the total unemployment rate. Each observation represents the proportion of the labor force that is unemployed due to fluctuations in the business cycle.\n\n\n\n\n\nQuestion 2 - Covariance and Correlation: Okun’s Law (30) points)\nOkun’s Law is an empirical relationship defined as a negative correlation between the Output Gap and Cyclical Unemployment. If the economy is expanding, businesses are producing more, and unemployment tends to decrease. Conversely, during economic contractions or recessions, output shrinks, leading to an increase in unemployment.\nPlease use the data okuns to test whether Okun’s Law applies to the US from 1960 to 2021.\n\na) Please create a scatter plot of the Output Gap in the x-axis and Cyclical Unemployment in the y-axis.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Convert 'Date' column to yearquarter format\nokuns$Date &lt;- lubridate::ymd(okuns$Date) # okuns2\nokuns$Date &lt;- yearquarter(okuns$Date) # okuns3\n\n# Convert the data frame to a tsibble\nokuns_tsbl &lt;- as_tsibble(okuns, key = NULL, index = Date, regular = TRUE) # okuns4\n\n\n\n# okus1 raw data file\n# okuns2 convert into a standardized date format (Year-Month-Day).\n# okuns3 yearquarter(okuns7$Date) is converting the 'Date' into year-quarter format. This function changes the Date object into a format that groups the data by year and quarter (e.g., \"2023 Q1\"). easier to analyze data at a quarterly level.\n# okuns4 as_tsibble function is converting okuns df into a \"tsibble\" (time series tibble). It uses the Date column as the time index, sets no key (key = NULL), and specifies that the data is expected to be equally spaced in time (regular = TRUE).\n# okuns5 & 6 gets rid of spaces in column for better manipulation\n\n# okuns7 The formula used in the code corresponds to the calculation of the COVARIANCE between two variables, output_gap and cycl_unemp. The code calculates the deviation of each value from the mean for both output_gap and cycl_unemp, multiplies these deviations, and assigns this result to the sq_dev column. \n# The formula for each row is:(x_bar - x_i)(y_bar - Y_i)\n\n# the book formula has the output gap minus the mean of the output gap, but this has the mean of the outputgap minus the outputgap! same for cycli. Ask Bro. Moncayo\n\n# okuns 8 \n# interval(okuns8_tsbl) is checking the time interval or frequency of the time series data, ensuring the observations are recorded at regular intervals (e.g., quarterly, yearly).\n# has_gaps(okuns8_tsbl) checks if there are any missing time periods in the okuns8_tsbl time series object, ensuring that there are no gaps in the sequence of dates.\n\n\n\n\n# Rename columns to avoid spaces and for consistency\nokuns_tsbl &lt;- okuns_tsbl |&gt;\n  rename(output_gap = `Output Gap`) |&gt; # okuns5\n  rename(cycl_unemp = `Cyclical Unemployment`) # okuns6\n\n# Create a new column 'sq_dev' with calculated square deviations\nokuns_tsbl &lt;- okuns_tsbl |&gt; \n  mutate(\n    sq_dev = (mean(output_gap) - output_gap) * (mean(cycl_unemp) - cycl_unemp)\n  ) # okuns7\n# (mean(output_gap) - output_gap) * (mean(cycl_unemp) - cycl_unemp)\n# (output_gap - mean(output_gap)) * (cycl_unemp - mean(cycl_unemp)) correct way?\n# they both seem to produce the same thing, why? \n\n\n\n## Troubleshooting - Check the time interval and gaps in the tsibble\ninterval(okuns_tsbl)\n\n\n&lt;interval[1]&gt;\n[1] 1Q\n\n\nCode\nhas_gaps(okuns_tsbl) # okuns8\n\n\n# A tibble: 1 × 1\n  .gaps\n  &lt;lgl&gt;\n1 FALSE\n\n\nCode\n# Create ggplot with color based on the sign of 'sq_dev'\nggplot(\n  data = okuns_tsbl,                   # Data source (tsibble)\n  aes(x = output_gap, y = cycl_unemp,  # Mapping aesthetics: x-axis, y-axis\n      color = factor(sign(sq_dev)))    # Color based on the sign of 'sq_dev'\n) +\n  geom_point(alpha = 0.5, size = 2) +   # Add points with specified transparency and size\n  scale_color_manual(\n    values = c(\"deepskyblue\", \"coral\"),  # Manual color scale for 'Negative' and 'Positive'\n    labels = c(\"Negative\", \"Positive\")   # Labels for the color legend\n  ) +\n  labs(\n    x = \"Output Gap\",                    # x-axis label\n    y = \"Cyclical Unemployment\",         # y-axis label\n    title = \"Okun's Law\",                # Plot title\n    color = 'Square Deviations'          # Color legend title\n  ) +\n  theme(plot.title = element_text(hjust = 0.5)) +  # Center the plot title\n  geom_hline(yintercept = mean(okuns_tsbl$cycl_unemp),  # Add horizontal dashed line at mean(cycl_unemp)\n             linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = mean(okuns_tsbl$output_gap),  # Add vertical dashed line at mean(output_gap)\n             linetype = \"dashed\", color = \"black\") +\n  scale_x_continuous(\n    breaks = c(mean(okuns_tsbl$output_gap)),  # Specify x-axis tick positions\n    labels = expression(bar(x))               # Latex x-axis tick label\n  ) +\n  scale_y_continuous(\n    breaks = c(mean(okuns_tsbl$cycl_unemp)),  # Specify y-axis tick positions\n    labels = expression(bar(y))               # Latex x-axis tick label\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Please calculate the covariance and correlation coefficient between the Output Gap and Cyclical Unemployment.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\n# Calculate the covariance and correlation\ncovariance &lt;- cov(okuns_tsbl$output_gap, okuns_tsbl$cycl_unemp)\ncorrelation &lt;- cor(okuns_tsbl$output_gap, okuns_tsbl$cycl_unemp) # okunsb\n\n# okunsb \n# covariance &lt;- cov(okuns8_tsbl$output_gap, okuns8_tsbl$cycl_unemp): calculates the covariance between the \"output_gap\" and \"cycl_unemp\" in the okuns_tsbl time series tibble, which measures how much these two variables change together.\n\n# correlation &lt;- cor(okuns8_tsbl$output_gap, okuns8_tsbl$cycl_unemp): calculates the correlation between the same two columns, which normalizes the covariance to provide a standardized measure of the strength and direction of the linear relationship between the two variables, ranging from -1 to 1.\n\n\n# Create a table\nresult_table_1 &lt;- data.frame(\n  Statistic = c(\"Covariance\", \"Correlation\"),\n  Value = c(covariance, correlation)\n)\n\n# Display the table\nresult_table_1 %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nCovariance\n-3.5311717\n\n\nCorrelation\n-0.8861978\n\n\n\n\n\n\n\n\n\n\n\nc) Evaluate Okun’s law. Is there sufficient evidence to suggest an empirical relationship between the Output Gap and Cyclical Unemployment?\n\n\n\n\n\n\nAnswer\n\n\n\nOkun’s Law suggests a negative relationship between the output gap and cyclical unemployment. Based on the covariance of -3.5311717 and the strong negative correlation of -0.8861978, there is sufficient evidence to support an empirical relationship between these variables. This indicates that as the output gap decreases (reflecting slower economic growth), cyclical unemployment tends to rise, consistent with Okun’s Law.\n\n\n\n\n\nQuestion 3 - Correlation vs Causation: Spurious Searches (30 points)\n\n“Every single person who confuses correlation and causation ends up dying.”\nHannah Fry\n\nPlease use the data gs_night to evaluate the empirical relationship between the Google search terms for night sweats and nightstand.\n\na) Please create a scatter plot of the night sweat search index in the x-axis and nightstand search index in the y-axis.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\ngs_night$date &lt;- lubridate::ym(gs_night$date) #night2\ngs_night$date &lt;- yearmonth(gs_night$date) #night3\n\ngs_night_tsbl &lt;- as_tsibble(gs_night, key = NULL, index = date, regular = TRUE) # night4 \n\ngs_night_tsbl &lt;- gs_night_tsbl |&gt;\n  rename(night_sweat= `night sweat`) # night5\n\ngs_night_tsbl &lt;- gs_night_tsbl |&gt; \n  mutate(sq_dev= (mean(nightstand)-nightstand)*(mean(night_sweat)-night_sweat)) # night6\n\n# night1 raw df\n# night2: Convert the 'date' column into a Year-Month (ym) format using the lubridate package.\n# night3: Further transform the 'date' into a year-month object, making it suitable for time series analysis.\n\n# night4: Convert the dataframe into a tsibble (time series tibble) with 'date' as the index, ensuring the data is evenly spaced (regular = TRUE).\n\n# night5: Rename the 'night sweat' column to 'night_sweat' for easier manipulation.\n\n# night6: Create a new column 'sq_dev' by calculating the product of the deviations from the mean for both 'nightstand' and 'night_sweat'.\n\n\nggplot(\n  gs_night_tsbl,\n  aes(x = nightstand, y = night_sweat, color = factor(sign(sq_dev)))\n) +\n  geom_point(alpha = 0.5, size = 2) +\n  scale_color_manual(\n    values = c(\"deepskyblue\", \"coral\"),  # Adjust colors as needed\n    labels = c(\"Negative\", \"Positive\")\n  )+\n  labs(\n    x = \"Nightstand\",\n    y = \"Night Sweat\",\n    title = \"Google Search Covariance: Nightstand and Night Sweat\",\n    color = 'Square Deviations'\n  )+\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )+\n  geom_hline(yintercept = mean(gs_night_tsbl$night_sweat), linetype = \"dashed\", color = \"black\")+\n  geom_vline(xintercept = mean(gs_night_tsbl$nightstand), linetype = \"dashed\", color = \"black\")+\n  scale_x_continuous(\n    breaks = c(mean(gs_night_tsbl$nightstand)),\n    labels = expression(bar(x))\n  )+\n  scale_y_continuous(\n    breaks = c(mean(gs_night_tsbl$night_sweat)),\n    labels = expression(bar(y))\n  ) # night7\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Please calculate the covariance and correlation between the night sweat search index and the nightstand search index.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\ncovariance &lt;- cov(gs_night_tsbl$nightstand, gs_night_tsbl$night_sweat)\ncorrelation &lt;- cor(gs_night_tsbl$nightstand, gs_night_tsbl$night_sweat) # night8\n\n# night8 \n# covariance - calculates the covariance between \"nightstand\" and \"night_sweat\" in the gs_night_tsbl time series tibble, which quantifies how much these two variables vary together.\n\n# correlation &lt;- cor(gs_night_tsbl$nightstand, gs_night_tsbl$night_sweat): computes the correlation between the same two columns, providing a normalized measure of the strength and direction of their linear relationship. The result ranges from -1 (perfect negative relationship) to 1 (perfect positive relationship).\n\n\n# Create a table\nresult_table_2 &lt;- data.frame(\n  Statistic = c(\"Covariance\", \"Correlation\"),\n  Value = c(covariance, correlation)\n)\n\n# Display the table\nresult_table_2 %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nCovariance\n271.0499827\n\n\nCorrelation\n0.7871518\n\n\n\n\n\n\n\n\n\n\n\nc) Please suggest a scenario where an increase in the frequency of searches for nightstand will cause higher searches for the term night sweats?\n\n\n\n\n\n\nAnswer\n\n\n\nScenario where an increase in the frequency of searches for nightstands causes higher searches for the term night sweats. A popular furniture brand releases a new line of nightstands with integrated cooling features to help with heat management at night. As people search for these nightstands, they become more aware of night sweats as a symptom related to nighttime overheating, prompting an increase in searches for night sweats to understand more about this condition.\n\n\n\n\nd) Please suggest a scenario where an increase in the frequency of searches for night sweats will cause higher searches for the term nightstand?\n\n\n\n\n\n\nAnswer\n\n\n\nScenario where an increase in the frequency of searches for night sweats causes higher searches for the term nightstand. A health campaign about the symptoms of sleep disorders increases awareness about night sweats. As people search for remedies, they may come across suggestions for keeping items like water or cooling devices on a nightstand, leading them to search for “nightstand” to explore better furniture options for addressing nighttime comfort.\n\n\n\n\ne) Please suggest an event or reason that would result in the correlation between searches for night sweats and searches for the term nightstand?\n\n\n\n\n\n\nAnswer\n\n\n\nEvent or reason that could correlate searches for night sweats and nightstand. A sudden rise in summer temperatures leads to a widespread increase in discomfort at night, causing people to search both for night sweats (as they experience symptoms) and for nightstand (as they seek solutions like placing fans, water, or cooling gadgets near their beds).\n\n\n\n\nf) Are any of your suggestions plausible? What does that teach you about the difference between correlation and causation? Please elaborate.\n\n\n\n\n\n\nAnswer\n\n\n\nThe calculated covariance is 271.0499827 and the correlation coefficient is 0.7871518​. This indicates a moderately strong positive correlation between searches for night sweats and nightstand. However, correlation does not equal causation.\nNightstand releases with cooling features causing more searches for night sweats. This scenario is possible but unlikely. The idea that a new product could make people more aware of night sweats, leading to increased searches, seems forced. While a cooling nightstand could theoretically help with heat at night, it’s not reasonable to assume that most users would link this directly to night sweats unless it is specifically marketed as such. Therefore, the correlation between the two search terms may be coincidental.\nHealth campaigns on night sweats driving searches for nightstands. This scenario is also possible but less plausible than it might initially seem. While people might seek out nightstands for convenience or comfort after learning about night sweats, it’s a stretch to assume that most individuals suffering from night sweats would search for furniture to solve the issue directly.\nCorrelation vs. Causation:\nCorrelation simply means that two variables tend to move together—either increasing or decreasing simultaneously—but it doesn’t tell us why this happens. Causation, on the other hand, implies that changes in one variable directly cause changes in another.\nIn this case, while the data shows a strong correlation between searches for night sweats and nightstand, this does not mean that one causes the other. As seen in the evaluation of the scenarios, plausible explanations for the correlation often involve a third external factor (e.g., weather) influencing both search terms. This highlights the importance of not assuming causality just because two variables are correlated. In reality, correlation can often result from unrelated factors, coincidences, or underlying variables that drive both outcomes.\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\nCriteria\nRatings\n\n\n\nComplete (10)\n\n\nQuestion 1: Context and Measurement\nThe student demonstrates a clear understanding of the context for each data series. The explanation includes details about the data collection process, unit of analysis, and the meaning of each observation.\n\n\n\nComplete (5)\n\n\nQuestion 2a: Scatter Plot\nThe scatter plot is appropriately titled, and all elements, including the plot itself, axis labels, and title, are clearly labeled. The scatter plot matches or exceeds the quality of the scatter plots shown in lecture.\n\n\n\nComplete (5)\n\n\nQuestion 2b: Covariance and Correlation Calculation\nThe student effectively utilizes R to correctly calculate covariance and correlation.\n\n\n\nComplete (10)\n\n\nQuestion 2c:Interpretation and Evaluation\nThe student provides a clear and accurate interpretation of the correlation between the variables. The response demostrates understanding of how the correlation coefficient quantifies the strength and direction of the relationship between the variables.\n\n\n\nComplete (10)\n\n\nQuestion 2c: Okun’s Law\nThe discussion of Okun’s law makes clear the student understands how to use statistical evidence to evaluate scientific claims in the context of an academic field.\n\n\n\nComplete (5)\n\n\nQuestion 3a: Scatter Plot\nThe scatter plot is appropriately titled, and all elements, including the plot itself, axis labels, and title, are clearly labeled. The scatter plot matches or exceeds the quality of the scatter plots shown in lecture.\n\n\n\nComplete (5)\n\n\nQuestion 3b: Covariance and Correlation Calculation\nThe student effectively utilizes R to correctly calculate covariance and correlation.\n\n\n\nComplete (10)\n\n\nQuestion 3(c,d,e):Scenarios\nThe scenarios suggested are clear and concise. The responses show an honest attempt at thinking of a connection between the variables according to the prompt.\n\n\n\nComplete (10)\n\n\nQuestion 3: Plausiblity\nThe student provides a critical evaluation is provided regarding the plausibility of the suggested scenarios and events. The evaluation includes a clear and comprehensive explanation on the difference between correlation and causation.\n\n\nTotal Points\n70",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_1_4.html",
    "href": "hw/homework_1_4.html",
    "title": "Time Series Homework: Chapter 1 Lesson 4",
    "section": "",
    "text": "Show the code\n# Weather data for Rexburg\nrex_temp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_1_4.html#data",
    "href": "hw/homework_1_4.html#data",
    "title": "Time Series Homework: Chapter 1 Lesson 4",
    "section": "",
    "text": "Show the code\n# Weather data for Rexburg\nrex_temp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_1_4.html#questions",
    "href": "hw/homework_1_4.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1: Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\na) Rexburg, ID Daily High Temperatures\n\n\n\n\n\n\nAnswer\n\n\n\nThe data consist of the daily high for the Rexburg airport so each observation reflects the highest fahrenheit temperature recorded at the Rexburg airport. The data is giving daily from 1999/01/02 to 2023/12/20. Data is pretty straight forward.\n\n\nShow the code\n# it is important to change to a tsibble for time series analysis. The idea is to have an index that works with time series formulas, and it is also important to make new data frames have the same index format. \n# One of my stumbling block for this hw was not having the same index format for some of the new df that where made so it delay me a lot\n# also, first reading the class samples and the steps done, then possibly do it in excel to understand how my data will look. \n\n\n\ntemps_ts &lt;- rex_temp |&gt;\n  arrange(dates) |&gt; # Sorting oldest to newest\n  dplyr::select(dates, rexburg_airport_high) |&gt;\n  rename(temp_high = rexburg_airport_high) |&gt;\n  mutate(dates = as.Date(dates)) |&gt; # Ensure dates is a Date object\n  as_tsibble(index = dates) # Convert to tsibble object\n\n\nFirst few rows of tsibble follow\n\n\nShow the code\ntemps_ts |&gt;\n  head()\n\n\n# A tsibble: 6 x 2 [1D]\n  dates      temp_high\n  &lt;date&gt;         &lt;int&gt;\n1 1999-01-02        30\n2 1999-01-03        25\n3 1999-01-04        26\n4 1999-01-05        29\n5 1999-01-06        32\n6 1999-01-07        31\n\n\n\n\n\n\n\n\nQuestion 2: Visualization (5 points)\nPlease plot the Rexburg Daily Temperature series choosing the range and frequency to illustrate the data in the most readable format. Use the appropriate axis labels, units, and captions.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Please provide your code here\n\nplot_ly(temps_ts, x = ~dates, y = ~temp_high, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    title = \"Rexburg Daily High Temperature\",\n    xaxis = list(title = \"Day\"),\n    yaxis = list(title = \"Rexburg High Temp\"),\n    plot_bgcolor = \"white\",  # optional: sets background color to white\n    title_x = 0.5  # centers the title\n  )\n\n\n\n\n\n\n\n\n\n\nQuestion 3: Additive Decomposition - Manual Approach (25 points)\nThis exercise will guide you through all the steps to conduct an additive decomposition of the Rexburg Daily Temperature Series. The first step is to aggregate the daily series to a monthly frequency to ease on the calculation. The code below accomplished the task.\n\na) Please use AI to comment and explain the steps of the code below. Replace the code below with the fully explained code.\n\n# Weather data for Rexburg\nmonthly_tsibble &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\") |&gt; \n  # Convert 'dates' to Date format\n  mutate(date2 = ymd(dates)) |&gt;\n  # Extract year and month from 'date2'\n  mutate(year_month = yearmonth(date2)) |&gt;\n  # Group data by 'year_month'\n  group_by(year_month) |&gt;\n  # Calculate mean of 'rexburg_airport_high' for each group\n  summarize(average_daily_high_temp = mean(rexburg_airport_high)) |&gt;\n  # Remove grouping\n  ungroup() |&gt; \n  # Convert data frame to time series tibble\n  as_tsibble(index = year_month)\n\n# Display the resulting tibble\n#view(monthly_tsibble) \n\n\n\nb) Please calculate the centered moving average of the Rexburg Monthly Temperature Series. Plot the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Doing m_hat & s_hat formulas & plotting m_hat\nmonthly_tsibble &lt;- monthly_tsibble |&gt;\n  mutate( # allows to do calculations - doing - m_hat & s_hat\n    m_hat = (\n          (1/2) * lag(average_daily_high_temp, 6)\n          + lag(average_daily_high_temp, 5)\n          + lag(average_daily_high_temp, 4)\n          + lag(average_daily_high_temp, 3)\n          + lag(average_daily_high_temp, 2)\n          + lag(average_daily_high_temp, 1)\n          + average_daily_high_temp\n          + lead(average_daily_high_temp, 1)\n          + lead(average_daily_high_temp, 2)\n          + lead(average_daily_high_temp, 3)\n          + lead(average_daily_high_temp, 4)\n          + lead(average_daily_high_temp, 5)\n          + (1/2) * lead(average_daily_high_temp, 6)\n        ) / 12,\n    s_hat = average_daily_high_temp - m_hat # s_hat calculation\n  )\n\n# Plot the CMA (centred moving average) and original data\nplain_plot &lt;- autoplot(monthly_tsibble, .vars = m_hat) +\n  labs(\n    x = \"Date\",\n    y = \"Rexburg Monthly avg High\",\n    title = \"Rexburg Monthly CMA\"\n  ) +\n  scale_y_continuous(limits = c(min(monthly_tsibble$m_hat, na.rm = TRUE), max(monthly_tsibble$m_hat, na.rm = TRUE))) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Plot the original unemployment data with CMA overlay\nfancy_plot &lt;- autoplot(monthly_tsibble, .vars = average_daily_high_temp) +\n  labs(\n    x = \"Date\",\n    y = \"Rexburg Monthly avg High\",\n    title = \"Rexburg Monthly CMA\"\n  ) +\n  geom_line(aes(x = year_month, y = m_hat), color = \"#D55E00\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine the two plots\nplain_plot\n\n\n\n\n\n\n\n\n\nShow the code\nfancy_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nc) Please calculate the seasonally adjusted means series. Plot the series\nIn this just the monthly additive effect?\n\n\nShow the code\n# Please provide your code here\n# Extracting year and month from the year_month column\n# adding s_bar manually do to joining errors\nmonthly_tsibble_extended &lt;- monthly_tsibble %&gt;%\n  mutate(\n    year = year(year_month),\n    month = month(year_month),\n    s_bar = case_when(\n      month == 1  ~ -28.7455842,\n      month == 2  ~ -24.7455412,\n      month == 3  ~ -11.7605428,\n      month == 4  ~ -0.4256582,\n      month == 5  ~ 9.9075202,\n      month == 6  ~ 19.4142229,\n      month == 7  ~ 30.0205787,\n      month == 8  ~ 28.1817736,\n      month == 9  ~ 17.5450445,\n      month == 10 ~ 1.6857404,\n      month == 11 ~ -14.1466707,\n      month == 12 ~ -26.9308833\n    )\n  )\n\n\n# Initialize an empty list to store the sums for each month\nmonthly_averages &lt;- vector(\"list\", 12)\n\n# Loop through months 1 to 12\nfor (month in 1:12) {\n  # Calculate the sum of s_hat for each month and store in the list\n  monthly_averages[[month]] &lt;- (mean(monthly_tsibble_extended$s_hat[monthly_tsibble_extended$month == month], na.rm = TRUE))\n}\n\n# Optionally, you can convert the list to a named vector for clearer output\nnames(monthly_averages) &lt;- month.name  # Assigning month names for clarity\n\n# Calculate the overall mean of the unadjusted monthly additive component\noverall_mean_of_the_unadjusted_monthly_additive_component &lt;- sum(unlist(monthly_averages)) / 12\n\n# Compute the seasonally adjusted mean for each month\nseasonally_adjusted_mean &lt;- sapply(monthly_averages, function(x) x - overall_mean_of_the_unadjusted_monthly_additive_component)\n\n# Naming the results for the seasonally adjusted means for clarity\nnames(seasonally_adjusted_mean) &lt;- month.name\n\n# Display the overall mean and the seasonally adjusted means\noverall_mean_of_the_unadjusted_monthly_additive_component\n\n\n[1] -0.00853748\n\n\nShow the code\n# Create a data frame for the results\ns_bar_df &lt;- data.frame(\n  month = month.name,\n  s_hat_bar = unlist(monthly_averages),\n  s_bar = unlist(seasonally_adjusted_mean)\n)\n\n# formulas for random and seasonally adjusted x\nadjusted_ts &lt;- monthly_tsibble_extended |&gt;\n  mutate(random = average_daily_high_temp - m_hat - s_bar) |&gt; \n  mutate(seasonally_adjusted_x = average_daily_high_temp - s_bar)\n\n# note difference between s_bar resuts\n# the last mutate, seasonally_adjusted_x is the SEASONALLY ADJUSTED SERIES\n# and just having the s_bar or seasonally_adjusted_mean (12 months only) is the SEASONALLY ADJUSTED MEANS \n\n\nplotting seasonally adjusted mean\n\n\nShow the code\n# plotting seasonally adjusted mean\nfancy_plot &lt;- autoplot(adjusted_ts, .vars = seasonally_adjusted_x) +\n  labs(\n    x = \"Date\",\n    y = \"Seasonally Adjusted Mean\",\n    title = \"Seasonally Adjusted Mean for Each Month\"\n  )\nfancy_plot\n\n\n\n\n\n\n\n\n\nShow the code\n# the seasonally adjusted mean for each month refers to just the 12 months not the adjusted series. but not exact which one we should use.\n\n\n\n\nd) Please calculate the random component. Please plot the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Plot of the random component\nfancy_plot &lt;- autoplot(adjusted_ts, .vars = random) +\n  labs(\n    x = \"Date\",\n    y = \"Random\",\n    title = \"Random Component\"\n  )\nfancy_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4: Additive Decomposition - R’s Decompose (15)\n\na) Please use additive decomposition model described in the Time Series notebook to decompose the Rexburg Monthly Temperature Series. Plot the algorithm’s output.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\n# Please provide your code here\n# Compute the additive decomposition for adjusted_ts\ntemps_decompose_add &lt;- adjusted_ts |&gt;\n  model(feasts::classical_decomposition(average_daily_high_temp,\n          type = \"add\"))  |&gt;\n  components()\n\n# Compute the multiplicative decomposition for adjusted_ts\ntemps_decompose_mult &lt;- adjusted_ts |&gt;\n  model(feasts::classical_decomposition(average_daily_high_temp,\n          type = \"mult\"))  |&gt;\n  components()\n\n# Display 14 rows of both decompositions\n# temps_decompose_mult |&gt;\n#  head(14)\n\ntemps_decompose_add |&gt;\n  head(14)\n\n\n# A dable: 14 x 7 [1M]\n# Key:     .model [1]\n# :        average_daily_high_temp = trend + seasonal + random\n   .model  year_month average_daily_high_t…¹ trend seasonal random season_adjust\n   &lt;chr&gt;        &lt;mth&gt;                  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1 \"feast…   1999 Jan                   30.3  NA    -28.7   NA              59.1\n 2 \"feast…   1999 Feb                   33.7  NA    -24.7   NA              58.5\n 3 \"feast…   1999 Mar                   44.4  NA    -11.8   NA              56.1\n 4 \"feast…   1999 Apr                   52.6  NA     -0.426 NA              53.0\n 5 \"feast…   1999 May                   61.3  NA      9.91  NA              51.4\n 6 \"feast…   1999 Jun                   71.4  NA     19.4   NA              52.0\n 7 \"feast…   1999 Jul                   82.1  56.2   30.0   -4.16           52.1\n 8 \"feast…   1999 Aug                   83.0  56.4   28.2   -1.59           54.8\n 9 \"feast…   1999 Sep                   71.8  56.6   17.5   -2.31           54.3\n10 \"feast…   1999 Oct                   63.0  57.0    1.69   4.25           61.3\n11 \"feast…   1999 Nov                   53.1  57.6  -14.1    9.66           67.2\n12 \"feast…   1999 Dec                   28.2  58.0  -26.9   -2.89           55.1\n13 \"feast…   2000 Jan                   30.3  58.3  -28.7    0.654          59.0\n14 \"feast…   2000 Feb                   37.2  58.6  -24.7    3.31           62.0\n# ℹ abbreviated name: ¹​average_daily_high_temp\n\n\n\n\n\n\nb) You can also decompose without aggregating the data series. The following code completes the additive decomposition with the original data set. Please use AI to comment and explain how to accomplish the task. Replace the code below with the fully explained code.\n\n\nShow the code\n# Weather data for Rexburg\ndaily_tsibble &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\") |&gt;\n  mutate(year_month_day = ymd(dates)) |&gt;  # Convert date strings to Date objects\n  dplyr::select(-imputed, -dates) |&gt;  # Remove 'imputed' and 'dates' columns\n  as_tsibble(index = year_month_day)  # Convert the tibble to a tsibble with dates as the index\n\n# Display the first few rows of the 'daily_tsibble'\ndaily_tsibble %&gt;% head\n\n\n# A tsibble: 6 x 2 [1D]\n  rexburg_airport_high year_month_day\n                 &lt;int&gt; &lt;date&gt;        \n1                   30 1999-01-02    \n2                   25 1999-01-03    \n3                   26 1999-01-04    \n4                   29 1999-01-05    \n5                   32 1999-01-06    \n6                   31 1999-01-07    \n\n\nShow the code\n# Decompose the daily high temperature series into seasonal components\ndaily_decompose &lt;- daily_tsibble  |&gt;\n  model(feasts::classical_decomposition(rexburg_airport_high ~ season(365.25),\n                                        type = \"add\"))  |&gt;\n  components()\n\n# Plot the seasonal component of the decomposition\ndaily_decompose |&gt; autoplot(.vars = seasonal)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5: Seasonally Adjusted Series - Analysis (20 points)\n\na) Justify why we use the additive decomposition model to seasonally adjust the Rexburg Daily Temperature series.\n\n\n\n\n\n\nAnswer\n\n\n\nThe Rexburg Daily Temperature series displays a consistent seasonal pattern throughout the period observed, which is a key for an additive model where seasonal fluctuations are stable and do not vary in magnitude over time. The temperature data does not exhibit a multiplicative trend where seasonal variations would proportionally increase or decrease as the level of the series changes. Instead, the variations around the trend appear consistent over time, suggesting that the additive model, where seasonal effects are assumed to be constant through the series, is more appropriate.\nThe deterministic nature of the trend in the temperature data supports the use of an additive model. This model assumes that the components of the series (trend, seasonality, and randomness) can be linearly added or subtracted to model or decompose the series. Since there is no evidence of increasing variability with temperature values, a characteristic more aligned with multiplicative models. The additive assumption holds more relevance. This allows for clearer interpretation and more accurate seasonal adjustment of the data.\n\n\n\n\nb) You calculated the random component of the series using three different procedures. Are the random component series the same? Are there patterns that are similar across all the random component series? Propose an explanation for the commonalities.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nIn the Classical decomposition, the error component is determined by removing trend and seasonal from the original time series, which is the same as the normal additive random component we do in this example. I’m honestly confused on this part because replicating some of the other random series we did in class, and also doing the multiplicative random graph made almost no difference. The changes were hard to see, but I’m obviously missing the idea of knowing the difference between the three and how we get them\n\n\n\\[\n  \\text{random component} = x_t - \\hat m_t - \\bar s_t\n\\]\nNext is personal notes for the random component variables.\nx is the monthly mean of the Rexburg temperature highs. m_hat is the monthly centered moving average, and s_hat is the seasonally adjusted mean for each month. X takes the mean of all observations for the given month (t). m_hat uses the previous six and next six observations (months), this is done to better capture seasonality in the months, because some seasons can be colder than the same season the previous year. Doing the CMA will smooth out the trend and make the time series analysis more accurate. Not using the CMA (using x_t) can cause the variance to spike during abnormal seasons and can falsely point to better doing a multiplicative model. s_bar takes the mean of all unique months, so with this data, s_bar returns 12 observations, one for each month. s_bar helps in finding how random the seasons are. So when a season is colder than usual, it will be below the s_bar value for that month, and if that month is also colder than the CMA for the given month, then it will result in a higher random value. Having the CMA and s_bar months subtracted from the actual normal mean (x) paints a better picture if that was a normal month of January by taking into account the season and means of all Januarys. In an additive model, the months have a pattern and that’s why we use a boxplot. So the seasonality is deterministic, so you can take January and February. A multiplicative model has either an increasing or decreasing trend, so plugging in those same two months a few years later will not work out. So, in essence, the variance moves out of the pattern over time for multiplicative, which makes it stochastic, but the variance stays deterministic because the variance stays constant or within the same seasonality over time.",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_1_4.html#rubric",
    "href": "hw/homework_1_4.html#rubric",
    "title": "Time Series Homework: Chapter 1 Lesson 4",
    "section": "Rubric",
    "text": "Rubric\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 2: Visualization\nChooses a reasonable manual range for the Rexburg Daily Temperature series, providing a readable plot that captures the essential data trends. Creates a plot with accurate and clear axis labels, appropriate units, and a caption that enhances the understanding of the Rexburg Daily Temperature series.\nAttempts manual range selection, but with significant issues impacting the readability of the plot. The chosen range may obscure important data trends, demonstrating a limited understanding of graphical representation.Fails to include, axis labels, units, or captions, leaving the visual representation and interpretation incomplete.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3a: Monthly Aggregation\nThe code has been updated with comments and clear explanations of what each command and function does. The student shows they understand the intuition behind the procedure.\nThe code has not been updated or the comments and explanation do not provide enough evidence to prove the student understand the code.\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3b: Centered Moving Average\nCorrectly calculates the centered moving average. Clearly presents the results with well-labeled axes, titles, and a properly formatted plot. | Incorrectly calculates the centered moving average or omits it entirely. The plot is either missing or poorly presented, lacking clear labels, titles, or proper formatting, making it difficult to interpret the results. |\n\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 3c: Seasonally Adjusted Means Series\nCorrectly calculates the seasonally adjusted means series using an appropriate method. Produces a clear, accurate plot of the seasonally adjusted time series with well-labeled axes and titles. | Incorrect calculation or missing/incorrect plot. Plot lacks essential elements like labels, titles, or fails to represent the seasonally adjusted series. |\n\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 3d: Random Component Series\nCorrectly calculates the random component of the series by removing the trend and seasonal components. Produces a clear, accurate plot with well-labeled axes, titles, and proper formatting.\nIncorrectly calculates the random component, omits steps (e.g., does not remove trend/seasonality), or the plot is unclear or missing essential elements like labels and titles.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 4a: Decompose Monthly\nCorrectly applies the additive decomposition model to the Rexburg Monthly Temperature Series. Clearly presents the trend, seasonal, and random components in well-labeled plots, with appropriate titles, axes labels, and formatting.\nFails to correctly apply the additive decomposition model, resulting in incorrect or incomplete separation of the trend, seasonal, and random components. The plot is either missing or poorly presented, lacking proper labels, titles, or clear distinction between components.\n\n\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nQuestion 4b: Decompose Daily\nThe code has been updated with comments and clear explanations of what each command and function does. The student shows they understand the intuition behind the procedure.\nThe code has not been updated or the comments and explanation do not provide enough evidence to prove the student understand the code.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 5a: Modeling Justification\nClearly differentiates between the multiplicative and additive model assumptions, and shows how the series best matches the additive model’s assumptions.\nIt’s not clear that the student understands the difference between the additive and multiplicative model or their assumptions.\n\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nQuestion 5b: Random Component Analysis\nCompares the random components derived from three different procedures, accurately stating whether they are the same or different. Identifies and explains any observed similarities or differences. Proposes a logical, well-reasoned explanation for why similar patterns might exist in the random components, using relevant time series concepts (e.g., common noise factors, model assumptions). | Fails to correctly compare the random components, or provides an unclear or inaccurate comparison.\n| Fails to provide a clear or accurate explanation for the commonalities, or provides irrelevant reasoning. |\n\n\n\n\nTotal Points\n75",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 4"
    ]
  },
  {
    "objectID": "hw/homework_1_2.html",
    "href": "hw/homework_1_2.html",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "",
    "text": "# Macroeconomic Data: unemployment rate\nunemp_rate &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/unemp_rate.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_1_2.html#data",
    "href": "hw/homework_1_2.html#data",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "",
    "text": "# Macroeconomic Data: unemployment rate\nunemp_rate &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/unemp_rate.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_1_2.html#questions",
    "href": "hw/homework_1_2.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the US unemployment series.\nPlease research the time series. The subheadings below has a link to a source to help you get started. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the US unemployment time series.\n\na) US unemployment\nhttps://www.bls.gov/cps/cps_htgm.htm\n\n\n\n\n\n\nAnswer\n\n\n\nData collection process: The US unemployment rate is collected via the current population survey, conducted monthly by the census bureau. It involves interviewing around 60,000 households to gather dta on labor force activities during a specific week.\nUnit of analysis: The unit of analysis is individuals aged 16 and over in the civilian population, excluding those in institutions or the military. Each observation represents whether a person is employed, unemployed, or not in the labor force.\nMeaning of Each Observation: Each data point represents the percentage of the labor force that is unemployed. A person is classified as unemployed if they are not working, have actively sought work in the past four weeks, and are available to start work.\n\n\n\n\n\nQuestion 2 - Estimating the Trend: Annual Aggregation (10 points)\nPlease plot the US Unemployment time series and superimpose the annual mean of the series in the same graph. Use the appropriate axis labels, units, and captions.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# load packages\npacman::p_load(\"tsibble\", \"fabletools\", \"feasts\", \"tidyverse\", \"lubridate\", \"rio\")\n\n# read in the data from a csv\nunemp_rate &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/unemp_rate.csv\")\n\n# create a tsibble where the index variable is the year/month\nunemp_rate_tsibble &lt;- unemp_rate %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  as_tsibble(index = date)\n\n# Aggregating data to annual level by computing mean\nunemp_annual_mean_tsibble &lt;- unemp_rate_tsibble %&gt;%\n  index_by(year = year(date)) %&gt;%\n  summarise(value = mean(value, na.rm = TRUE)) %&gt;%\n  mutate(date = ymd(paste0(year, \"-07-01\"))) %&gt;%\n  as_tsibble(index = date)\n\n# combined plot\nmonthly_plot &lt;- ggplot(unemp_rate_tsibble, aes(x = date, y = value)) +\n  geom_line(color = \"black\") +\n  labs(x = \"Date\", y = \"Unemployment Rate (%)\", title = \"Monthly US Unemployment Rate\") +\n  theme_minimal()\n\n\nfinal_plot &lt;- monthly_plot +\n  geom_line(data = unemp_annual_mean_tsibble, aes(x = date, y = value), color = \"#56B4E9\", size = 1.2) +\n  labs(\n    subtitle = \"Annual mean superimposed in blue\",\n    caption = \"Data Source: Bureau of Labor Statistics\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3 - Trend Analysis (30 points)\n\na) Describe the US Unemployment time series and its trend. Comment on the series’ sampling interval, time series trend characteristics, seasonal variation, and cycle.\n\n\n\n\n\n\nAnswer\n\n\n\nThe data is sampled monthly; each month corresponds to a point on the black line in the plot. This interval provides a view of short-term fluctuations in unemployment. The trend of the unemployment rate has multiple fluctuations but shows peaks and lows corresponding to economic shocks/cycles. Recessions like in the early 1980s and early 2000’s and other economic shocks like the 2020 pandemic appear clear on the plot with noticeable spikes in unemployment. Seasonal variation is not apparent from this plot and or data alone, and further statistical research needs to be done to determine if there are seasonal patterns in unemployment. An example is how certain areas can see a rise in employment during Thanksgiving and Christmas and an increase in unemployment after New Year’s. The cycles in the unemployment rate are clearly evidenced by the rise and fall of the rate over time. Economic cycles can be seen where periods of economic expansion lead to lower unemployment rates, and economic downturns see a rise in unemployment rates.\n\n\n\n\nb) What does the trend represent? What do you suspect is causing the patterns in the trend? Hint: Research the Natural Unemployment Rate\n\n\n\n\n\n\nAnswer\n\n\n\nThe trend in the unmeployment rate time series represents the general direction of unemployment changes over time, reflecting economic cycles and structural shifts in the economy. Factors such as economic recessions, technological advacements, and policy changes contribute to this trend. The natural rate of unemployment suggest that there is a baseline level of unemployemnt below which unemployemnt rate does not typically fall, influence by both frictional and structural unemployemnt. Te,porary deciations from the natural rate of unemployment may occur due to economic shocks or cyclical fluctuations.\n\n\n\n\nc) Please justify whether the trend is deterministic or stochastic.\n\n\n\n\n\n\nAnswer\n\n\n\nThe trend in the unemployment rate time series is likely stochastic because it evolves due to unpredictable economic shocks and policy changes, making its exact path inherently uncertain and not precisely predictable by a fixed mathematical equation.",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_1_2.html#rubric",
    "href": "hw/homework_1_2.html#rubric",
    "title": "Time Series Homework: Chapter 1 Lesson 2",
    "section": "Rubric",
    "text": "Rubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student demonstrates a clear understanding of the context for each data series (US unemployment, term premium, and Google Trends for “chocolate”). The explanation includes details about the data collection process, unit of analysis, and the meaning of each observation.\nThe student fails to provide a clear understanding of the context for one or more data series.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2: Data Visualization\nThe student creates clear and informative plots to visualize the time series and trend. The plot is professional and at a minimum includes the appropriate axis labels, units, and captions\nThe student does not include visualizations or the visualizations provided are unclear and do not effectively represent the data. The plot doesn’t include one or more of the required components.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3a: Data description\nThe student uses appropriate technical language to describe the main features of time series data including sampling interval, time series trend, seasonal variation and cycle.\nThe student does not use technical language to describe the main features of time series data or does not define one or more specified terms.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3b: Causes of US Unemployment trend\nDemonstrates understanding of potential factors influencing the patterns in the trend. Their understanding of potential factors shows understanding of the underlying data, it’s source, and required independent research.\nFails to demonstrate any understanding of potential factors influencing the patterns in the trend. Shows a lack of awareness of the underlying data, its source, and the need for independent research.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3c: Trend Classification\nShows understanding of the relevant concepts. Accurately justifies the trend’s classification. Shows a good understanding of the definitions deterministic and stochastic trends.\nFails to demonstrate any understanding of the relevant concepts or provides no accurate justification for the trend’s classification.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nGeneral: R Programming Usage\nThe student effectively utilizes R to complete the assignment. Code snippets or outputs are appropriately included to support the analysis. Code is annotated and commented enough for a third party to understand and evaluate easily.\nOne or more code snippets or outputs are missing. Code is minimally annotated and commented, making it challenging for a third party to understand and evaluate. The code doesn’t work or render.\n\n\nTotal Points\n60",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 2"
    ]
  },
  {
    "objectID": "definitions.html",
    "href": "definitions.html",
    "title": "Applied Time Series Analysis Outcomes",
    "section": "",
    "text": "Time series analysis1\nTime series2\nSampling Interval3\nSerial Dependence or Autocorrelation4\ntime series trend5\nSeasonal Variation6\nCycle7\nStochastic Trend8\nDeterministic Trend9\nsmoothing or smoothed10\ncentred (center or centered) moving average11\nAdditive decomposition model12\nMultiplicative decomposition model13\ntsibble14\nmonthly additive effect15\nergodic16\ndefinition^\ndefinition^\ndefinition^\ndefinition^\ndefinition^\ndefinition^\ndefinition^"
  },
  {
    "objectID": "definitions.html#footnotes",
    "href": "definitions.html#footnotes",
    "title": "Applied Time Series Analysis Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA time series analysis quantifies the main features in data and the random variation. These reasons, combined with improved computing power, have made time series methods widely applicable in government, industry, and commerce. (1.2)↩︎\nTime series are analysed to understand the past and to predict the future, enabling managers or policy makers to make properly informed decisions.↩︎\nWhen a variable is measured sequentially in time over or at a fixed interval, known as the sampling interval, the resulting data form a time series.↩︎\nA correlation of a variable with itself at different times is known as autocorrelation or serial correlation. (1.2, 2.2.5)↩︎\nIn general, a systematic change in a time series that does not appear to be periodic is known as a trend. The simplest model for a trend is a linear increase or decrease, and this is often an adequate approximation. (1.2 1.4.1)↩︎\nRepeated pattern within each year (or any other fixed time period). (1.2)↩︎\nRepeated pattern that does not correspond to some fixed natural period.↩︎\nRandom trend that does not follow a discernible or predictable pattern. (1.2)↩︎\nCan be modeled with mathematical functions, facilitating the long-term prediction of the behavior↩︎\nThe centred moving average is an example of a smoothing procedure that is applied retrospectively to a time series with the objective of identifying an underlying signal or trend. (1.3 1.5.4)↩︎\nA “centered moving average” is a statistical method used to smooth out short-term fluctuations in time series data by calculating the average of a set of observations, but placing the average value directly in the middle of the data points used, effectively “centering” it on the midpoint of the timeframe, which helps to reduce lag and provide a more accurate representation of the underlying trend compared to a standard moving average. (1.3 1.5.3 1.5.4)↩︎\n\\(x_t = m_t + s_t + z_t\\) or after taking log \\(\\log(x_t) = m_t + s_t + z_t\\). (1.3 1.5.2)↩︎\n\\(x_t = m_t \\cdot s_t + z_t\\)↩︎\nA tsibble (short for “time series tibble”) is sorted by its key first and index. The index (e.g., Date, POSIXct, yearmonth, yearweek) must be sequential and capable of being ordered. Key: (e.g., a sensor ID or region) One or more variables that uniquely identify each time point. Values: One or more measured variables that correspond to observations at each time point.↩︎\nThe centered moving average, , is then used to compute the monthly additive effect↩︎\nA time series model that is stationary in the mean is ergodic in the mean if the time average for a single time series tends to the ensemble mean as the length of the time series increases (2.2 2.2.3).↩︎"
  },
  {
    "objectID": "chapters/chapter_5_lesson_1.html",
    "href": "chapters/chapter_5_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4_overview_code_draft.html",
    "href": "chapters/chapter_4_overview_code_draft.html",
    "title": "Chapter 4 r code examples and practice",
    "section": "",
    "text": "This chapter is not finished.",
    "crumbs": [
      "r code Models draft",
      "Chapter 4 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_4_overview_code_draft.html#visualizing-white-noise",
    "href": "chapters/chapter_4_overview_code_draft.html#visualizing-white-noise",
    "title": "Chapter 4 r code examples and practice",
    "section": "Visualizing White Noise",
    "text": "Visualizing White Noise\nspacer\n\n# 1\n\n\nfile_path &lt;- \"../data/white_noise.parquet\"\n\n# ../data goes back one level. ../../data goes back two folders\n\n\n# straight from lesson code\n# This code was used to create the white noise data file\n\n# Set random seed\nset.seed(10)\n\n# Specify means and standard deviation\nn &lt;- 2500                           # number of points\nwhite_noise_sigma &lt;- rnorm(1, 5, 1) # choose a random standard deviation\n\n# Simulate normal data\n# data.frame(x = rnorm(n, 0, white_noise_sigma)) |&gt;\n#   rio::export(\"../data/white_noise.parquettest\") # uncomment this two lines to run\n\n\n# 2\n\n\n# White noise data\ndf0 &lt;- rio::import(file_path)\n\nname changes\nchunk 3: white_noise_df = df0\nchunk 4: x = v1\n\n# 3\n# words b/ code: The first 250 points in this time....\n\ndf1 &lt;- df0 |&gt; # this code updates names, but now t = x. and x is y in latter codes so keep that in mind.\n  mutate(t = 1:nrow(df0)) |&gt;\n  rename(x = t, v1 = x) # rename this, original code had those names for x and y. now just x and y\n\ndf1 |&gt; # original, but this code does not rename or change df. \n  head(250) |&gt;  \n  ggplot(aes(x = x, y = v1)) + \n    geom_line() +\n    theme_bw() +\n    labs(\n      x = \"Time\",\n      y = \"Values\",\n      title = \"First 250 Values of a Gaussian White Noise Time Series\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n  acf(df1$v1, type = \"covariance\") # doing this in class to get to the density plot below\n\n\n\n\n\n\n\n  acf(df1$v1, type = \"correlation\") # use this acf samples from previous lesson. \n\n\n\n\n\n\n\n\nspacer\n\n# 4.1 - 4\n# words before code: Here is a histogram of the 2500 values from....\n\n# this x is the variable, but not necessarily the x axis variable. x is y here \ndf1 |&gt;\n  mutate(density = dnorm(v1, mean(df1$v1), sd(df1$v1))) |&gt;\n  ggplot(aes(x = v1)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n    geom_line(aes(x = v1, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values or variable 1\",\n      y = \"Frequency\",\n      title = \"Histogram of Values from a Gaussian White Noise Process\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n# left off in this histogram code because the differences between the y and x names is confusing. I need to just hard edit the code to clearly define the variable and avoid the name changes. I just need to make sure I take note of it.\n\nRandom Walk Cumulative Sum\nname changes\ny = v2\nthe x in this code refers to the x - axis values. This code is meant for when the x axis values are number of observations (eg. 1-60). Dates can maybe work, but anything else can cause troubles.\n\n# 4.1 - 5\n# sample code to simulate a random walk\n# words b/ code: Complete steps 2 and 3 a total of\n\n# set.seed(7)\n\ndf2 &lt;- df1 |&gt;\n  # mutate(w = ifelse(row_number() == 1, 0, sample(c(-1,1), size = 60, replace = TRUE))) |&gt; # generates coin flips, but no longer needed for chapter model\n  mutate(v2 = cumsum(v1)) # creates cumulitve v2 column\n\nggplot(data=df2, aes(x=x, y=v2)) +\n  # geom_point(data = df2, aes(x=x, y=v2), size = 0.01) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  # scale_x_continuous(limits = c(0,60), # limits to only 60 obs\n  #                    breaks = seq(0, 60, by = 5),\n  #                    minor_breaks = seq(0, 60, 1)) +\n  # scale_y_continuous(limits = c(-20,20),\n  #                    breaks = seq(-20, 20, by = 5),\n  #                    minor_breaks = seq(-20, 20, 1)) +\n  labs(\n      x = \"Toss Number\",\n      y = expression(paste(\"$x_t$\")),\n      title = \"Cumulative Results of Coin Tosses\" # cum results (v2) of v1. \n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(colour = \"black\")\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nspacer\n\n# 4.1 - 6 \n# words b/ code: be a time series with the following values.\n\nset.seed(6)\nn &lt;- 8\nd_operator &lt;- data.frame(t = c(1:n), x = sample(1:15, n, replace = FALSE)) |&gt;\n  mutate(diff = t - n) # what is this code doing. right now is its doing t - n. n is always the set 8, and t is just the number of observations. so if n is 8, then the first t- n is -7, second is -6 and so on, but this is just using the number/date assigned to the actual variable. its like comparing hot days when only using the data, but not the temperature. so what is this code doing exactly????\n\n#cat( paste( paste0(\"$x_{t\", ifelse(d_operator$t==n,\"\",d_operator$t-n), \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\ncat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) ) \n\n# Computes the value of the \"power_on_d\"^th difference from x_n\nd_value &lt;- function(power_on_d = 0) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(diff == -power_on_d) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n\nts_val &lt;- function(t_value) {\n  out &lt;- d_operator |&gt; #### Note the use of this global variable\n    filter(t == t_value) |&gt;\n    dplyr::select(x) |&gt;\n    pull()\n  \n  return(out)\n}\n\n# this code below was the last r chunk for lesson 4-1, but it is not needed since it is in this r chunk. This r chunk is set not to evaluate for class. \n\n#cat( paste( paste0(\"$x_{\", d_operator$t, \"} = \", d_operator$x, \"$\"), collapse = \",$~$ \" ) )\n\nspacer\nThis is solution to backwards shift operator so formula for this code is done in a previous r chunk.",
    "crumbs": [
      "r code Models draft",
      "Chapter 4 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4.html",
    "href": "chapters/chapter_4_lesson_4.html",
    "title": "Fitted AR Models",
    "section": "",
    "text": "Code\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4.html#lesson-4.4-fitted-ar-models",
    "href": "chapters/chapter_4_lesson_4.html#lesson-4.4-fitted-ar-models",
    "title": "Fitted AR Models",
    "section": "Lesson 4.4 Fitted AR Models",
    "text": "Lesson 4.4 Fitted AR Models\n\nSimulate an AR(1) Time Series\nIn this simulation, we first simulate data from the \\(AR(1)\\) model \\[\n  x_t = 0.75 ~ x_{t-1} + w_t\n\\] where \\(w_t\\) is a white noise process with variance 1.\n\n\nShow the code\nset.seed(123)\nn_rep &lt;- 1000\nalpha1 &lt;- 0.75\n\ndat_ts &lt;- tibble(w = rnorm(n_rep)) |&gt;\n  mutate(\n    index = 1:n(),\n    x = purrr::accumulate2(\n      lag(w), w, \n      \\(acc, nxt, w) alpha1 * acc + w,\n      .init = 0)[-1]) |&gt;\n  tsibble::as_tsibble(index = index)\n\ndat_ts |&gt; \n  autoplot(.vars = x) +\n    labs(\n      x = \"Time\",\n      y = \"Simulated Time Series\",\n      title = \"Simulated Values from an AR(1) Process\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nThe R command mean(dat_ts$x) gives the mean of the \\(x_t\\) values as 0.067.\n\n\nFit an \\(AR(1)\\) Model with Zero Mean\n\n\nShow the code\n# Fit the AR(1) model\nfit_ar &lt;- dat_ts |&gt;\n  model(AR(x ~ order(1)))\ntidy(fit_ar)\n\n\n# A tibble: 1 × 6\n  .model           term  estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 AR(x ~ order(1)) ar1      0.720    0.0220      32.8 2.07e-160\n\n\nThe estimate of the parameter \\(\\alpha_1\\) (i.e. the fitted value of the parameter \\(\\alpha_1\\)) is \\(\\hat \\alpha_1 = 0.72\\).\nWhen R fits an AR model, the mean of the time series is subtracted from the data before the parameter values are estimated. If R detects that the mean of the time series is not significantly different from zero, it is omitted from the output.\nBecause the mean is subtracted from the time series before the parameter values are estimated, R is using the model \\[\n  z_t = \\alpha_1 ~ z_{t-1} + w_t\n\\] where \\(z_t = x_t - \\mu\\) and \\(\\mu\\) is the mean of the time series.\nThings to do\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nAnswer the following questions with your partner.\n\nUse the expression for \\(z_t\\) above to solve for \\(x_t\\) in terms of \\(x_{t-1}\\), \\(\\mu\\), \\(\\alpha_1\\), and \\(w_t\\).\nWhat does your model reduce to when \\(\\mu = 0\\)?\nExplain to your partner why this correctly models a time series with mean \\(\\mu\\).\n\n\n\nWHat do I know\n\nFor what type of Time Series will a fitted AR model be use?\nIs the sample above a stationary or non-stationary time series?\nStochastic vs deterministic time series?\nWhat does this model tell us about the time series?\n\nWe replace the parameter \\(\\mu\\) with its estimator \\(\\hat \\mu = \\bar x\\). We also replace \\(\\alpha_1\\) with the fitted value from the output \\(\\hat \\alpha_1\\). This gives us the fitted model: \\[\n  \\hat x_t = \\bar x + \\hat \\alpha_1 ~ (x_{t-1} - \\bar x)\n\\]\nThe fitted model can be expressed as:\n\\[\\begin{align*}\n  \\hat x_t\n    &= 0.067 + 0.72 \\left( x_{t-1} - 0.067 \\right) \\\\\n    &= 0.067 - 0.72 ~ (0.067) + 0.72 ~ \\left( x_{t-1} \\right) \\\\\n    &= 0.019 + 0.72 ~ x_{t-1}\n\\end{align*}\\]\nEven though R does not report the parameter for the mean of the process, \\(\\hat \\mu = 0.019\\), it is not significantly different from zero. One could argue that we should not use a model that contains the mean and instead focus on a simple fitted model that has only one parameter:\n\\[\n  \\hat x_t = 0.72 ~ x_{t-1}\n\\]\n\n\nConfidence Interval for the Model Parameter\nThe P-value given above tests the hypothesis that \\(\\alpha_1=0\\). This is not helpful in this context. We are interested in the plausible values for \\(\\alpha_1\\), not whether or not it is different from zero. For this reason, we consider a confidence interval and disregard the P-value.\nWe can compute an approximate 95% confidence interval for \\(\\alpha_1\\) as: \\[\n  \\left(\n    \\hat \\alpha_1 - 2 \\cdot SE_{\\hat \\alpha_1}\n    , ~\n    \\hat \\alpha_1 + 2 \\cdot SE_{\\hat \\alpha_1}\n  \\right)\n\\] where \\(\\hat \\alpha_1\\) is our parameter estimate and \\(SE_{\\hat \\alpha_1}\\) is the standard error of the estimate. Both of these values are given in the R output.\n\n\nShow the code\nci_summary &lt;- tidy(fit_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\n\n\nSo, our 95% confidence interval for \\(\\alpha_1\\) is: \\[\n  \\left(\n  0.72 - 2 \\cdot 0.022\n  , ~\n  0.72 + 2 \\cdot 0.022\n  \\right)\n\\] or \\[\n  \\left(\n  0.676\n  , ~\n  0.764\n  \\right)\n\\] Note that the confidence interval contains \\(\\alpha_1 = 0.75\\), the value of the parameter we used in our simulation. The process of estimating the parameter worked well. In practice, we will not know the value of \\(\\alpha_1\\), but the confidence interval gives us a reasonable estimate of the value.\n\n\nResiduals\nFor an \\(AR(1)\\) model where the mean of the time series is not statistically significantly different from 0, the residuals are computed as \\[\\begin{align*}\n  r_t\n    &= x_t - \\hat x_t \\\\\n    &= x_t - \\left[ 0.72 ~ x_{t-1} \\right]\n\\end{align*}\\]\n\n\nCode\n# had include false, and eval false.\n\n\n# Computing the residuals manually\ndat_ts |&gt;\n  # Zero mean model\n  mutate(resid0 = x - ( (tidy(fit_ar) |&gt; select(estimate) |&gt; pull()) * lag(x) ) ) |&gt;\n  # Non-zero mean model\n  mutate(resid1 = x - (mean(x) + (tidy(fit_ar) |&gt; select(estimate) |&gt; pull()) * (lag(x) - mean(x)) ) )\n\n\n# A tsibble: 1,000 x 5 [1]\n         w index      x resid0  resid1\n     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.560      1 -0.560 NA     NA     \n 2 -0.230      2 -0.651 -0.247 -0.266 \n 3  1.56       3  1.07   1.54   1.52  \n 4  0.0705     4  0.874  0.103  0.0842\n 5  0.129      5  0.784  0.156  0.137 \n 6  1.72       6  2.30   1.74   1.72  \n 7  0.461      7  2.19   0.531  0.512 \n 8 -1.27       8  0.376 -1.20  -1.22  \n 9 -0.687      9 -0.405 -0.675 -0.694 \n10 -0.446     10 -0.749 -0.458 -0.477 \n# ℹ 990 more rows\n\n\nWe can easily obtain these residual values in R:\n\n\nShow the code\nfit_ar |&gt; residuals()\n\n\n# A tsibble: 1,000 x 3 [1]\n# Key:       .model [1]\n   .model           index .resid\n   &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n 1 AR(x ~ order(1))     1 NA    \n 2 AR(x ~ order(1))     2 -0.247\n 3 AR(x ~ order(1))     3  1.54 \n 4 AR(x ~ order(1))     4  0.103\n 5 AR(x ~ order(1))     5  0.156\n 6 AR(x ~ order(1))     6  1.74 \n 7 AR(x ~ order(1))     7  0.531\n 8 AR(x ~ order(1))     8 -1.20 \n 9 AR(x ~ order(1))     9 -0.675\n10 AR(x ~ order(1))    10 -0.458\n# ℹ 990 more rows\n\n\nThe variance of the residuals is \\(0.982\\). This is very close to the actual value used in the simulation: \\(\\sigma^2 = 1\\).\n\n\n\n\n\n\nFitting a Simulated AR(1) Part 1\nFitting a Simulated AR(1) Model with Non-Zero Mean\n\nFit an AR(1) Model with Non-Zero Mean\nSame as above? I guess in this one we use R to fit an AR(1) model to the time series data.\n\n\nConfidence Interval for the Model Parameters\n\nWe can compute approximate 95% confidence intervals for \\(\\alpha_0\\) and \\(\\alpha_1\\):\n\nI need to review this\n\nReview the two columns\n\n\n\nResiduals\nLooks like this sections is one full that can be summarize in one for fitting a simulated AR(1) model with non-zero mean???\n\n\n\nRepeat Part 1: Class Activity\n\nThis section is a class activity but we just repeat part 1 (above section)\n\n\n\nFitting a Simulated AR(2) Model\nSeems to be same as part 1 above, definetly need to understand what this code is doing before I tried solving it.\n\nFit an AR(2) Model\n\n\nConfidence Interval for the Model Parameters\n\n\nResiduals\n\nWe can compute the residuals in the same manner as we did for the other models.\n\n\n\n\nActivity: Global Warming\n\nUsing the PACF to Choose p for an AR(p) Process\n\nIn the previous lesson, we noted that the partial correlogram can be used to assess the number of parameters in an AR model. Here is a partial correlogram for the change in the mean annual global temperature.\n\n\n\nFitting Models (Dynamic Number of Parameters)\n\n\nStationary of the AR(p) Model\n\n\n\nForecasting with an AR(p) Model\nClas Activity: Forecasting with an AR(p) Model\n\nComparison to results in Section 4.6.3 of the Book\n\nClass Activity: Comparison to the Results in Section 4.6.3 of the Book (5 min)\n\n\n\n\nsearch for words for lesson 4.4\nconceive - white noise process - variance - estimate for the parameter for the constant -",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_2.html",
    "href": "chapters/chapter_4_lesson_2.html",
    "title": "White Noise and Random Walks - Part 2",
    "section": "",
    "text": "Learning Outcomes\nCharacterize the properties of a random walk - Define the second order properties of a random walk - Define the backward shift operator - Use the backward shift operator to state a random walk as a sequence of white noise realizations - Define a random walk with drift (book pg 77) - So the drift is like the random variable? That fits random variable. volatility is randomness. Except that for this Xt, the drift is part of the total for Xt, while the random component is what the classical decomposition can not account for. - Company stockholders generally expect their investment to increase in value despite the volatility of financial markets. The random walk model can be adapted to allow for this by including a drift parameter &. Closing prices (US dollars) for Hewlett-Packard Company stock for 672 trading days up to June 7, 2007 are read into R and plotted (see the code below and Fig. 4.8). The lag 1 differences are calculated using diff() and plotted in Figure 4.9. The correlogram of the differences is in Figure 4.10, and they appear to be well modelled as white noise. The mean of the differences is 0.0399, and this is our estimate of the drift parameter. The standard deviation of the 671 differences is 0.460, and an approximate 95% confidence interval for the drift parameter is [0.004, 0.075]. Since this interval does not include 0, we have evidence of a positive drift over this period. - How to calculate random drift parameter?\nSimulate realizations from basic time series models in R Simulate a random walk Plot a random walk\nFit time series models to data and interpret fitted parameters Motive the need for differencing in time series analysis Define the difference operator Explain the relationship between the difference operator and the backward shift operator Test whether a series is a random walk using first differences Explain how to estimate a random walk with increasing slope using Holt-Winters Estimate the drift parameter of a random walk\n\n\n\n Back to top",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1.html",
    "href": "chapters/chapter_4_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_3_overview_code_draft.html",
    "href": "chapters/chapter_3_overview_code_draft.html",
    "title": "Chapter 3 r code examples and practice",
    "section": "",
    "text": "This qmd is made to summarize chapter 3 and have models for the chapter to view.\n\nI have a goal to make the code not be reliant on adjusting variable names and titles.\n\nThis will require having an initial r chunk that assigns variable names and dataset.\nI will still have multiple df.\n\nThis will be a bit hard and the models will have to be pretty general because some data sets required different approaches\n\nI think maybe I can also assign if its yearmonth, yearquater etc, but that will require so many if statements.\n\nIdk how I’m going to approach this\n\n\n\n\n# Assign your column names to variables\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n\ndates &lt;- \"date\"\nx_col &lt;- \"constructionequip_ord\"\ny_col &lt;- \"constructionequip_ship\"\n\n\n\n# Assign plot labels\nx_label &lt;- \"Month\"\ny_label &lt;- \"New Orders & Value of Equip\"\nplot_title &lt;- \"Time Series of Construction Equip: New Orders & Equipment\"\n\n\ndf &lt;- df0 |&gt;\n  mutate(\n    date = lubridate::mdy(.data[[dates]]),\n    x = as.numeric(.data[[x_col]]), # Convert and rename to x\n    y = as.numeric(.data[[y_col]])  # Convert and rename to y\n  ) |&gt;\n  select(date, x, y) \n\n\ndf1 &lt;- df |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(date, obs, x, y)\n\ndfx &lt;- df |&gt; # lone df for variable x = ord\n  mutate(obs = row_number()) |&gt; \n  select(date, x)\n\ndfy &lt;- df |&gt; # lone df for y = ship\n  mutate(obs = row_number()) |&gt; \n  select(date, y)\n\ntest\n\n# this is code for hw 3-1\n\n# this code is same as the dfx2 & dfy2 code\ndf1 &lt;- df1 |&gt;\n  mutate(index = tsibble::yearmonth(date)) |&gt; # 3.1\n  as_tsibble(index = index) |&gt;\n  select(index, date, x, y)\n\n\n\nautoplot(df1, .vars = x) +\n  geom_line(data = df1, aes(x = index, y = y), color = \"#E69F00\") +\n  labs(\n    x = x_label,\n    y = y_label,\n    title = plot_title\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "r code Models draft",
      "Chapter 3 r code examples and practice"
    ]
  },
  {
    "objectID": "chapters/chapter_3.html",
    "href": "chapters/chapter_3.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_1.html",
    "href": "chapters/chapter_1.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\ncode for lesson 1.5\nthe following does the multiplicative model\n\n\nCode\nsource(\"../common_functions.R\")\n\n\n\n\nCode\n# Set random seed for reproducibility\nset.seed(123) \n\n# Set parameters & initialize vectors\nnum_years &lt;- 10\nn &lt;- 12 * num_years\nsigma &lt;- .75\na &lt;- 0.03\nb &lt;- 1\nc &lt;- 0.5 \ntrend &lt;- seasonal &lt;- x_t &lt;- rep(0,n)\ntime_seq &lt;- seq(1,n)\n\n# Generate correlated error terms\nw &lt;- rnorm(n + 4, 0.2, 0.1) # Changed to a mean of 1 and sd of 0.03\nz = w + lead(w,1) + lead(w,2) + lead(w,3) + lead(w,4)\nz  = head(z, n)\n\n# Get date\nyear_seq &lt;- lubridate::year(today()) - num_years  + (time_seq - 1) %/% 12\nmonth_seq &lt;- (time_seq - 1) %% 12 + 1\ndate_seq &lt;- ymd(paste0(year_seq,\"-\",month_seq,\"-01\"))\n\n# Get data\nfor (t in 1:n) {\n  trend[t] &lt;- exp(a * t)\n  seasonal[t] &lt;- exp( b * sin(t / 12 * 2 * pi * 1)  + c * cos(t / 12 * 2 * pi * 3) + 1 )\n  x_t[t] &lt;- trend[t] * seasonal[t] * z[t] # Note R's definition of the mult. model\n}\n\nx_df &lt;- data.frame(x_t = x_t, trend = trend, seasonal = seasonal)\n\nstart_year &lt;- lubridate::year(today()) - num_years\nstart_date &lt;- lubridate::ymd(paste0(start_year,\"-01-01\"))\n\n# start_date &lt;- lubridate::ymd(\"1958-01-01\")\ndate_seq &lt;- seq(start_date,\n    start_date + months(nrow(x_df)-1),\n    by = \"1 months\")\n\nx_df_ts &lt;- x_df |&gt;\n  mutate(\n    date = date_seq,\n    month = tsibble::yearmonth(date)\n  ) |&gt;\n  select(date, month, trend, seasonal, x_t) |&gt;\n  as_tsibble(index = month)\n\n\n\n\n\n\n\nDate\nMonth\nTrend, $$m_t$$\nSeasonal, $$s_t$$\nData, $$x_t$$\n\n\n\n\n2014-01-01\n2014 Jan\n1.03\n4.482\n5.065\n\n\n2014-02-01\n2014 Feb\n1.062\n3.92\n5.512\n\n\n2014-03-01\n2014 Mar\n1.094\n7.389\n11.266\n\n\n2014-04-01\n2014 Apr\n1.127\n10.655\n13.348\n\n\n2014-05-01\n2014 May\n1.162\n4.482\n5.391\n\n\n2014-06-01\n2014 Jun\n1.197\n1.649\n1.93\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2023-11-01\n2023 Nov\n35.517\n1.649\n39.853\n\n\n2023-12-01\n2023 Dec\n36.598\n4.482\n121.366\n\n\n\n\n\n\n\n\n\nCode\ntrend_plot &lt;- ggplot(x_df_ts, aes(x=month, y=trend)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Trend\", \n    x=\"Month\", \n    y=\"Trend\"\n    ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nseasonal_plot &lt;- ggplot(x_df_ts, aes(x=month, y=seasonal)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Seasonal Effect\", \n    x=\"Month\", \n    y=\"Seasonal\"\n    ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nerror_plot &lt;- ggplot(x_df_ts, aes(x = month, y = x_t / trend / seasonal)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Random Error Term\", \n    x=\"Month\", \n    y=\"Random\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nx_plot &lt;- ggplot(x_df_ts, aes(x=month, y=x_t)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Simulated Time Series\", \n    x=\"Month\", \n    y=\"x_t\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nx_plot &lt;- x_plot  + labs(title = \"True (Simulated) Values\", x = NULL)\ntrend_plot &lt;- trend_plot + labs(title = NULL, x = NULL)\nseasonal_plot &lt;- seasonal_plot + labs(title = NULL, x = NULL)\nerror_plot &lt;- error_plot + labs(title = NULL)\n\nx_plot / trend_plot / seasonal_plot / error_plot \n\n\n\n\n\n\n\n\n\nRandom Error Term (Error Plot): \\[ e_t = \\frac{x_t}{m_t \\cdot s_t} \\]\nTrend: \\[ m_t = e^{a \\cdot t} \\]\nSeasonal: \\[ s_t = e^{b \\cdot \\sin\\left(\\frac{2 \\pi t}{12}\\right) + c \\cdot \\cos\\left(\\frac{6 \\pi t}{12}\\right) + 1} \\]\nCombined Time Series (Multiplicative Model): \\[ x_t = m_t \\cdot s_t \\cdot z_t \\]\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_2.html",
    "href": "chapters/chapter_2.html",
    "title": "Autocorrelation Concepts",
    "section": "",
    "text": "This is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapters/chapter_3_lesson_1.html",
    "href": "chapters/chapter_3_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapters/chapter_4.html",
    "href": "chapters/chapter_4.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "Code\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2, knitr,\n               kableExtra\n               )\n\n\nThis is the overview of the chapter 4. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\nAdding this from homework_4_0 chapter notes.qmd file in old repo. This are notes intended to connect time series concepts\n\n\nCode\n# Load necessary library\n\n# Define the checklist table\nchecklist_data &lt;- data.frame(\n  Topic = c(\"Trend\", \"Seasonality\", \"Stationarity\", \"White Noise\", \"Serial Correlation\", \"Drift\"),\n  Description = c(\n    \"Presence of an upward or downward movement over time.\",\n    \"Repeating patterns at regular intervals.\",\n    \"Mean, variance, and autocorrelation are constant over time.\",\n    \"Residuals are independent, identically distributed with mean zero.\",\n    \"Past values are correlated with current values.\",\n    \"Consistent positive or negative change over time.\"\n  ),\n  White_Noise = c(\"\", \"\", \"✔\", \"✔\", \"\", \"\"),\n  Random_Walk = c(\"\", \"\", \"\", \"\", \"✔\", \"\"),\n  RW_with_Drift = c(\"\", \"\", \"\", \"\", \"✔\", \"✔\"),\n  AR_p = c(\"✔\", \"\", \"✔\", \"\", \"✔\", \"\"),\n  ARIMA = c(\"\", \"\", \"✔\", \"\", \"✔\", \"\"),\n  Holt_Winters = c(\"✔\", \"✔\", \"✔\", \"\", \"✔\", \"\")\n)\n\n# Render the table\nkable(checklist_data, format = \"html\", escape = F) %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nTopic\nDescription\nWhite_Noise\nRandom_Walk\nRW_with_Drift\nAR_p\nARIMA\nHolt_Winters\n\n\n\n\nTrend\nPresence of an upward or downward movement over time.\n\n\n\n✔\n\n✔\n\n\nSeasonality\nRepeating patterns at regular intervals.\n\n\n\n\n\n✔\n\n\nStationarity\nMean, variance, and autocorrelation are constant over time.\n✔\n\n\n✔\n✔\n✔\n\n\nWhite Noise\nResiduals are independent, identically distributed with mean zero.\n✔\n\n\n\n\n\n\n\nSerial Correlation\nPast values are correlated with current values.\n\n✔\n✔\n✔\n✔\n✔\n\n\nDrift\nConsistent positive or negative change over time.\n\n\n✔\n\n\n\n\n\n\n\n\n\n\n\nTime Series Model Checklist\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nDescription\nWhite Noise\nRandom Walk\nRW with Drift\nAR(p)\nARIMA\nHolt-Winters\n\n\n\n\nTrend\nPresence of an upward or downward movement over time.\n\n\n\n✔\n\n✔\n\n\nSeasonality\nRepeating patterns at regular intervals.\n\n\n\n\n\n✔\n\n\nStationarity\nMean, variance, and autocorrelation are constant over time.\n✔\n\n\n✔\n✔\n✔\n\n\nWhite Noise\nResiduals are independent, identically distributed with mean zero.\n✔\n\n\n\n\n\n\n\nSerial Correlation\nPast values are correlated with current values.\n\n✔\n✔\n✔\n✔\n✔\n\n\nDrift\nConsistent positive or negative change over time.\n\n\n✔\n\n\n\n\n\n\nspacer\n\n\nCode\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nCode\n# second r chunk\npacf(temps_ts$change)\n\n\n\n\n\n\n\n\n\nCode\n# 3rd r chunk\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n\n# A tibble: 7 × 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\nCode\n# 4th r chunk\nalphas &lt;- global_ar |&gt; coefficients() |&gt; tail(-1) |&gt; dplyr::select(estimate) |&gt; pull()\ncat(\n  \"0 = 1\", \n        \"- (\", alphas[1], \") * x\",\n        \"- (\", alphas[2], \") * x^2\",\n        \"- (\", alphas[3], \") * x^3\",\n        \"\\n     \",\n        \"- (\", alphas[4], \") * x^4\",\n        \"- (\", alphas[5], \") * x^5\",\n        \"- (\", alphas[6], \") * x^6\"\n)\n\n\n0 = 1 - ( 0.6559292 ) * x - ( -0.06617426 ) * x^2 - ( 0.140204 ) * x^3 \n      - ( 0.2653744 ) * x^4 - ( -0.1627911 ) * x^5 - ( 0.2056924 ) * x^6\n\n\nCode\nalphas\n\n\n[1]  0.65592918 -0.06617426  0.14020403  0.26537444 -0.16279106  0.20569242\n\n\nCode\n# 5th r chunk\n\ntemps_forecast &lt;- global_ar |&gt; forecast(h = \"50 years\")\ntemps_forecast |&gt;\n  autoplot(temps_ts, level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(global_ar)) +\n  scale_color_discrete(name = \"\") +\n  labs(\n    x = \"Year\",\n    y = \"Temperature Change (Celsius)\",\n    title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\"),\n    subtitle = paste0(\"50-Year Forecast Based on our AR(\", tidy(global_ar) |&gt; as_tibble() |&gt; dplyr::select(term) |&gt; tail(1) |&gt; stringr::str_sub(1), \") Model\")\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Import and prepare the data\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\n# Fit an AR model to the 'change' variable\nar_fit &lt;- ar(temps_ts$change, method = \"mle\", na.action = na.omit)\n\n# Extract and display the order and coefficients\norder &lt;- ar_fit$order\ncoefficients &lt;- ar_fit$ar\n\ncat(\"Order of the fitted AR model: \", order, \"\\n\")\n\n\nOrder of the fitted AR model:  4 \n\n\nCode\ncat(\"Coefficients of the AR model: \", coefficients, \"\\n\")\n\n\nCoefficients of the AR model:  0.6770191 -0.03582926 0.1560533 0.1929323 \n\n\nCode\n# Visualization of the original data\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1_code_notes.html",
    "href": "chapters/chapter_4_lesson_1_code_notes.html",
    "title": "Ch 4.1 Code Notes",
    "section": "",
    "text": "set.seed(1)\nn_points &lt;- 50  # Number of points to use in both parts\nsigma &lt;- 2 # Standard deviation for rnorm\naver &lt;- 0\n\n# Generate the tibble using n_points and sigma\nwd_gaussian &lt;- tibble(\n    index = 1:n_points,\n    y = rnorm(n_points, mean = aver, sd = sigma)\n) |&gt; \n  as_tsibble(index = index) |&gt;\n  mutate(\n  density = dnorm(y, mean = aver, sd = sigma))\n\n\n# Plot the first tibble\nwd_gaussian |&gt; \n    ggplot(aes(x = index, y = y)) + \n    geom_line() +\n    theme_bw() +\n    ggtitle(\"Generated White Noise Series\")\n\n\n\n\n\n\n\n# 2. Calculate and print the mean and variance\nsample_mean &lt;- mean(wd_gaussian$y)\nsample_variance &lt;- var(wd_gaussian$y)\ncat(\"Estimated Mean:\", sample_mean, \"\\n\")\n\nEstimated Mean: 0.2008966 \n\ncat(\"Estimated Variance:\", sample_variance, \"\\n\")\n\nEstimated Variance: 2.764863 \n\n# 3. Plot the Autocorrelation Function (ACF)\nacf_plot &lt;- wd_gaussian |&gt;\n  ACF(y, type = \"correlation\") |&gt;\n  autoplot() +\n  ggtitle(\"Autocorrelation Function of White Noise\") +\n  theme_bw()\n\n\n# Plot the histogram using the data from wd_gaussian\nhist_plot &lt;- wd_gaussian |&gt;\n    ggplot(aes(x = y)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"darkgrey\", bins = 10) +\n    geom_line(aes(x = y, y = density)) +\n    theme_bw() +\n  ggtitle(\"Histogram with Theoretical Normal Density Curve\")\n\n\nacf_plot\n\n\n\n\n\n\n\nhist_plot",
    "crumbs": [
      "Lesson 1",
      "Ch 4.1 Code Notes"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_1_code_notes.html#homework-4-question-3.a",
    "href": "chapters/chapter_4_lesson_1_code_notes.html#homework-4-question-3.a",
    "title": "Ch 4.1 Code Notes",
    "section": "homework 4 question 3.a",
    "text": "homework 4 question 3.a\nthis is sample code given by ai that uses an rexp function which we dont use but it does appear once in the book for one of the practice problems. I decided not to use it and show more of the math being done on the code\n\n#| include: false\nset.seed(42)\n\n# Parameters\nlambda &lt;- 1\nn &lt;- 500\n\n# Simulate white noise using the exponential distribution\ns_t &lt;- rexp(n, rate = lambda)\nw_t &lt;- s_t - lambda\n\n# Plot the simulation\nlibrary(ggplot2)\n\ndf &lt;- data.frame(Time = 1:n, White_Noise = w_t)\nggplot(df, aes(x = Time, y = White_Noise)) +\n  geom_line(color = \"blue\") +\n  theme_minimal() +\n  labs(\n    title = \"Simulated White Noise Process Using Exponential Distribution\",\n    x = \"Time\",\n    y = expression(w[t])\n  )",
    "crumbs": [
      "Lesson 1",
      "Ch 4.1 Code Notes"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_3.html",
    "href": "chapters/chapter_4_lesson_3.html",
    "title": "Autoregressive (AR) Models",
    "section": "",
    "text": "Autoregressive (AR) Models Lesson 4.3\n\nAn AR is a linear regression model that uses lagged values of the time series to predict future values.\nAn AR is a stochastic process that uses a linear combination of past values of the time series to predict future values.\n\n\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\n\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\n\n\n\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do - DO group activity: Simulation of an AR(1) process\n\n\n\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\n\nHere is a partial autocorrelation plot for the McDonald’s stock price data:\n\n\nCode\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ncompany &lt;- \"McDonald's\"\n\n# Retrieve static file\nstock_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\n\nThe only significant partial correlation is at lag \\(k=1\\). This suggests that an \\(AR(1)\\) process could be used to model the McDonald’s stock prices.\n\n\n\nLook at lesson shinny code\n\n\n\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\nWe can use the polyroot function to find the roots of polynomials in R. For example, to find the roots of the polynomial \\(x^2-x-6\\), we apply the command\n\n\nCode\npolyroot(c(-6,-1,1))\n\n\n[1]  3+0i -2+0i\n\n\nNote the order of the coefficients. They are given in increasing order of the power of \\(x\\).\nOf course, we could simply factor the polynomial: \\[\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n\\] which implies that \\[\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n\\]\n\n\n\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\n\n\nWhat is an exponential smoothing model?\n\n\n\n\nexponential smoothing model - polyroot function -",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_3.html#lesson-4.3-autoregressive-ar-models",
    "href": "chapters/chapter_4_lesson_3.html#lesson-4.3-autoregressive-ar-models",
    "title": "Autoregressive (AR) Models",
    "section": "",
    "text": "Autoregressive (AR) Models Lesson 4.3\n\nAn AR is a linear regression model that uses lagged values of the time series to predict future values.\nAn AR is a stochastic process that uses a linear combination of past values of the time series to predict future values.\n\n\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nAutoregressive Properties of an AR model\n\nThe mean of an AR model is a constant.\nThe variance of an AR model is finite.\nThe covariance of an AR model is a function of the lag.\nThe autocorrelation of an AR model is a function of the lag.\n\n\n\n\nDefinitino Recall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\n\n\n\n\nThe autocorrelation function of an \\(AR(1)\\) model is a function of the lag.\n\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\nThings to do - DO group activity: Simulation of an AR(1) process\n\n\n\n\n\n\n\n\n\nDefinition: Partial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? answer: 0\n\n\n\n\n\n\nHere is a partial autocorrelation plot for the McDonald’s stock price data:\n\n\nCode\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ncompany &lt;- \"McDonald's\"\n\n# Retrieve static file\nstock_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\n\nThe only significant partial correlation is at lag \\(k=1\\). This suggests that an \\(AR(1)\\) process could be used to model the McDonald’s stock prices.\n\n\n\nLook at lesson shinny code\n\n\n\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\nWe can use the polyroot function to find the roots of polynomials in R. For example, to find the roots of the polynomial \\(x^2-x-6\\), we apply the command\n\n\nCode\npolyroot(c(-6,-1,1))\n\n\n[1]  3+0i -2+0i\n\n\nNote the order of the coefficients. They are given in increasing order of the power of \\(x\\).\nOf course, we could simply factor the polynomial: \\[\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n\\] which implies that \\[\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n\\]\n\n\n\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nThis sections check for this - We will now practice assessing whether an AR process is stationary using the characteristic equation.\nco-pilot notes\n\nStationary and Non-Stationary AR Processes Lesson 4.3\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\nThe characteristic equation of an AR process is the polynomial \\(\\theta_p(\\mathbf{B}) = 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p\\).\nThe roots of the characteristic polynomial are the solutions of the characteristic equation.\nThe absolute value of the roots of the characteristic polynomial must be greater than 1 for the AR process to be stationary.\n\n\nco-pilot notes end\n\n\n\n\nWhat is an exponential smoothing model?\n\n\n\n\nexponential smoothing model - polyroot function -",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapters/chapter_4_lesson_4_code_notes.html",
    "href": "chapters/chapter_4_lesson_4_code_notes.html",
    "title": "Ch 4.4 Code Notes",
    "section": "",
    "text": "transfer from chapter_4_lesson_4_notes.qmd old repo\nI belive this are notes taken from class.\n\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n# chunk number 2\npacf(temps_ts$change)\n\n\n\n\n\n\n\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n# A tibble: 7 × 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\nTo go over\ngo over the table fromt he Fitting Models (Dynamic Number of Parameters) exercise.\nwrite the ar model for this time series. * What does the table tell us. * estimeate * estimate * statistic * p.value\nKnow how to indentify if a model is stationary or not stationary. Moncayo’s r 6 model is not stationary because the of the .98, but the book model, I think the ar 4 model shows it is stationary. ([1] 1.011 1.755 1.453 1.453). Moncayo’s ar6 shows that it is not stationary. so the plot in the lesson, shows an increasing forecasted trend. Since the model says it is stationary, and the temps has been in an increasing trend since the 1980, the book model plot hasthe forecast coming back down to a mean of zero. This makes sense for a stiationary model because it has a mean of zero.\n\n\n\n Back to top",
    "crumbs": [
      "Lesson 4",
      "Ch 4.4 Code Notes"
    ]
  },
  {
    "objectID": "chapters/chapter_5.html",
    "href": "chapters/chapter_5.html",
    "title": "Chapter overview and Task",
    "section": "",
    "text": "This is the overview of the chapter 5. I will add info from the index file here. I will filter from there to what will go here. My goal is to first have the website have a map of the concepts in time series. I want to first avoid doing it by chapters since it seperates topics and limit my learning to that lesson. Bigger picture first.\n\n\n\n Back to top",
    "crumbs": [
      "Overview",
      "Chapter overview and Task"
    ]
  },
  {
    "objectID": "chapters/chapter_5_lesson_1_code_notes.html",
    "href": "chapters/chapter_5_lesson_1_code_notes.html",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "",
    "text": "transfer from chapter_5_lesson_1_notes.qmd old repo\nnotes taken during class\n\n# Loading R packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra\n               )\n\n\nchocolate_month2 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    stats_time = year + (month - 1) / 12,\n    month_seq = 1:n()\n  ) |&gt;\n  mutate(month = factor(month)) |&gt;\n  as_tsibble(index = dates)\n\n# Fit regression model\nchocolate_lm &lt;- chocolate_month2 |&gt;\n  model(TSLM(chocolate ~ 0 + stats_time + month))\n\n# Estimated parameter values\nparam_est &lt;- chocolate_lm |&gt;\n  tidy() |&gt;\n  pull(estimate)\n\nparam_est\n\n [1]     1.131642 -2227.910370 -2219.304673 -2233.148977 -2233.443280\n [6] -2234.837584 -2237.931887 -2236.476190 -2237.370494 -2237.514797\n[11] -2232.659101 -2223.003404 -2200.097708\n\n\nwhen removing 0 + from the tslm function. it makes january as the mt\n\nchocolate_month2 |&gt; # &lt;- #1 rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    stats_time = year + (month - 1) / 12,\n    month_seq = 1:n()\n  ) |&gt; #2\n  mutate(month = factor(month)) |&gt; # factor converts numerical month into categorical levels. eg january = 1. so this parts gets us b1 jan + b2 feb ... + b12 dec. \n  as_tsibble(index = dates)\n\n# A tsibble: 240 x 7 [1M]\n   Month   chocolate    dates month  year stats_time month_seq\n   &lt;chr&gt;       &lt;int&gt;    &lt;mth&gt; &lt;fct&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;int&gt;\n 1 2004-01        36 2004 Jan 1      2004      2004          1\n 2 2004-02        45 2004 Feb 2      2004      2004.         2\n 3 2004-03        29 2004 Mar 3      2004      2004.         3\n 4 2004-04        32 2004 Apr 4      2004      2004.         4\n 5 2004-05        29 2004 May 5      2004      2004.         5\n 6 2004-06        26 2004 Jun 6      2004      2004.         6\n 7 2004-07        27 2004 Jul 7      2004      2004.         7\n 8 2004-08        27 2004 Aug 8      2004      2005.         8\n 9 2004-09        29 2004 Sep 9      2004      2005.         9\n10 2004-10        33 2004 Oct 10     2004      2005.        10\n# ℹ 230 more rows\n\n# Fit regression model\nchocolate_lm &lt;- chocolate_month2 |&gt;\n  model(TSLM(chocolate ~ stats_time + month))\n\n# Estimated parameter values\nparam_est &lt;- chocolate_lm |&gt;\n  tidy() |&gt;\n  pull(estimate)\n\nparam_est\n\n [1] -2227.910370     1.131642     8.605697    -5.238607    -5.532910\n [6]    -6.927214   -10.021517    -8.565821    -9.460124    -9.604428\n[11]    -4.748731     4.906965    27.812662\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Lesson 1 Notes",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "hw/homework_1_1.html",
    "href": "hw/homework_1_1.html",
    "title": "Time Series Homework: Chapter 1 Lesson 1",
    "section": "",
    "text": "What is something unusual or unique about you?\n\n\n\n\n\n\nAnswer\n\n\n\nI have been coming to BYUI for almost four years, and therefore, I have been focusing on school for nearly four years, so I can’t think of anything unique or unusual about me besides school, but I have some ideas. I have about 14 siblings, but I’m the only child between my mom and dad. Back in 2007, I was able to solve about 90% of a Rubik’s cube without looking, which was impressive back then but not so much now. My ability to figure out and solve things like the Rubik’s cube remains so I can solve many complex things that the average person will not think to even try. One unusual thing is that although I’m really smart in some scenarios like the Rubik’s cube, I can also be really dumb in some basic university study habits, so yeah. \n\n\n\n\n\nWhat do you hope to gain from this class?\n\n\n\n\n\n\nAnswer\n\n\n\nI’m looking to improve my ability to explain business and financial numbers using statistics. I hope this class gives me the skills and vocabulary to properly and correctly explain the statistical meanings behind business, financial, and/or economic data. I also hope that this course can help me use knowledge and skills related to my data science minor and apply them to my major. I’m also excited to expand my econometrics knowledge with this course. \n\n\n\n\n\n\n\n\n\nTip for Exercise 3\n\n\n\nIn a qmd file, you can do all sorts of fun things. If you type &lt;ctrl&gt; &lt;alt&gt; i, RStudio will insert an R code chunk, where you can run any R code. The code below generates 10 uniform random variables between 0 and 1.\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\nreps &lt;- 4\n\n# Create a data frame with a counter variable, t, \n# and the simulated values, called x\ndf1 &lt;- data.frame(t = 1:reps, x = runif(reps, min = 0, max = 1))\ndf1\n\n  t          x\n1 1 0.30776611\n2 2 0.25767250\n3 3 0.55232243\n4 4 0.05638315\n\n\nYou can use inline R code as well. For example, the mean of the n=4 simulated x values is: 0.293536.\nIf you do not know how to use any command such as rnorm in R, you can type the name of the command after a question mark (e.g. ?rnorm) in the console area, and the help file will appear on the right.\n\n\n\n\n\nUse the rnorm command to simulate 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Please do not list all the numbers. Instead, give a histogram of the data.*\n\n\n\n\n\n\nAnswer\n\n\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\n# Simulate 100,000 normally-distributed random variables with mean=50 and sd=10\n\nsimulated_data &lt;- rnorm(100000, mean = 50, sd = 10)\n\n\n# histogram\n\nhist(simulated_data, \n\n     main = \"Histogram of Simulated Normally-Distributed Data\", \n\n     xlab = \"Value\", \n\n     ylab = \"Frequency\", \n\n     col = \"skyblue\", \n\n     border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose your pulse was measured at the start of every hour today and the values are given in the data frame pulse_df.\n\nset.seed(123)\npulse_df &lt;- data.frame(\n  times = paste0(as.Date(substr(now(),1,10)), \" \", c(0:23), \":00\"),\n  value = sample(70:100, size = 24, replace = TRUE)\n)\n\nWe can convert a character representation of a date to a date-time object. The Lubridate package contains commands such as mdy(\"12/31/2024\") which converts this value to: 2024-12-31. There are other variations of this command such as dmy_hms(\"31/12/2024 15:16:47\") which gives us: 2024-12-31 15:16:47.\nUse the command ymd_hm() to convert the times variable into a date-type variable. Then filter the four observations from noon to 3 PM (15:00). There are many ways to accomplish this but here is one simple example:\n\npulse_df$times &lt;- ymd_hm(pulse_df$times) # convert times to date-type data\npulse_df_filtered &lt;- pulse_df %&gt;% filter(hour(times) &gt;= 12 & hour(times) &lt;= 15) # using the hour() function from lubridate we look at just the hour section of the 'times' variable and then filter to values between 12 and 15\n\nThen, compute the mean of the observed value for these four times. Write your code in the R chunk below. Write an English sentence giving this mean and its interpretation in terms of your pulse rate.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# put your code  \n# To avoid code clutter, and since I was just going to use the giving samples, I only included the code to compute the mean. So when a qmd file is rendered, the first two chunks of r code fit with this third r chunk and are not just extra chunks of code. \n\n# computing mean\nmean_pulse &lt;- mean(pulse_df_filtered$value)\n\n# display\nmean_pulse\n\n[1] 94\n\npulse_df_filtered\n\n                times value\n1 2024-12-12 12:00:00    91\n2 2024-12-12 13:00:00    94\n3 2024-12-12 14:00:00    95\n4 2024-12-12 15:00:00    96\n\n\nThe mean pulse rate observed from noon to 3 PM is 94 beats per minute. This value represents the average pulse rate during these four specific hours.\n\n\n\n\n\nDo you have any concerns about your ability to succeed in this class? If so, please share them.\n\n\n\n\n\n\nAnswer\n\n\n\nYes, my main concern is getting lost in the math portion of the statistics. I fear I can get stuck in some parts, and will cause me to lose much time figuring out statistical math that should be done quickly. I also have a concern of understanding all the math symbols, I just don’t want to spend an insane amount of time figuring out what symbols mean which it happen in ECON 381.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (5)\nIncomplete (0)\n\n\n\nOffers a solid and well-elaborated description of unique or unusual aspects, demonstrating a good level of self-awareness and the ability to express distinctive qualities effectively.\nGives a limited or unclear description of unique or unusual aspects, lacking essential details. Demonstrates a minimal level of self-awareness.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates clear and specific goals for the class, demonstrating a good level of understanding of what the individual aims to gain.\nStates goals with limited clarity or specificity, lacking detail and depth. The understanding of what is hoped to be gained is minimal.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nUtilizes the rnorm command accurately, generating 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Demonstrates a precise understanding of the command and its parameters.\nFails to use the rnorm command to generate normally-distributed random variables, leaving the question unanswered or without the required simulation.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nSuccessfully applies the ymd_hm() command to convert the times variable into a date-type variable. Accurately filters the four observations from noon to 3 PM (15:00). Accurately computes the mean of the observed values for the specified times and provides a clear English sentence interpreting the mean in terms of pulse rate. The interpretation is insightful.\nFails to provide any code for applying the ymd_hm() command and filtering, leaving the R chunk incomplete or without the required transformation. Attempts to compute the mean and provide an interpretation but with significant inaccuracies or lack of clarity. The interpretation may be incorrect or incomplete.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates concerns with clarity and provides a good level of detail in explaining potential challenges to success in the class. The response is clear and relevant.\nStates concerns with limited clarity or detail, lacking in-depth explanations. Demonstrates a minimal understanding of potential challenges to success.\n\n\nTotal Points\n35",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_1_1.html#questions",
    "href": "hw/homework_1_1.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 1",
    "section": "",
    "text": "What is something unusual or unique about you?\n\n\n\n\n\n\nAnswer\n\n\n\nI have been coming to BYUI for almost four years, and therefore, I have been focusing on school for nearly four years, so I can’t think of anything unique or unusual about me besides school, but I have some ideas. I have about 14 siblings, but I’m the only child between my mom and dad. Back in 2007, I was able to solve about 90% of a Rubik’s cube without looking, which was impressive back then but not so much now. My ability to figure out and solve things like the Rubik’s cube remains so I can solve many complex things that the average person will not think to even try. One unusual thing is that although I’m really smart in some scenarios like the Rubik’s cube, I can also be really dumb in some basic university study habits, so yeah. \n\n\n\n\n\nWhat do you hope to gain from this class?\n\n\n\n\n\n\nAnswer\n\n\n\nI’m looking to improve my ability to explain business and financial numbers using statistics. I hope this class gives me the skills and vocabulary to properly and correctly explain the statistical meanings behind business, financial, and/or economic data. I also hope that this course can help me use knowledge and skills related to my data science minor and apply them to my major. I’m also excited to expand my econometrics knowledge with this course. \n\n\n\n\n\n\n\n\n\nTip for Exercise 3\n\n\n\nIn a qmd file, you can do all sorts of fun things. If you type &lt;ctrl&gt; &lt;alt&gt; i, RStudio will insert an R code chunk, where you can run any R code. The code below generates 10 uniform random variables between 0 and 1.\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\nreps &lt;- 4\n\n# Create a data frame with a counter variable, t, \n# and the simulated values, called x\ndf1 &lt;- data.frame(t = 1:reps, x = runif(reps, min = 0, max = 1))\ndf1\n\n  t          x\n1 1 0.30776611\n2 2 0.25767250\n3 3 0.55232243\n4 4 0.05638315\n\n\nYou can use inline R code as well. For example, the mean of the n=4 simulated x values is: 0.293536.\nIf you do not know how to use any command such as rnorm in R, you can type the name of the command after a question mark (e.g. ?rnorm) in the console area, and the help file will appear on the right.\n\n\n\n\n\nUse the rnorm command to simulate 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Please do not list all the numbers. Instead, give a histogram of the data.*\n\n\n\n\n\n\nAnswer\n\n\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Set the random number seed\nset.seed(100)\n\n# Define the number of values to simulate\n# Simulate 100,000 normally-distributed random variables with mean=50 and sd=10\n\nsimulated_data &lt;- rnorm(100000, mean = 50, sd = 10)\n\n\n# histogram\n\nhist(simulated_data, \n\n     main = \"Histogram of Simulated Normally-Distributed Data\", \n\n     xlab = \"Value\", \n\n     ylab = \"Frequency\", \n\n     col = \"skyblue\", \n\n     border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose your pulse was measured at the start of every hour today and the values are given in the data frame pulse_df.\n\nset.seed(123)\npulse_df &lt;- data.frame(\n  times = paste0(as.Date(substr(now(),1,10)), \" \", c(0:23), \":00\"),\n  value = sample(70:100, size = 24, replace = TRUE)\n)\n\nWe can convert a character representation of a date to a date-time object. The Lubridate package contains commands such as mdy(\"12/31/2024\") which converts this value to: 2024-12-31. There are other variations of this command such as dmy_hms(\"31/12/2024 15:16:47\") which gives us: 2024-12-31 15:16:47.\nUse the command ymd_hm() to convert the times variable into a date-type variable. Then filter the four observations from noon to 3 PM (15:00). There are many ways to accomplish this but here is one simple example:\n\npulse_df$times &lt;- ymd_hm(pulse_df$times) # convert times to date-type data\npulse_df_filtered &lt;- pulse_df %&gt;% filter(hour(times) &gt;= 12 & hour(times) &lt;= 15) # using the hour() function from lubridate we look at just the hour section of the 'times' variable and then filter to values between 12 and 15\n\nThen, compute the mean of the observed value for these four times. Write your code in the R chunk below. Write an English sentence giving this mean and its interpretation in terms of your pulse rate.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# put your code  \n# To avoid code clutter, and since I was just going to use the giving samples, I only included the code to compute the mean. So when a qmd file is rendered, the first two chunks of r code fit with this third r chunk and are not just extra chunks of code. \n\n# computing mean\nmean_pulse &lt;- mean(pulse_df_filtered$value)\n\n# display\nmean_pulse\n\n[1] 94\n\npulse_df_filtered\n\n                times value\n1 2024-12-12 12:00:00    91\n2 2024-12-12 13:00:00    94\n3 2024-12-12 14:00:00    95\n4 2024-12-12 15:00:00    96\n\n\nThe mean pulse rate observed from noon to 3 PM is 94 beats per minute. This value represents the average pulse rate during these four specific hours.\n\n\n\n\n\nDo you have any concerns about your ability to succeed in this class? If so, please share them.\n\n\n\n\n\n\nAnswer\n\n\n\nYes, my main concern is getting lost in the math portion of the statistics. I fear I can get stuck in some parts, and will cause me to lose much time figuring out statistical math that should be done quickly. I also have a concern of understanding all the math symbols, I just don’t want to spend an insane amount of time figuring out what symbols mean which it happen in ECON 381.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nMastery (5)\nIncomplete (0)\n\n\n\nOffers a solid and well-elaborated description of unique or unusual aspects, demonstrating a good level of self-awareness and the ability to express distinctive qualities effectively.\nGives a limited or unclear description of unique or unusual aspects, lacking essential details. Demonstrates a minimal level of self-awareness.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates clear and specific goals for the class, demonstrating a good level of understanding of what the individual aims to gain.\nStates goals with limited clarity or specificity, lacking detail and depth. The understanding of what is hoped to be gained is minimal.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nUtilizes the rnorm command accurately, generating 100,000 normally-distributed random variables with a mean of 50 and a standard deviation of 10. Demonstrates a precise understanding of the command and its parameters.\nFails to use the rnorm command to generate normally-distributed random variables, leaving the question unanswered or without the required simulation.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\n\nSuccessfully applies the ymd_hm() command to convert the times variable into a date-type variable. Accurately filters the four observations from noon to 3 PM (15:00). Accurately computes the mean of the observed values for the specified times and provides a clear English sentence interpreting the mean in terms of pulse rate. The interpretation is insightful.\nFails to provide any code for applying the ymd_hm() command and filtering, leaving the R chunk incomplete or without the required transformation. Attempts to compute the mean and provide an interpretation but with significant inaccuracies or lack of clarity. The interpretation may be incorrect or incomplete.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\n\nStates concerns with clarity and provides a good level of detail in explaining potential challenges to success in the class. The response is clear and relevant.\nStates concerns with limited clarity or detail, lacking in-depth explanations. Demonstrates a minimal understanding of potential challenges to success.\n\n\nTotal Points\n35",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_1_3.html",
    "href": "hw/homework_1_3.html",
    "title": "Time Series Homework: Chapter 1 Lesson 3",
    "section": "",
    "text": "# Macroeconomic Data: unemployment rate\nunemp_rate &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/unemp_rate.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_1_3.html#data",
    "href": "hw/homework_1_3.html#data",
    "title": "Time Series Homework: Chapter 1 Lesson 3",
    "section": "",
    "text": "# Macroeconomic Data: unemployment rate\nunemp_rate &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/unemp_rate.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_1_3.html#questions",
    "href": "hw/homework_1_3.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Estimating the Trend: Centered Moving Average (10 points)\nPlease plot the US Unemployment time series and superimpose the centered moving average series \\(\\hat{m}_{t}\\) in the same graph. Don’t use an R command; rather, do it by coding \\(\\hat{m}_{t}\\) like in Chapter 1: Lesson 3. Use the appropriate axis labels, units, and captions.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Load packages\npacman::p_load(\"tsibble\", \"fable\", \"feasts\", \"tidyverse\", \"lubridate\", \"rio\")\n\n\n# Create a tsibble where the index variable is the year/month\nunemp_rate_tsibble &lt;- unemp_rate %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  as_tsibble(index = date)\n\n# Calculate the Centred Moving Average (CMA)\nunemp_rate_tsibble &lt;- unemp_rate_tsibble %&gt;%\n  mutate(\n    # Creating a new column 'm_hat', representing the centered moving average (CMA)\n    m_hat = (\n      # Math: (1/2) * x_{t-6}\n      # Explanation: Fetches the value 6 time periods back and applies half-weight to it (beginning of the window)\n      (1/2) * lag(value, 6) + \n      \n      # Math: x_{t-5}\n      # Explanation: Fetches the value 5 time periods back with full weight\n      lag(value, 5) + \n      \n      # Math: x_{t-4}\n      lag(value, 4) + \n      \n      # Math: x_{t-3}\n      lag(value, 3) + \n      \n      # Math: x_{t-2}\n      lag(value, 2) + \n      \n      # Math: x_{t-1}\n      # Explanation: Fetches the value 1 time period back with full weight\n      lag(value, 1) + \n      \n      # Math: x_{t}\n      # Explanation: Includes the value at the current time period\n      value + \n      \n      # Math: x_{t+1}\n      # Explanation: Fetches the value 1 time period forward with full weight\n      lead(value, 1) + \n      \n      # Math: x_{t+2}\n      lead(value, 2) + \n      \n      # Math: x_{t+3}\n      lead(value, 3) + \n      \n      # Math: x_{t+4}\n      lead(value, 4) + \n      \n      # Math: x_{t+5}\n      # Explanation: Fetches the value 5 time periods forward with full weight\n      lead(value, 5) + \n      \n      # Math: (1/2) * x_{t+6}\n      # Explanation: Fetches the value 6 time periods forward and applies half-weight to it (end of the window)\n      (1/2) * lead(value, 6)\n      \n    ) / 12  # Math: 1/12\n            # Explanation: Divides the sum by 12 to compute the average over 12 time periods\n  )\n\n\n# Plot the CMA (centred moving average) and original data\nplain_plot &lt;- autoplot(unemp_rate_tsibble, .vars = m_hat) +\n  labs(\n    x = \"Date\",\n    y = \"Unemployment Rate (%)\",\n    title = \"Centred Moving Average (CMA) of Unemployment Rate\"\n  ) +\n  scale_y_continuous(limits = c(min(unemp_rate_tsibble$m_hat, na.rm = TRUE), max(unemp_rate_tsibble$m_hat, na.rm = TRUE))) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Plot the original unemployment data with CMA overlay\nfancy_plot &lt;- autoplot(unemp_rate_tsibble, .vars = value) +\n  labs(\n    x = \"Date\",\n    y = \"Unemployment Rate (%)\",\n    title = \"Monthly US Unemployment Rate with CMA\"\n  ) +\n  geom_line(aes(x = date, y = m_hat), color = \"#D55E00\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine the two plots\nplain_plot\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nfancy_plot\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2 - Seasonal Averages: Side-by-Side Box Plots by Month (10 points)\nPlease create a box plot to illustrate the monthly averages in the US Unemployment time series. Use the appropriate axis labels, units, and captions.\n\n\n\n\n\n\nAnswer\n\n\n\n\n# Please provide your code here\n# Box plot of monthly averages in the US Unemployment time series\n\n# Prepare Data: Converts the date column to month names and categorizes them as factors.\nunemp_rate_tsibble &lt;- unemp_rate_tsibble %&gt;%\n  mutate(month = factor(format(date, \"%m\"), labels = month.name))\n\n# Create Boxplot: Initializes a boxplot with months on the x-axis and unemployment rates on the y-axis, excluding individual data points.\n# Adjust Layout: Adds a title and labels to the plot for clarity and presentation.\np &lt;- plot_ly(unemp_rate_tsibble, x = ~month, y = ~value, type = 'box', boxpoints = FALSE) %&gt;%\n  layout(\n    title = \"Boxplots of US Unemployment Rate by Month\",\n    xaxis = list(title = \"Month\"),\n    yaxis = list(title = \"Unemployment Rate (%)\")\n  )\n\np\n\n\n\n\n\n\n\n\n\nQuestion 3 - Seasonal Averages: Analysis (20 points)\n\na) Describe the seasonality of the US unemployment time series. Comment on the series’ periods of highest and lowest unemployment. Are there any notable outliers?\n\n\n\n\n\n\nAnswer\n\n\n\nJanuary, February, and June have the highest unemployment in general. January and February have high unemployment because it’s after the holiday rush, so many employers let go of their seasonal help. I can see many retailers doing most of the layoffs, but also, many local economies slow down during winter. The spike in unemployment in June can be link to end of school year, so many high school and college graduates are looking for employment, which can lead to a rise in unemployment.\nOctober and August had the lowest minimum unemployment rate of 2.4%. October had the lowest Q1 unemployment rate of 3.85%. January had the highest median unemployment rate of 6.25%. February had the highest Q3 unemployment rate of 7.5%. April had the highest maximum unemployment rate of all months, with an unemployment rate of 14.4%.\nThe highest outliers occurred during the years 1982-1983, 2009-2011, and 2020(14.4%). These years were marked by economic downturns, so it is obvious why they were the highest.\n\n\n\n\nb) Please explain the patterns you found. Include information from your prior research on the series.\n\n\n\n\n\n\nAnswer\n\n\n\nThis research was great in introducing us to the CMA and its importance. Below is a screenshot with the annual mean superimposed on the left and the Monthly US unemployment Rate with CMA on the right. The line in the plot between 2000 and 2020 is for the year 2010. We can see that the annual mean hits its high right after 2010, but on the CMA orange line, that high happened during December 2009. When looking at average data over time using the normal mean, we would have assumed 2010 had high unemployment, which it did, but the peak happened at the end of 2009. We can also see how the spike in 2020 is lower on the annual mean. Although these patterns are from visuals, by doing further research into the numbers, we will find obvious differences in the data. This teaches me that simply doing averages can cause misunderstandings in data.\n\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (20)\nIncomplete (0)\n\n\nQuestion 1: Centered Moving Average\nThe student correctly employs the centering procedure to seasonally adjust the US unemployment series.\nThe student does not employ the centering procedure to seasonally adjust the US unemployment series.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2: Box Plot\nCreates a clear and well-constructed box plot that effectively illustrates seasonality in the US Unemployment time series. Labels both the x and y-axes, including appropriate units. Ensures clarity and accuracy in axis labeling, contributing to the overall understanding of the box plot.\nThere are mistakes in the plot. Fails to include any axis labels or units, resulting in an incomplete representation that lacks essential context.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 5a: Description\nProvides an accurate description of the seasonality in the US Unemployment time series, capturing key patterns and trends effectively. Shows a good understanding of the recurring cycles.\nAttempts to identify peaks and troughs but with significant inaccuracies or lack of clarity in the commentary. Shows a limited understanding of the variations. Fails to identify or comment on notable outliers, providing no insight into unusual data points in the US Unemployment time series.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 5b: Patterns\nShows understanding of the data to infer meaning in the seasonal averages. It’s clear the student did background research on the unemployment time series\nShows a lack of effort. It’s not clear the student understands the meaning of the data.\n\n\nTotal Points\n50",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_1_5.html",
    "href": "hw/homework_1_5.html",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "",
    "text": "vessels &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_1_5.html#data",
    "href": "hw/homework_1_5.html#data",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "",
    "text": "vessels &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_1_5.html#questions",
    "href": "hw/homework_1_5.html#questions",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1: Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\na) Vessels Cleared in Foreign Trade for United States\nhttps://fred.stlouisfed.org/series/M03022USM583NNBR\n\n\n\n\n\n\nAnswer\n\n\n\nThe Vessels data gives the monthly number of vessels (in thousands) cleared in Foreign Trade for the United States. The plot below plots every month from January 1901 to September 1940. Base on the plot we can observe the number of vessels cleared each month was increasing over time. We can see that the number of vessels cleared monthly hit a decline in growth during certain years, but the time series is in general increasing over time.\n\n\nShow the code\nvessels_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") |&gt; # ts1\n  mutate(\n    dates = dmy(date),\n    year = lubridate::year(date),\n    months = lubridate::month(date),\n    value = vessels\n  ) |&gt; # ts2\n  dplyr::select(dates, year, months, value)  |&gt; # ts3\n  arrange(dates) |&gt; # ts4\n  mutate(index = tsibble::yearmonth(dates)) |&gt; # ts5\n  as_tsibble(index = index) |&gt; # ts6\n  dplyr::select(index, dates, year, months, value) |&gt; # ts7\n  rename(Vessels = value) # rename value to emphasize data context\nvessels_ts |&gt; #ts8\n  autoplot(.vars = Vessels) +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Seasonally Adjusted Series - Calculation and Plot (10 points)\n\na) Decompose the Vessels Cleared in Foreign Trade series using the multiplicative decomposition model. Show the first 10 rows of results from the decomposition like shown in the Time Series Notebook.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose &lt;- vessels_ts |&gt; # Ts\n  model(feasts::classical_decomposition(Vessels,\n          type = \"mult\"))  |&gt; # make sure to update x value\n  components()\n\n# something wrong with the display_table. has to do w/ common libraries\n# vessels_decompose |&gt;\n#   head(10) |&gt;\n#   display_table()\n\n# so this code takes the prepare data and does the classical_decomposition\n\n\n\n\n\n\nb) Illustrate the original, trend, and the seasonally adjusted series in the same plot. Use the appropriate axis labels, units, and captions.\nColor code: Original = black, Trend = blue, and seasonally adjusted series = orange.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose |&gt;\n  ggplot() +\n  geom_line(data = vessels_decompose, aes(x = index, y = Vessels), color = \"black\") +\n  geom_line(data = vessels_decompose, aes(x = index, y = season_adjust), color = \"#D55E00\") +\n  geom_line(data = vessels_decompose, aes(x = index, y = trend), color = \"#0072B2\") +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates(in thousands)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3: Seasonally Adjusted Series - Analysis (30 points)\n\na) Plot the random component of the multiplicative decomposition from Q2a.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose |&gt;\n  ggplot() +\n  geom_line(data = vessels_decompose, aes(x = index, y = random), color = \"black\")  +\n  labs(\n    x = \"Month\",\n    y = \"Random Component\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for US(in thousands)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Use the additive decomposition method to decompose the Vessels Cleared in Foreign Trade series. Plot the random component.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nShow the code\nvessels_decompose_additive &lt;- vessels_ts |&gt; # Ts\n  model(feasts::classical_decomposition(Vessels,\n          type = \"add\"))  |&gt; # make sure to update x value\n  components()\n\nvessels_decompose_additive |&gt;\n  ggplot() +\n  geom_line(data = vessels_decompose_additive, aes(x = index, y = random), color = \"black\")  +\n  labs(\n    x = \"Month\",\n    y = \"Random Component\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for US(in thousands)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nShow the code\n# The random component in the multiplicative decomposition model supports insights giving by the seasonal and trend assumptions. We can research maritime events that could have impacted certain certain years of the vessel data, and reasoning that research using the multiplicative model will make the most sense. \n\n# The random component is taken by subtracting the trend and seasonally adjusted mean from any giving month in the index. By doing this, the random component takes into account seasons where vessel numbers could have been trending twice as much as they were the year before. Other events in the data that happen every year would be captured by the seasonally adjusted mean. For example we can argue that before the Christmas season, the number of vessels cleared would be higher then December and the first few months of the year. By doing this the random component tells us how random any giving month in the index is compared to the rest of the data. We can see in the plot of the multiplicative random component that the randomness is not much, and only stays between 0.8 and 1.2. \n\n# The additive random component plot shows the random component raging anywhere from -780 to 1000. We have months as low as 1659 vessels cleared with the highest month being 8475 vessels cleared. This is like trying to predict the number of vessels for next January will be 2000 (+-800). +-800 is a big difference. In contrast using the multiplicative model, the prediction can be 2000 (+-1). \n\n\n\n\n\n\nc) Please describe the differences between the random component series and use it as part of your justification for why we use the multiplicative decomposition model instead of the additive model to seasonally adjust the series.\n\n\n\n\n\n\nAnswer\n\n\n\nThe multiplicative decomposition model’s random component supports insights given by the seasonal and trend assumptions. We can research maritime events that could have impacted certain years of the vessel data, and reasoning that research using the multiplicative model will make the most sense.\n\nThe random component is taken by subtracting the trend and seasonally adjusted mean from any given month in the index. By doing this, the random component considers seasons where vessel numbers could have been trending twice as much as they were the year before. The seasonally adjusted mean would capture other events in the data that happen every year. For example, we can argue that before the Christmas season, the number of vessels cleared would be higher than in December and the first few months of the year. Subtracting the trend (m_hat) and seasonally adjusted mean from any month in the index will result in the random component being a great metric to determined how random any given month in the index is compared to the rest of the data. We can see in the plot of the multiplicative random component that the randomness is not much and only stays between 0.8 and 1.2.\n\nThe additive random component plot shows the random component ranging anywhere from -780 to 1000. We have months as low as 1659 vessels cleared with the highest month being 8475 vessels cleared. This is like trying to predict the number of vessels for next January will be 2000 (+-800). +-800 is a big difference. In contrast, using the multiplicative model, the prediction can be 2000 (+-1).",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_1_5.html#rubric",
    "href": "hw/homework_1_5.html#rubric",
    "title": "Time Series Homework: Chapter 1 Lesson 5",
    "section": "Rubric",
    "text": "Rubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student provides a clear and detailed explanation of the data collection process, unit of analysis, and meaning of each observation for the series\nThe student provides a basic explanation of the data context, but some details are missing or unclear.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2a: Multiplicative Decomposition\nCorrectly applies the multiplicative decomposition model. Displays the first ten rows of decomposition results (trend, seasonal, and random components) in a clear, organized format with appropriate labeling.\nIncorrectly applies the decomposition model, produces inaccurate results, or fails to present the first ten rows correctly, or the presentation needs to be clearer and more organized.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2b: Plot\nAccurately illustrates all three series in a single, clear plot where each series is distinguishable. Clearly labels the axes with appropriate units and includes informative captions. All elements are well-presented and properly formatted.\nAttempts to create a plot, but with significant inaccuracies or lack of clarity. The plot may not effectively communicate the results due to labeling or presentation issues.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3a: Multiplicative Random Component Plot\nPlots the random component of the multiplicative decomposition with clear axis labels, appropriate units, and proper formatting for readability.\nLacks proper labeling (e.g., missing axis labels, incorrect units), or presents a poorly formatted and unclear plot.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3b: Additive Random Component Plot\nCorrectly applies the additive decomposition method to decompose the series and accurately plots the random component with clear axis labels, units, and proper formatting.\nFails to correctly apply the decomposition method, inaccurately plots the random component, or provides a plot that lacks proper labeling, units, or formatting.\n\n\n\nMastery (20)\nIncomplete (0)\n\n\nQuestion 3c: Random Component Analysis\nClearly describes the differences between the random components derived from the multiplicative and additive decomposition models, and provides a logical and well-reasoned justification for using the multiplicative model based on these differences.\nThe description of the differences between the random components or provides an unclear, unsupported, or incorrect justification for using the multiplicative model.\n\n\nTotal Points\n50",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 1 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_2_2.html",
    "href": "hw/homework_2_2.html",
    "title": "Time Series Homework: Chapter 2 Lesson 2",
    "section": "",
    "text": "Code\nmanu4 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/manu_mat_invent.csv\")\n\nearlier &lt;- rio::import(\"https://raw.githubusercontent.com/1Ramirez7/uni_data/refs/heads/main/data/mearlier.csv\")\n\nlatter &lt;- rio::import(\"https://raw.githubusercontent.com/1Ramirez7/uni_data/refs/heads/main/data/mlatter.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_2_2.html#data",
    "href": "hw/homework_2_2.html#data",
    "title": "Time Series Homework: Chapter 2 Lesson 2",
    "section": "",
    "text": "Code\nmanu4 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/manu_mat_invent.csv\")\n\nearlier &lt;- rio::import(\"https://raw.githubusercontent.com/1Ramirez7/uni_data/refs/heads/main/data/mearlier.csv\")\n\nlatter &lt;- rio::import(\"https://raw.githubusercontent.com/1Ramirez7/uni_data/refs/heads/main/data/mlatter.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_2_2.html#questions",
    "href": "hw/homework_2_2.html#questions",
    "title": "Time Series Homework: Chapter 2 Lesson 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\nManufacturers’ Materials and Supplies Inventories\nhttps://fred.stlouisfed.org/series/UMTMMI\n\n\n\n\n\n\nAnswer\n\n\n\n\nData Collection Process: The data is collected monthly by the U.S. Census Bureau and represents manufacturers’ inventories for materials and supplies across the manufacturing sector in the U.S.\nUnit of Analysis: The values are measured in millions of dollars, indicating the monetary worth of materials and supplies inventories at the end of each month.\nMeaning of Each Observation: Each observation represents the total dollar value of materials and supplies inventories held by U.S. manufacturers at the end of the specified month, without adjustments for seasonal variations.\n\n\n\nCode\nmanu4$date &lt;- lubridate::mdy(manu4$date) # manu_1\nmanu4$date &lt;- yearmonth(manu4$date) # manu_2\nmanu4$manu_inv &lt;- as.numeric(manu4$manu_inv) # manu_3 \n\nmissing_values &lt;- anyNA(manu4$manu_inv) | anyNA(manu4$date) # this portion only checks for missing and the above line of code is what changes the df\nif (anyNA(manu4$manu_inv) | anyNA(manu4$date)) {\n  cat(\"Missing values in 'manu_inv' or 'date'.\\n\")\n} else {\n  cat(\"No missing values.\\n\")\n} # manu_3 only checks for NA\n\n\nNo missing values.\n\n\nCode\nmanu4_tsbl &lt;- as_tsibble(manu4, key = NULL, index = date, regular = TRUE) # manu_4 \ninterval(manu4_tsbl) # checks for interval\n\n\n&lt;interval[1]&gt;\n[1] 1M\n\n\nCode\nhas_gaps(manu4_tsbl) # checks for gaps\n\n\n# A tibble: 1 × 1\n  .gaps\n  &lt;lgl&gt;\n1 FALSE\n\n\nCode\n# manu4_tsbl &lt;- manu4_tsbl %&gt;% fill_gaps() # to fill gaps # manu_4\n\nmanu4_tsbl |&gt;\n    autoplot()\n\n\n\n\n\n\n\n\n\nCode\n#manu4_tsbl |&gt;\n#    slice(1:12) |&gt; # this code just plots the first 12 rows.\n#    autoplot()\n\n\n\n\n\n\n\nQuestion 2 - Manufacturer’s Inventory: Autocorrelation and autocovariance (10 points)\n\na) Please calculate the list of autocorrelation and autocovariance values for the Manufacturer’s Inventory series.\n\n\n\n\n\n\nAnswer\n\n\n\nAutocorrelation Values for the Manufacture’s Inventory\n\n\nCode\n# autocorrelation values for the manufacture's inventory\nmanu4_tsbl |&gt; ACF(manu_inv)\n\n\n# A tsibble: 25 x 2 [1M]\n        lag   acf\n   &lt;cf_lag&gt; &lt;dbl&gt;\n 1       1M 0.988\n 2       2M 0.976\n 3       3M 0.963\n 4       4M 0.949\n 5       5M 0.934\n 6       6M 0.920\n 7       7M 0.904\n 8       8M 0.889\n 9       9M 0.873\n10      10M 0.856\n# ℹ 15 more rows\n\n\nCode\nACF(manu4_tsbl) |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\nAutocovariance Values for the Manufacture’s Inventory Series\n\n\nCode\nmanu4_tsbl |&gt; ACF(manu_inv, type = \"covariance\")\n\n\n# A tsibble: 25 x 2 [1M]\n        lag         acf\n   &lt;cf_lag&gt;       &lt;dbl&gt;\n 1       1M 2364790710.\n 2       2M 2334722786.\n 3       3M 2303877261.\n 4       4M 2270652440.\n 5       5M 2235855386.\n 6       6M 2201015019.\n 7       7M 2164212123.\n 8       8M 2126778125.\n 9       9M 2089074989.\n10      10M 2049563630.\n# ℹ 15 more rows\n\n\nCode\nACF(manu4_tsbl, type = \"covariance\") |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) If autocovariance and autocorrelation are trying to evaluate a similar linear relationship across time in our series, why do we get different values for autocorrelation and autocovariance at the same lag.\n\n\n\n\n\n\nAnswer\n\n\n\nAutocovariance and autocorrelation both measure the linear relationship between a time series and its lagged values, but they differ in scale. Autocovariance reflects the raw magnitude of dependence, while autocorrelation is a standardized version that divides by the variance, yielding values between -1 and 1 for easy comparison. This is why autocovariance values are large (in original units) and autocorrelation values are always between -1 and 1, even at the same lag.\n\n\n\n\n\nQuestion 3 - Manufacturer’s Inventory: Stationary (20 points)\nWeak stationarity is a form of stationarity important for the analysis of time series data. A time series is said to be weakly stationary if its statistical properties such as mean, variance, and autocovariance are constant over time. Here are the key components of weak stationarity:\nConstant Mean: The mean of the time series remains constant over time. This doesn’t necessarily mean that the time series is centered around zero; it just implies that the average value remains the same throughout the observed period.\nConstant Variance: The variance of the time series is uniform across all time points. Like the mean, this doesn’t imply that the variance must be zero, just that it doesn’t change systematically with time.\nConstant Autocovariance: The autocovariance between any two observations of the time series depends only on the time lag between them and not on the absolute positions of the observations in time. This implies that the dependence structure of the time series remains constant over time.\n\na) Please split the time series into two halves according to the date recorded, the earlier half of the data and the latter part of the data. Calculate the mean, variance, and autocovariance for each half. Note: (it doesn’t really matter if it’s precisely half. An approximate middle is sufficient.)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\nearlier$date &lt;- lubridate::mdy(earlier$date) # manu_1\nearlier$date &lt;- yearmonth(earlier$date) # manu_2\nearlier$manu_inv &lt;- as.numeric(earlier$manu_inv) # manu_3 \nearlier_tsbl &lt;- as_tsibble(earlier, key = NULL, index = date, regular = TRUE) # manu_4 \n\nmean_earlier &lt;- mean(earlier_tsbl$manu_inv, na.rm = TRUE)\nvar_earlier &lt;- var(earlier_tsbl$manu_inv, na.rm = TRUE)\n\ncat(\"Earlier Autocovariance table & Plot\")\n\n\nEarlier Autocovariance table & Plot\n\n\nCode\nearlier_tsbl |&gt; ACF(manu_inv, type = \"covariance\")\n\n\n# A tsibble: 22 x 2 [1M]\n        lag        acf\n   &lt;cf_lag&gt;      &lt;dbl&gt;\n 1       1M 271864918.\n 2       2M 263934953.\n 3       3M 255685221.\n 4       4M 246527856.\n 5       5M 237030968.\n 6       6M 227618413.\n 7       7M 217212817.\n 8       8M 207420675.\n 9       9M 198025275.\n10      10M 187730547.\n# ℹ 12 more rows\n\n\nCode\nACF(earlier_tsbl, type = \"covariance\") |&gt; autoplot()\n\n\n\n\n\n\n\n\n\nspacer\n\n\nCode\nlatter$date &lt;- lubridate::mdy(latter$date) # manu_1\nlatter$date &lt;- yearmonth(latter$date) # manu_2\nlatter$manu_inv &lt;- as.numeric(latter$manu_inv) # manu_3 \nlatter_tsbl &lt;- as_tsibble(latter, key = NULL, index = date, regular = TRUE) # manu_4 \n\nmean_latter &lt;- mean(latter_tsbl$manu_inv, na.rm = TRUE)\nvar_latter &lt;- var(latter_tsbl$manu_inv, na.rm = TRUE)\n\ncat(\"Latter Autocovariance table & Plot\")\n\n\nLatter Autocovariance table & Plot\n\n\nCode\nlatter_tsbl |&gt; ACF(manu_inv, type = \"covariance\")\n\n\n# A tsibble: 22 x 2 [1M]\n        lag         acf\n   &lt;cf_lag&gt;       &lt;dbl&gt;\n 1       1M 1568354967.\n 2       2M 1540251678.\n 3       3M 1511153596.\n 4       4M 1478539410.\n 5       5M 1443884911.\n 6       6M 1409207644.\n 7       7M 1371666130.\n 8       8M 1333456667.\n 9       9M 1294884311.\n10      10M 1253651023.\n# ℹ 12 more rows\n\n\nCode\nACF(latter_tsbl, type = \"covariance\") |&gt; autoplot()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Earlier Mean:\", mean_earlier, \"\\n\")\n\n\nEarlier Mean: 149865 \n\n\nCode\ncat(\"Latter mean:\", mean_latter, \"\\n\")\n\n\nLatter mean: 226136.7 \n\n\nCode\ncat(\"Earlier Variance:\", var_earlier, \"\\n\")\n\n\nEarlier Variance: 280707884 \n\n\nCode\ncat(\"Latter Variance:\", var_latter, \"\\n\")\n\n\nLatter Variance: 1603110465 \n\n\n\n\n\n\nb) Is there evidence to suggest that the Manufacturer’s Inventory series is weakly stationary?\n\n\n\n\n\n\nAnswer\n\n\n\nweakly stationary? what is weakly??\nIn order for a data to be stationary, any two periods in a data-set would have the same mean and variance. In the case for the Manu data set. the mean and variance for both the earlier & latter data sets are different so to that definition this data set is not stationary but maybe it is weakly stationary?\n\nhere is chatgpt’s summarizing my results and thoughts for this question.\nThe Manufacturer’s Inventory series does not appear to be weakly stationary since the mean, variance, and autocovariance differ substantially between the earlier and latter halves of the data. The significant increase in mean and variance over time indicates that the statistical properties of the series are not constant.\n\n\n\n\nc) The variance function for a times series, \\(\\sigma^2(t)=E[(x_t-\\mu)^2]\\), is defined for the entire ensemble. Why is determining whether a time series has constant variance so difficult using sample data?\n\n\n\n\n\n\nAnswer\n\n\n\nWell, first seasonality and randomness can effect the variance at different points, and doing the variance for just a sample data can missed shocks in the dataset.\n\nhere is chatgpt’s summary of my results and thoughts.\nDetermining whether a time series has constant variance is difficult using sample data because the variance function, defined for the entire ensemble, must ideally consider all possible realizations of the process. In practice, we only have a finite sample, which makes it challenging to accurately assess whether variance is stable, especially in the presence of trends or other systematic changes over time.\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3a: Autocorrelation and Covariance\nThe student correctly computes the autocorrelation and autocovariance values for the Manufacturer’s Inventory series using R.The R code is well-commented and structured, facilitating understanding of each step in the calculation process. Results are presented clearly.\nThe student attempts to compute autocorrelation and autocovariance values for the Manufacturer’s Inventory series, but significant errors are present in the computations. The R code lacks clear documentation, with unclear or missing comments that hinder comprehension of the calculation process. Presentation of results may be confusing or incomplete, making it challenging to interpret the autocorrelation and autocovariance values accurately.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3b:Theoretical understanding\nThe student provides a clear and accurate explanation of why different values are obtained for the same lag of the autocorrelation and autocovariance estimates. The explanation demonstrates a solid understanding of the underlying concepts.\nThe student attempts to explain why different values are obtained for the same lag of the autocorrelation and autocovariance estimates but does so with significant inaccuracies or lack of clarity. The explanation may lacks coherence or fails to address key differences between autocorrelation and autocovariance adequately.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 4a: Stationarity Calculations\nThe student accurately splits the dataset into two parts and calculates the mean, variance, and autocovariance for each part using R. The R code is well-commented, providing clear explanations of the steps taken to perform the analysis. The calculated statistics are presented clearly, aiding interpretation of the results, and the student shows a solid understanding of the concepts involved in analyzing time series data.\nThe student attempts to split the dataset into two parts and calculate the mean, variance, and autocovariance for each part using R, but does so with significant errors or inaccuracies. The R code lacks clear and sufficient commenting, making it difficult to understand the steps taken in the analysis. The calculated statistics may be presented poorly or inaccurately, indicating a limited understanding of the concepts involved in analyzing time series data.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 4b: Evaluation\nThe student assesses whether there is evidence to suggest that the Manufacturer’s Inventory series is weakly stationary. The analysis is supported by clear and concise explanations, demonstrating a solid understanding of the concept of weak stationarity.\nThe student attempts to assess whether the Manufacturer’s Inventory series is weakly stationary but does so with significant errors or lacks clarity in their analysis. There may be inaccuracies in the methodology or misinterpretation of results, indicating a limited understanding of weak stationarity\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 4c: Evaluation\nThe students understand the definition and application of a time series variance function to an ensemble.\nThe submission doesn’t provide enough evidence of understanding of the definition and application of the variance function.\n\n\nTotal Points\n40",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 2 Lesson 2"
    ]
  },
  {
    "objectID": "hw/homework_3_1.html",
    "href": "hw/homework_3_1.html",
    "title": "Time Series Homework: Chapter 3 Lesson 1",
    "section": "",
    "text": "Loading and preparing data\nCode\ndf &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n# str(df$date)\n\n# ------------- section 1 -----------------------------------\ndf &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date),\n         constructionequip_ord = as.numeric(constructionequip_ord), # make sure numeric for x variables\n         constructionequip_ship = as.numeric(constructionequip_ship)\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) |&gt; # renames columns and converts to numeric\n  select(date, x, y) # re orders and or removes not selected columns\n\n# -----------end of section 1 -----------------------\n\n# -------------------- section 2 --------------------------\n\n# df1 &lt;- df |&gt; # this makes a new df so either df before or this one is use. \n#   mutate(obs = row_number()) |&gt; # makes new column with periods\n#   select(date, obs, x, y)\n# \n# dfx &lt;- df |&gt; # lone df for variable x = ord\n#   mutate(obs = row_number()) |&gt; \n#   select(obs, x)\n# \n# dfy &lt;- df |&gt; # lone df for y = ship\n#   mutate(obs = row_number()) |&gt; \n#   select(obs, y)\n\n# ----------------- end of section 2 --------------------------\nCode\ndf1 &lt;- df |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(date, obs, x, y)\n\ndfx &lt;- df1 |&gt; # lone df for variable x = ord\n  mutate(obs = row_number()) |&gt; \n  select(date, x)\n\ndfy &lt;- df1 |&gt; # lone df for y = ship\n  mutate(obs = row_number()) |&gt; \n  select(date, y)",
    "crumbs": [
      "Lesson 1",
      "Time Series Homework: Chapter 3 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_3_1.html#question-1---context-and-measurement-10-points",
    "href": "hw/homework_3_1.html#question-1---context-and-measurement-10-points",
    "title": "Time Series Homework: Chapter 3 Lesson 1",
    "section": "Question 1 - Context and Measurement (10 points)",
    "text": "Question 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\n\na) Construction Equipment Manufacturing Orders and Shipments\nhttps://fred.stlouisfed.org/graph/?g=1f4dN\n\n\n\n\n\n\nAnswer\n\n\n\nBoth x (ord) and y (ship) variables are increasing over time. So a multiplicative classical decomposition model is best. The data is collected monthly and the unit of analysis is in thousands of dollars. The data is collected by the U.S. Census Bureau and is used to measure the value of new orders and shipments for construction equipment manufacturing. The observations represent the total value of new orders and shipments in thousands of dollars for each month.\n\n\nCode\n# this is code for hw 3-1\n\n# this code is same as the dfx2 & dfy2 code\ndf1 &lt;- df1 |&gt;\n  mutate(index = tsibble::yearmonth(date)) |&gt; # 3.1\n  as_tsibble(index = index) |&gt;\n  select(index, date, x, y)\n\n\n\nautoplot(df1, .vars = x) +\n  geom_line(data = df1, aes(x = index, y = y), color = \"#E69F00\") +\n  labs(\n    x = \"Month\",\n    y = \"New orders & Value of Equip\",\n    title = \"Time Series of Construction Equip: New Orders & Equipment\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\nFigure: CCF plot of random component of New Orders and Equipment.\n\n\nCode\ndf11 &lt;- model(df1, feasts::classical_decomposition(x)) |&gt;\n  components() |&gt;\n  select(index, random) |&gt;\n  rename(x_random = random)\n\ndf111 &lt;- model(df1, feasts::classical_decomposition(y)) |&gt;\n    components() |&gt;\n  select(index, random) |&gt;\n  rename(y_random = random)\n\nrandom_joint &lt;- df11 |&gt;\n  right_join(df111, by = join_by(index))\n\nautoplot(random_joint, .vars = x_random) +\n  geom_line(data = random_joint, aes(x = index, y = y_random), color = \"#E69F00\") +\n  labs(\n    x = \"Month\",\n    y = \"Random Component (Thousands)\",\n    title = \"Random Component\",\n    subtitle = \"Construction Equip: New Orders & Value of Shipmentfor Construction Equipment\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )",
    "crumbs": [
      "Lesson 1",
      "Time Series Homework: Chapter 3 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_3_1.html#question-2---construction-equipment-manufacturing-graphical-analysis-15-points",
    "href": "hw/homework_3_1.html#question-2---construction-equipment-manufacturing-graphical-analysis-15-points",
    "title": "Time Series Homework: Chapter 3 Lesson 1",
    "section": "Question 2 - Construction Equipment Manufacturing: Graphical Analysis (15 points)",
    "text": "Question 2 - Construction Equipment Manufacturing: Graphical Analysis (15 points)\n\na) Please plot a correlogram of the random components of the New Orders and Value of Shipments for the Construction Equipment Manufacturing data. Include both plots in the same illustration as exemplified in the Figure 5 of Chapter 3 Lesson 1.\n\n\nCode\nx_random &lt;- ACF(random_joint, y = x_random) |&gt; autoplot() +\n    labs(title = \"ACF of Random Component of New Orders Over Time\")\ny_random &lt;- ACF(random_joint, y = y_random) |&gt; autoplot() +\n    labs(title = \"ACF of Random Component of Equipment Over Time\")\nx_random / y_random\n\n\n\n\n\n\n\n\n\n\n\nb) Interpret and analyze the correlogram of the New Orders and Value of Shipments series.\n\n\n\n\n\n\nAnswer\n\n\n\nObserving the correlogram patterns over time reveals a prominent cyclical or seasonal patterns. Both New Orders and Shipments random components show a decaying pattern, this could suggest autoregressive characteristics or lingering seasonal influences not fully removed by decomposition, but new orders looks to have more peaks and downs. New orders shows more persistent peaks or valleys than the other, indicating differing sensitivities of New Orders and Shipments to economic or seasonal factors over time. Overall, this analysis helps in identifying whether fluctuations in each series’ random component are synchronized or vary independently across the observed dates.",
    "crumbs": [
      "Lesson 1",
      "Time Series Homework: Chapter 3 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_3_1.html#question-3---construction-equipment-manufacturing-cross-correlation-30-points",
    "href": "hw/homework_3_1.html#question-3---construction-equipment-manufacturing-cross-correlation-30-points",
    "title": "Time Series Homework: Chapter 3 Lesson 1",
    "section": "Question 3 - Construction Equipment Manufacturing: Cross-correlation (30 points)",
    "text": "Question 3 - Construction Equipment Manufacturing: Cross-correlation (30 points)\n\na) Please plot a cross-correlogram of the New Orders and Value of Shipments series.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCode\nrandom_joint %&gt;%\n  CCF(y = y_random, x = x_random) %&gt;%\n  autoplot() +\n  labs(\n    title = \"CCF for Random Component of Construction Equipment\\nManufacturing Orders (x) and Shipments (y)\",\n    subtitle = \"\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nb) Please interpret the cross-correlogram of the New Orders and Value of Shipments series. Include descriptions of the statistical and practical significance of the results.\n\n\n\n\n\n\nAnswer\n\n\n\nThe correlogram analysis of the New Orders and Value of Shipments series reveals certain underlying temporal relationships. Positive correlations at lags 0M and 1M (0.229 and 0.059) suggest that recent monthly values of New Orders are closely aligned with those of Shipments, indicating a strong association. As the lag increases, the correlation values fluctuate, with both positive and negative shifts, suggesting possible seasonality or periodic variations. Notably, there is a stronger positive correlation up to around lag 2M, after which correlations decline, pointing to diminishing linear dependence over time. This pattern may imply that, while New Orders and Shipments align in short periods, their association weakens over extended lags, possibly due to external economic factors affecting manufacturing demand and supply differently over time.\n\n\nCode\nlag12\n\n\n# A tibble: 1 × 14\n  lag   `-12M` `-11M` `-10M` `-9M` `-8M` `-7M` `-6M` `-5M`  `-4M`  `-3M` `-2M`\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n1 ccf   -0.099 -0.077 0.045  0.081 -0.03 0.012 0.039 -0.016 -0.078 0.104 0.18 \n# ℹ 2 more variables: `-1M` &lt;chr&gt;, `0M` &lt;chr&gt;\n\n\nCode\nlead12\n\n\n# A tibble: 1 × 14\n  lag   `0M`  `1M`  `2M`   `3M`  `4M`  `5M`  `6M`  `7M`  `8M`  `9M`  `10M` `11M`\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 ccf   0.229 0.059 -0.026 -0.1… -0.1… -0.1… -0.0… -0.0… -0.0… 0.004 -0.07 -0.0…\n# ℹ 1 more variable: `12M` &lt;chr&gt;\n\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2a: Correlograms\nThe student plots a correlogram of the random components of the New Orders and Value of Shipments for the Construction Equipment Manufacturing data using R. The correlogram includes both plots in the same illustration, similar to Figure 5 of Chapter 3 Lesson 1, allowing for easy comparison between the two series. The student correctly uses labels. The R code is well-commented, providing clear explanations of each step in the plotting process, and adheres to best practices for visualization in time series analysis.\nThe correlogram may be missing one or both plots, or the plots may not be correctly aligned for comparison. There may be errors in identifying or labeling the autocorrelation values, making it challenging to interpret the results accurately. The R code may lack sufficient commenting or follow best practices for visualization, hindering comprehension of the plotting process.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2b: Interpretation and Analysis\nThey accurately identify and discuss significant autocorrelation values at various lags, indicating the presence of temporal dependencies within each series. Additionally, the student compares and contrasts the correlograms of the two series, highlighting similarities and differences in their autocorrelation patterns. The analysis includes insights into the potential underlying factors driving the observed autocorrelation patterns, demonstrating a deep understanding of time series analysis concepts and their practical implications.\nThe student attempts to interpret and analyze the correlogram for the New Orders and Value of Shipments series but encounters issues with accuracy, clarity, or completeness. Their discussion may lack depth or coherence, with unclear or erroneous interpretations of significant autocorrelation values. Additionally, the comparison between the correlograms of the two series may be vague or missing key points, indicating a limited understanding of time series analysis concepts. Overall, the student’s analysis may lack sufficient detail or insight, suggesting areas for improvement in their understanding of autocorrelation analysis and its practical implications in time series analysis.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3a: Cross-correlogram\nThe student accurately plots a cross-correlogram of the New Orders and Value of Shipments series using R. The plot includes clear labels and titles allowing for easy interpretation of the results. The R code is well-commented, providing clear explanations of each step in the cross-correlation analysis and plot generation process.\nThe student attempts to plot a cross-correlogram of the New Orders and Value of Shipments series in R but encounters issues with accuracy, clarity, or completeness. The plot may lack clear labels and titles, making it challenging to interpret the results accurately. Additionally, the R code may lack sufficient commenting or clarity, hindering comprehension of the analysis process. Overall, the student’s performance falls short of expectations, indicating a need for improvement in plotting cross-correlograms, labeling plots effectively, and providing clear explanations in R code.\n\n\n\nMastery (20)\nIncomplete (0)\n\n\nQuestion 3b: Interpretation and Analysis\nThey accurately identify and discuss significant cross-correlation values at various lags, indicating the strength and direction of the relationship between the two series. Additionally, the student includes a thoughtful discussion of the statistical significance of the results, highlighting how cross-correlation coefficients measure the linear relationship between the series and whether the observed correlations are statistically significant. Furthermore, the student discusses the practical significance of the results, addressing the real-world relevance or impact of the observed relationships, and how they may inform decision-making or forecasting in the context of construction equipment manufacturing.\nThe student attempts to interpret the cross-correlogram for the New Orders and Value of Shipments series but encounters issues with accuracy, clarity, or completeness. Their discussion may lack depth or coherence, with unclear or erroneous interpretations of significant cross-correlation values. Additionally, the discussion of statistical and practical significance may be vague or missing key points, indicating a limited understanding of these concepts. Overall, the student’s analysis may lack sufficient detail or insight, suggesting areas for improvement in their understanding of cross-correlation analysis and its practical implications in time series analysis.\n\n\nTotal Points\n55",
    "crumbs": [
      "Lesson 1",
      "Time Series Homework: Chapter 3 Lesson 1"
    ]
  },
  {
    "objectID": "hw/homework_3_3.html",
    "href": "hw/homework_3_3.html",
    "title": "Time Series Homework: Chapter 3 Lesson 3",
    "section": "",
    "text": "I did not evaluate the last two code chunks for faster transferring since the forecast models take a bit to load.",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_3_3.html#data",
    "href": "hw/homework_3_3.html#data",
    "title": "Time Series Homework: Chapter 3 Lesson 3",
    "section": "Data",
    "text": "Data\n\ndf &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\")",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_3_3.html#questions",
    "href": "hw/homework_3_3.html#questions",
    "title": "Time Series Homework: Chapter 3 Lesson 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - US Unemployment Rate: Additive Holt-Winters Forecasting (30 points)\nWe have used a measure of US unemployment rates before. This series imported above has not been seasonally adjusted.\n\na) Please use the Holt-Winters smoothing method to the series.\n\n\n\n\n\n\nAnswer\n\n\n\n\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\") |&gt;\n    mutate(ymonth = yearmonth(lubridate::mdy(date)))\ndf &lt;- as_tsibble(df0, index = ymonth)\n# interval(df)\n# has_gaps(df)\n\n\n\n\nts_add &lt;- df |&gt;\n  model(feasts::classical_decomposition(unratensa,\n          type = \"add\"))  |&gt;\n  components()\nautoplot(ts_add)\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nts_mult &lt;- df |&gt;\n  model(feasts::classical_decomposition(unratensa,\n          type = \"mult\"))  |&gt;\n  components()\nautoplot(ts_mult)\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nautoplot(df) +\n    labs(x = \"Months\", y = \"Unratensa\")\n\nPlot variable not specified, automatically selected `.vars = unratensa`\n\n\n\n\n\n\n\n\ndf1 &lt;- df |&gt;\n    model(Additive = ETS(unratensa ~\n        trend(\"A\", alpha = 0.1429622, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\nsum(components(df1)$remainder^2, na.rm = T)\n\n[1] 487.4117\n\naugment(df1) |&gt;\n    ggplot(aes(x = ymonth, y = unratensa)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\n\n\n\n\n\n\n\ndf2 &lt;- df |&gt;\n    model(Additive = ETS(unratensa ~\n        trend(\"A\", alpha = 0.2, beta = 0) +\n        error(\"A\") +\n        season(\"N\"),\n        opt_crit = \"amse\", nmse = 1))\nsum(components(df2)$remainder^2, na.rm = T)\n\n[1] 386.0684\n\n\n\n\n\n\nb) What parameters values did you choose for \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). Justify your choice.\n\n\n\n\n\n\nAnswer\n\n\n\nThe parameters chosen were alpha = 0.1429622 and alpha = 0.2 in two models, with beta = 0 and no seasonality (gamma) excluded, to balance trend sensitivity without overreacting to short-term changes. The moderate alpha values capture recent trends in unemployment while maintaining stability, and beta = 0 prevents fluctuating trend updates. Optimized by minimizing short-term forecast error (average mean squared error), these settings are well-suited for smoothing non-seasonal economic data like unemployment rates.\n\n\n\n\nc) Please plot the Holt-Winters forecast of the series for the next 24 months superimposed against the original series. Please see Figure 7 in Chapter 3: Lesson 3\n\n\n\n\n\n\nAnswer\n\n\n\nOverview of the data\nI wonder if the trend is multiplicative because of how different in can be, and seem to have a increasing or decreasing trend.\nWhen doing the classical add & mult models, this data doesn’t really fit any of the models. I can see how the random component handles the random events a little better the the additive which will make sense since the data can have a downward or upward trend for certain time periods. (which one is the model that we talk about in class that adjust to take into account changes in the season\n\ndf4 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\") |&gt;\n    mutate(ymonth = yearmonth(lubridate::mdy(date)))\ndf5 &lt;- as_tsibble(df4, index = ymonth)\n\ndf6 &lt;-  df5 |&gt;\n    model(Multiplicative = ETS(unratensa ~\n        trend(\"M\") +\n        error(\"A\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\naugment(df6) |&gt;\n    ggplot(aes(x = ymonth, y = unratensa)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndf6 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df5 |&gt; filter(ymonth &gt;= yearmonth(\"2016 Jan\") & ymonth &lt;= yearmonth(\"2019 Dec\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df6) |&gt; filter(ymonth &gt;= yearmonth(\"2016 Jan\") & ymonth &lt;= yearmonth(\"2019 Dec\"))) +\n  scale_color_discrete(name = \"\")\n\ndoing additive\n\ndf4 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\") |&gt;\n    mutate(ymonth = yearmonth(lubridate::mdy(date)))\ndf5 &lt;- as_tsibble(df4, index = ymonth)\n\ndf6 &lt;- df5 |&gt;\n    model(Additive = ETS(unratensa ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\n\naugment(df6) |&gt;\n    ggplot(aes(x = ymonth, y = unratensa)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\ndf6 |&gt;\n  forecast(h = \"2 years\") |&gt; \n  autoplot(df5 |&gt; filter(ymonth &gt;= yearmonth(\"1948 Jan\") & ymonth &lt;= yearmonth(\"2019 Dec\")), level = 95) + \n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(df6) |&gt; filter(ymonth &gt;= yearmonth(\"2016 Jan\") & ymonth &lt;= yearmonth(\"2019 Dec\"))) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\nQuestion 2 - Additive Holt-Winters Forecasting: Evaluation (20 points)\n\na) The Unemployment Rate data stops at the end of 2019. Please access the latest data HERE. Please don’t look at it until you have reached this point in the homework. Compare your forecasting work with with the actual realization of the series. What lessons about forecasting can you learn from the occurrence of Gray Rhino events like a pandemic?\n\n\n\n\n\n\nAnswer\n\n\n\nComparing the forecasted unemployment rates with the actual post-2019 data highlights that the Holt-Winters model could not anticipate the sharp rise in unemployment due to the COVID-19 pandemic, a Gray Rhino event. This underscores that traditional forecasting models, which rely on historical trends, are limited in predicting sudden, large disruptions. Such events emphasize the need for adaptive models that account for potential shocks in economic data.\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1a: HW Smoothing\nDemonstrate the implementation of the Holt-Winters smoothing method in R, providing well-commented code that clearly explains each step of the algorithm. They correctly specify the necessary parameters, including trend and seasonality components.\nStudents encounter difficulties in accurately implementing the Holt-Winters smoothing method in R. Their code may lack sufficient comments or clarity, making it challenging to understand the implementation process. Additionally, they may overlook important parameters or make errors in the application of the method, leading to inaccuracies in the results.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1b: Parameter Choice\nResponses not only specify the chosen parameter values for $\\alpha$, $\\beta$, and $\\gamma$ in the context of the Holt-Winters smoothing method but also correctly identify the purpose of each parameter in their explanation. They provide a thorough justification for each parameter choice, considering factors such as the data characteristics, seasonality patterns, and the desired level of smoothing\nStudent struggles to clearly specify the chosen parameter values for $\\alpha$, $\\beta$, and $\\gamma$. It’s no clear that they understand the purpose of each parameter in their explanation. They may provide limited or vague justification for each parameter choice, lacking consideration of important factors such as data characteristics or seasonality patterns.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1c: Forecast Plot\nResponses effectively create a plot of the Holt-Winters forecast for the next 24 months superimposed against the original series in R. The forecasted values align with the original series and display relevant trends and seasonality patterns. Additionally, they appropriately label the axes, title the plot, and provide a clear legend to distinguish between the original series and the forecast. The plot closely resembles Figure 7 in the Time Series Notebook\nStudent encounter challenges in creating a plot of the Holt-Winters forecast. They may struggle with accurately implementing the plotting code, resulting in inaccuracies or inconsistencies in the plotted forecast. Additionally, their plot may lack proper labeling of the axes, a title, or a legend, making it difficult to interpret the information presented. Furthermore, their plot may deviate significantly from Figure 7 in the Time Series Notebook.\n\n\n\nMastery (20)\nIncomplete (0)\n\n\nQuestion 1a: Gray Rhinos\nStudents provide a comparison between their forecasting results and the actual realization of the series, highlighting areas of accuracy and any discrepancies observed. They demonstrate an understanding of the impact of Gray Rhino event on forecasting. Responses are clear, coherent, and well-structured, ensuring that the evaluation and lessons learned are effectively communicated\nSubmissions provide a limited or superficial comparison between their forecasting results and the actual realization of the series, lacking depth in their analysis or failing to identify key discrepancies.Responses demonstrate a lack of understanding of the impact of Gray Rhino events on forecasting. Explanations may lack clarity, coherence, or structure, making it difficult to understand the evaluation and lessons learned effectively.\n\n\nTotal Points\n50",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_3_5.html",
    "href": "hw/homework_3_5.html",
    "title": "Time Series Homework: Chapter 3 Lesson 5",
    "section": "",
    "text": "ngdp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/gdp_fred.csv\") |&gt;\n  mutate(yquarter = yearquarter(lubridate::mdy(quarter)))\nngdp1 &lt;- as_tsibble(ngdp, index = yquarter)\n# interval(ngdp1)\n# has_gaps(ngdp1)\n\n\n\n\n\nrgdp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/GDPC1.csv\") |&gt;\n  mutate(yq = yearquarter(lubridate::ymd(DATE)))\nrgdp1 &lt;- as_tsibble(rgdp, index = yq)\n# interval(rgdp1)\n# has_gaps(rgdp1)",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_3_5.html#data",
    "href": "hw/homework_3_5.html#data",
    "title": "Time Series Homework: Chapter 3 Lesson 5",
    "section": "",
    "text": "ngdp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/gdp_fred.csv\") |&gt;\n  mutate(yquarter = yearquarter(lubridate::mdy(quarter)))\nngdp1 &lt;- as_tsibble(ngdp, index = yquarter)\n# interval(ngdp1)\n# has_gaps(ngdp1)\n\n\n\n\n\nrgdp &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/GDPC1.csv\") |&gt;\n  mutate(yq = yearquarter(lubridate::ymd(DATE)))\nrgdp1 &lt;- as_tsibble(rgdp, index = yq)\n# interval(rgdp1)\n# has_gaps(rgdp1)",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_3_5.html#questions",
    "href": "hw/homework_3_5.html#questions",
    "title": "Time Series Homework: Chapter 3 Lesson 5",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - Context and Measurement (10 points)\nThe first part of any time series analysis is context. You cannot properly analyze data without knowing what the data is measuring. Without context, the most simple features of data can be obscure and inscrutable. This homework assignment will center around the series below.\nPlease research the time series. In the spaces below, give the data collection process, unit of analysis, and meaning of each observation for the series.\nNominal Gross Domestic Product - United States\nhttps://fred.stlouisfed.org/series/NA000334Q\n\n\n\n\n\n\nAnswer\n\n\n\n\ndf &lt;- left_join(ngdp1, rgdp1, by = c(\"yquarter\" = \"yq\")) |&gt; # joined df\n  select(yquarter, gdp_millions, GDPC1)\n\ndf &lt;- df |&gt;\n    rename(\n        index = yquarter  # rename `yquarter` to `index`\n    ) |&gt;\n    mutate(\n        GDPC1 = as.numeric(GDPC1),\n        gdp_millions = as.numeric(gdp_millions)\n    ) |&gt;\n    as_tsibble(index = index)  # set `index` as the tsibble index\n\nmax_GDPC1 &lt;- max(df$GDPC1, na.rm = TRUE)\nmax_gdp_millions &lt;- max(df$gdp_millions, na.rm = TRUE)\ndf &lt;- df %&gt;%\n    mutate(scaled_gdp_millions = gdp_millions / max_gdp_millions * max_GDPC1)\n\n# Plot with dual y-axes\nggplot(df, aes(x = index)) +\n    geom_line(aes(y = GDPC1, color = \"GDPC1\")) +\n    geom_line(aes(y = scaled_gdp_millions, color = \"gdp_millions\")) +  # Use scaled gdp_millions for plotting\n    scale_y_continuous(\n        name = \"GDPC1\",\n        sec.axis = sec_axis(~ . * max_gdp_millions / max_GDPC1, name = \"gdp_millions\")  # Secondary axis transformation\n    ) +\n    scale_color_manual(values = c(\"GDPC1\" = \"blue\", \"gdp_millions\" = \"red\")) +\n    theme(legend.position = \"bottom\") +\n    labs(color = \"Legend\")\n\n\n\n\n\n\n\nacf_appr &lt;- ACF(df, y = GDPC1) |&gt; autoplot() + # rename df\n    labs(title = \"GDPC1\")\nacf_act &lt;- ACF(df, y = gdp_millions) |&gt; autoplot() +\n    labs(title = \"gdp_millions\")\njoint_ccf_plot &lt;- df |&gt;\n  CCF(y = GDPC1, x = gdp_millions) |&gt; autoplot() +\n  labs(title = \"CCF Plot\")\n(acf_appr + acf_act) / joint_ccf_plot\n\n\n\n\n\n\n\nCCF(df, GDPC1, gdp_millions)\n\n# A tsibble: 43 x 2 [1Q]\n        lag   ccf\n   &lt;cf_lag&gt; &lt;dbl&gt;\n 1     -21Q 0.711\n 2     -20Q 0.722\n 3     -19Q 0.733\n 4     -18Q 0.745\n 5     -17Q 0.757\n 6     -16Q 0.769\n 7     -15Q 0.780\n 8     -14Q 0.790\n 9     -13Q 0.801\n10     -12Q 0.813\n# ℹ 33 more rows\n\n# Fit trend models to the data\n# Fit trend models to the data\napp_model &lt;- df %&gt;%\n    model(trend_model = TSLM(GDPC1 ~ trend()))\n\nact_model &lt;- df %&gt;%\n    model(trend_model = TSLM(gdp_millions ~ trend()))\n\n# Extract the fitted values and residuals using augment()\napp_decompose &lt;- app_model %&gt;%\n    augment() %&gt;%\n    select(index, GDPC1, trend = .fitted, remainder = .resid)\n\nact_decompose &lt;- act_model %&gt;%\n    augment() %&gt;%\n    select(index, gdp_millions, trend = .fitted, remainder = .resid)\n\n# Plot the ACF of the remainder components\napp_random &lt;- ACF(app_decompose, remainder) %&gt;% autoplot() +\n    labs(title = \"ACF of GDPC1 Remainder\")\n\nact_random &lt;- ACF(act_decompose, remainder) %&gt;% autoplot() +\n    labs(title = \"ACF of gdp_millions Remainder\")\n\n# Merge the remainder components for cross-correlation analysis\nrandom_decompose &lt;- app_decompose %&gt;%\n    select(index, random_app = remainder) %&gt;%\n    left_join(\n        act_decompose %&gt;% select(index, random_act = remainder),\n        by = \"index\"\n    )\n\n# Plot the cross-correlation function between the remainder components\njoint_ccf_random &lt;- random_decompose %&gt;%\n    CCF(y = random_app, x = random_act) %&gt;%\n    autoplot() +\n    labs(title = \"CCF between GDPC1 and gdp_millions Remainders\")\n\n# Display the plots\nlibrary(patchwork)\n(app_random + act_random) / joint_ccf_random\n\n\n\n\n\n\n\n\n\n\nReal Gross Domestic Product - United States\nhttps://fred.stlouisfed.org/series/GDPC1\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nQuestion 2 - US Nominal GDP (25 points)\n\na) Please use the Holt-Winters smoothing method to the US Nominal GDP series. Justify your model choice.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nb) What parameters values did you choose for \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). Justify your choice.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nc) Please plot the Holt-Winters forecast of the series for the next 12 months superimposed against the original series. Please see Figure 7 in Chapter 3: Lesson 3.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nd) How confident are you in your forecast? Please explain.\n\n\n\nQuestion 3 - US Real GDP (25 points)\nNominal GDP measures the market value of US output. Because market prices change over time, inflation affects the series. Real GDP is an output measure that keeps prices constant at a point in time, which eliminates the effect of inflation.\nIn homework 2.1 you learned about the output gap, which is a measure of business cycles. In order to estimate the output gap we need some measure of potential GDP, that is, a series that smoothes-out the cycles in the real GDP series. We can do that by choosing the smoothing parameters of the H-W algorithm to capture only the long run path of the economy. The US Congressional Budget Office calculates a series for US Real Potential GDP, please refer to it for the questions that follow.\n\na) Please use H-W to smooth the US Real GDP series to obtain an estimate of US Potential GDP.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nb) What parameters values did you choose for \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). Justify your choice.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1: Context and Measurement\nThe student thoroughly researches the data collection process, unit of analysis, and meaning of each observation for both the requested time series. Clear and comprehensive explanations are provided.\nThe student does not adequately research or provide information on the data collection process, unit of analysis, and meaning of each observation for the specified series.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2a: HW Smoothing\nDemonstrate the implementation of the Holt-Winters smoothing method in R, providing well-commented code that clearly explains each step of the algorithm. They correctly specify the necessary parameters, including trend and seasonality components.\nStudents encounter difficulties in accurately implementing the Holt-Winters smoothing method in R. Their code may lack sufficient comments or clarity, making it challenging to understand the implementation process. Additionally, they may overlook important parameters or make errors in the application of the method, leading to inaccuracies in the results.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2b: Parameter Choice\nResponses not only specify the chosen parameter values for $\\alpha$, $\\beta$, and $\\gamma$ in the context of the Holt-Winters smoothing method but also correctly identify the purpose of each parameter in their explanation. They provide a thorough justification for each parameter choice, considering factors such as the data characteristics, seasonality patterns, and the desired level of smoothing\nStudent struggles to clearly specify the chosen parameter values for $\\alpha$, $\\beta$, and $\\gamma$. It’s no clear that they understand the purpose of each parameter in their explanation. They may provide limited or vague justification for each parameter choice, lacking consideration of important factors such as data characteristics or seasonality patterns.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2c: Forecast Plot\nResponses effectively create a plot of the Holt-Winters forecast for the next 24 months superimposed against the original series in R. The forecasted values align with the original series and display relevant trends and seasonality patterns. Additionally, they appropriately label the axes, title the plot, and provide a clear legend to distinguish between the original series and the forecast. The plot closely resembles Figure 7 in the Time Series Notebook\nStudent encounter challenges in creating a plot of the Holt-Winters forecast. They may struggle with accurately implementing the plotting code, resulting in inaccuracies or inconsistencies in the plotted forecast. Additionally, their plot may lack proper labeling of the axes, a title, or a legend, making it difficult to interpret the information presented. Furthermore, their plot may deviate significantly from Figure 7 in the Time Series Notebook.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3a: HW Smoothing\nDemonstrate the implementation of the Holt-Winters smoothing method in R, providing well-commented code that clearly explains each step of the algorithm. They correctly specify the necessary parameters, including trend and seasonality components.\nStudents encounter difficulties in accurately implementing the Holt-Winters smoothing method in R. The students chose the incorrect model. Additionally, they may overlook important parameters or make errors in the application of the method, leading to inaccuracies in the results.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 3b: Parameter Choice\nResponses not only specify the chosen parameter values for $\\alpha$, $\\beta$, and $\\gamma$ in the context of the Holt-Winters smoothing method but also correctly identify the purpose of each parameter in their explanation. It’s clear the student compared the US Real Potential GDP series to their estimate and chose the parameters to match the characteristics of the example\nStudent struggles to clearly specify the chosen parameter values for $\\alpha$, $\\beta$, and $\\gamma$. It’s no clear that they understand the purpose of each parameter in their explanation. They may provide limited or vague justification for each parameter choice, lacking consideration of important factors such as matching the example time series, data characteristics or seasonality patterns.\n\n\nTotal Points\n55",
    "crumbs": [
      "Overview",
      "Time Series Homework: Chapter 3 Lesson 5"
    ]
  },
  {
    "objectID": "hw/homework_4_3.html",
    "href": "hw/homework_4_3.html",
    "title": "Time Series Homework: Chapter 4 Lesson 3",
    "section": "",
    "text": "ind_prod &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/ind_prod_us.csv\")",
    "crumbs": [
      "Lesson 3",
      "Time Series Homework: Chapter 4 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_4_3.html#data",
    "href": "hw/homework_4_3.html#data",
    "title": "Time Series Homework: Chapter 4 Lesson 3",
    "section": "",
    "text": "ind_prod &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/ind_prod_us.csv\")",
    "crumbs": [
      "Lesson 3",
      "Time Series Homework: Chapter 4 Lesson 3"
    ]
  },
  {
    "objectID": "hw/homework_4_3.html#questions",
    "href": "hw/homework_4_3.html#questions",
    "title": "Time Series Homework: Chapter 4 Lesson 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 - AR(1) with non-zero mean (10 points)\nAn AR(1) process with a non-zero mean \\(\\mu\\) can be expressed by either:\n\\[x_t-\\mu=\\alpha_1(x_{t-1}-\\mu)+w_t \\hspace{5mm}\\mbox{or}\\hspace{5mm}x_t=\\alpha_0+\\alpha_1x_{t-1}+w_t\\]\n\na) Define a similar relationship for an AR(2) process with mean \\(\\mu\\)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nQuestion 2 - Stationarity (25 points)\n\na) Show that the series {\\(x_t\\)} given by \\(x_t=\\frac{3}{2}x_{t-1}-\\frac{1}{2}x_{t-2}+w_t\\) is non-stationary. Please use the characteristic equation method.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nb) Please simulate the series {\\(x_t\\)} using 1000 realizations and calculate the first difference {\\(y_t\\)}, where \\(y_t=\\nabla x_t\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nc) Prove that \\(y_t\\) is stationary, use the characteristic equation method.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nQuestion 3 - Partial Correlogram (40 points)\n\na) Please use the Holt-Winters method to decompose the US Industrial Production Index. Please justify your choice of parameters. Use the context and your understanding of the series as part of your answers.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nb) Plot the remainder (random) component of the decomposed series.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nc) Plot a partial correlogram of the random component of the the US Industrial Production Index. What kind of autoregressive process would you pick to model the random component?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nCriteria\nMastery (10)\nIncomplete (0)\n\n\nQuestion 1a: AR(2) with non-zero mean\nStudents show the mathematical relationship between the parameters of the two expressions for the AR(2) case. They demonstrate this by using algebraic steps to connect the parameters in a clear and logical manner, showing a solid understanding of the mathematical equivalence between the two representations.\nStudents show the mathematical relationship between the parameters of the two expressions. They demonstrate this by using algebraic steps to connect the parameters in a clear and logical manner, showing a solid understanding of the mathematical equivalence between the two representations.\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2a: Non-stationarity\nResponses show that the series is non-stationary using the characteristic equation method. They derive the characteristic equation, find the roots with polyroot(), and identify if any roots lie outside the unit circle using the absolute value if necessary. Submissions provide clear steps, including deriving the characteristic equation, using polyroot, and interpreting the results to conclude non-stationarity.\nSubmissions make errors in computing the characteristic equation or incorrectly interpret the roots. Responses lack clarity in their mathematical steps or misunderstand the concept of stationarity.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 2b: Simulation\nRespones simulate the series {$x_t$} and {$y_t$}. Students provide clear and well-commented code that is easy to follow and execute the first-difference calculation process accurately.\nSturents’ code may lack clarity or sufficient comments, making it difficult to follow. They might make errors in the simulation process or fail to provide clear explanations for each step\n\n\n\nMastery (10)\nIncomplete (0)\n\n\nQuestion 2c: Stationarity\nResponses show that the series is non-stationary using the characteristic equation method. They derive the characteristic equation, find the roots with polyroot(), and identify if any roots lie outside the unit circle using the absolute value if necessary. Submissions provide clear steps, including deriving the characteristic equation, using polyroot, and interpreting the results to conclude non-stationarity.\nSubmissions may lack coherence or precision in articulating how the absence of correlations at higher lags indicates stationarity in {$y_t$}. Additionally, they may overlook the importance of discussing the statistical significance of autocorrelations or fail to provide sufficient evidence from the correlogram to support their explanation.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3a: Decomposition\nResponses apply the Holt-Winters method to decompose the US Industrial Production Index, providing a report of the parameters chosen and justifying their choice. They provide a clear explanation for selecting values for the smoothing parameters (alpha, beta, and gamma) based on their understanding of the series and its characteristics. Proficient submissions show a solid grasp of the method and how the chosen parameters correspond to the trend, seasonality, and noise in the data. They consider contextual factors such as historical behavior to inform their parameter selection.\nStudents attempt to use the Holt-Winters method but lack clarity in justifying their parameter choices. They may struggle to explain why specific values were selected or fail to relate them to the Industrial Production Index. Below expectations submissions may lack coherence in explaining how the chosen parameters align with the data’s components and may overlook contextual factors. Overall, their analysis may lack clarity, suggesting a need for improvement in understanding time series decomposition techniques.\n\n\n\nMastery (5)\nIncomplete (0)\n\n\nQuestion 3b: Plot\nResponses plot the remainder (random) component of the decomposed series. They provide a well-labeled and clear plot that effectively visualizes the random component.\nPlot that lacks clarity, with insufficient labeling or unclear visualization of the random component.\n\n\n\nMastery (20)\nIncomplete (0)\n\n\nQuestion 3c: Partial Correlogram\nResponses plot a partial correlogram of the remainder component and provide a clear interpretation regarding the choice of autoregressive process. They articulate how the significant coefficients in the partial correlogram indicate the lag values of the autoregressive process. Submissions effectively identify the practically significant autocorrelations in the partial correlogram and relate them to the characteristics of potential autoregressive processes, providing a coherent explanation for their choice.\nStudent struggles to accurately plot the partial correlogram of the remainder component or provide a clear interpretation regarding the choice of autoregressive process. They may produce a plot that lacks clarity or fail to effectively communicate the autocorrelation patterns. Submissions overlook important details or fail to provide a coherent explanation for their choice of autoregressive process based on the partial correlogram.\n\n\nTotal Points\n75",
    "crumbs": [
      "Lesson 3",
      "Time Series Homework: Chapter 4 Lesson 3"
    ]
  },
  {
    "objectID": "outcomes.html",
    "href": "outcomes.html",
    "title": "Applied Time Series Analysis Outcomes",
    "section": "",
    "text": "1.2: Use technical language to describe the main features of time series data\n\n\nDefine time series analysis\n\nA time series analysis quantifies the main features in data and the random variation. These reasons, combined with improved computing power, have made time series methods widely applicable in government, industry, and commerce.\n\nDefine time series\n\nTime series are analysed to understand the past and to predict the future, enabling managers or policy makers to make properly informed decisions.\n\nDefine sampling interval\n\nWhen a variable is measured sequentially in time over or at a fixed interval, known as the sampling interval, the resulting data form a time series.\n\nDefine serial dependence or autocorrelation\n\nA correlation of a variable with itself at different times is known as autocorrelation or serial correlation.\n\nDefine a time series trend\n\nIn general, a systematic change in a time series that does not appear to be periodic is known as a trend. The simplest model for a trend is a linear increase or decrease, and this is often an adequate approximation.\n\nDefine seasonal variation\n\nRepeated pattern within each year (or any other fixed time period).\n\nDefine cycle\n\nRepeated pattern that does not correspond to some fixed natural period.\n\nDifferentiate between deterministic and stochastic trends\n\nStochastic Trend: Random trend that does not follow a discernible or predictable pattern. Figure 1\nDeterministic Trend: Can be modeled with mathematical functions, facilitating the long-term prediction of the behavior. Figure 2"
  },
  {
    "objectID": "projects/project1.html",
    "href": "projects/project1.html",
    "title": "Time Series: China Export Commodities",
    "section": "",
    "text": "In today’s global economy, analyzing time series data is essential for predicting trends, understanding seasonality, and identifying unexpected shocks. The objective of this analysis is to explore China’s export commodities over time, using time series analysis techniques to gain insights that can aid in strategic planning and effective decision-making.\n\n\n\nTime series analysis involves collecting and interpreting data recorded over time. By examining China’s export data from early 1990 to the present, we can observe trends, seasonality, and irregular movements that are crucial for understanding the dynamics of international trade.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#introduction",
    "href": "projects/project1.html#introduction",
    "title": "Time Series: China Export Commodities",
    "section": "",
    "text": "In today’s global economy, analyzing time series data is essential for predicting trends, understanding seasonality, and identifying unexpected shocks. The objective of this analysis is to explore China’s export commodities over time, using time series analysis techniques to gain insights that can aid in strategic planning and effective decision-making.\n\n\n\nTime series analysis involves collecting and interpreting data recorded over time. By examining China’s export data from early 1990 to the present, we can observe trends, seasonality, and irregular movements that are crucial for understanding the dynamics of international trade.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#data-preparation",
    "href": "projects/project1.html#data-preparation",
    "title": "Time Series: China Export Commodities",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nShow the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, tsibble, fable, feasts, tsibbledata, fable.prophet, patchwork, lubridate, rio, ggplot2, kableExtra, data.table, plotly)\n\n# Data Import and Preparation\ncexports_ts &lt;- rio::import(\"../data/CHNXTEXVA01NCMLM.csv\") |&gt; \n  mutate(\n    CHNXTEXVA01NCMLM = as.numeric(gsub(\",\", \"\", CHNXTEXVA01NCMLM)), # Remove commas and convert to numeric\n    dates = mdy(date),\n    year = lubridate::year(dates), \n    month = lubridate::month(dates), \n    value = as.numeric(CHNXTEXVA01NCMLM)\n  ) |&gt;\n  dplyr::select(dates, year, month, value)  |&gt;\n  arrange(dates) |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index) |&gt;\n  dplyr::select(index, dates, year, month, value) |&gt;\n  rename(exports_commodities = value) |&gt;\n  mutate(exports_commodities = exports_commodities / 1000)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#chinas-export-trade-commodities-time-series-plot",
    "href": "projects/project1.html#chinas-export-trade-commodities-time-series-plot",
    "title": "Time Series: China Export Commodities",
    "section": "China’s Export Trade Commodities Time Series Plot",
    "text": "China’s Export Trade Commodities Time Series Plot\n\n\nShow the code\nplain_plot &lt;- ggplot(cexports_ts, aes(x = dates, y = exports_commodities)) +\n  geom_rect(aes(xmin = as.Date(\"2008-01-01\"), xmax = as.Date(\"2010-01-01\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_rect(aes(xmin = as.Date(\"2020-02-01\"), xmax = as.Date(\"2020-04-01\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_line() +\n  geom_rect(aes(xmin = as.Date(\"2015-04-10\"), xmax = as.Date(\"2015-04-28\"), ymin = -Inf, ymax = Inf), fill = \"firebrick\", alpha = 0.2) +\n  geom_line() +\n  labs(x = \"\", y = \"Exports Commodities (Billions)\", title = \"Fig 1 - China: Exports Commodities Time Series\") +\n  scale_y_continuous(limits = range(cexports_ts$exports_commodities, na.rm = TRUE)) +\n    scale_x_yearmonth(\n    labels = scales::label_date(\"%Y %b\"),  # Format labels as \"Jan 2020\"\n    breaks = \"36 months\"                    # Show labels every 8 months\n  ) + \n  theme(plot.title = element_text(hjust = 0.5))\n\nplain_plot\n\n\n\n\n\nFig 1 - China’s Exports Commodities Time Series\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig1_exports_commodities_plot.png\", plot = plain_plot, width = 12, height = 6, dpi = 300)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#time-series-decomposition",
    "href": "projects/project1.html#time-series-decomposition",
    "title": "Time Series: China Export Commodities",
    "section": "Time Series Decomposition",
    "text": "Time Series Decomposition\n\nUnderstanding Decomposition\nTime series decomposition breaks down data into three components: trend, seasonality, and irregularities. This helps us understand underlying patterns and make better forecasts.\n\n\nWhy Use a Multiplicative Decomposition?\nThe upward trend in China’s export data suggests that fluctuations in seasonality and irregularities increase in magnitude as exports grow. That’s why we use a multiplicative classical decomposition model, where the data is broken down into:\n\nTrend Component: Captures the long-term growth pattern. Here, we observe steady growth in exports.\nSeasonal Component: Identifies repeating patterns within a year. Exports peak in December (possibly driven by holiday demand, especially in the US) and dip to their lowest in February (likely due to the Chinese New Year and post-holiday slowdowns).\nRandom Component: Captures what’s left after accounting for the trend and seasonal components. It represents what the model cannot explain. These unexpected changes often correlate with macroeconomic events like the 2008 Global Financial Crisis, but not all irregular variations can be directly linked to specific events.\n\n\n\nMultiplicative Classical Decomposition Model\n\n\nShow the code\nchina_decompose_mult &lt;- cexports_ts |&gt;\n  model(feasts::classical_decomposition(exports_commodities, type = \"mult\"))  |&gt;\n  components()\n\ndecompose_plot &lt;- autoplot(china_decompose_mult) +\n  ggtitle(\"Fig 2 - Classical Multiplicative Decomposition of China's Exports\") +\n  labs(subtitle = \"Export commodities = trend (billions) * seasonal * random\")\n\ndecompose_plot\n\n\n\n\n\nFig 2 - Classical Multiplicative Decomposition of China’s Exports\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig2_mult_decomposition.png\", plot = decompose_plot, width = 12, height = 6, dpi = 300)",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#seasonal-component",
    "href": "projects/project1.html#seasonal-component",
    "title": "Time Series: China Export Commodities",
    "section": "Seasonal Component",
    "text": "Seasonal Component\n\n\nShow the code\nplot_decomp_seasonal &lt;- ggplot(china_decompose_mult %&gt;% filter(index &gt;= yearmonth('2019 Dec') & index &lt;= yearmonth('2021 Jan')), aes(x = index, y = seasonal)) +\n  geom_line(group = 1, color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Fig 3 - China: Exports Commodities Seasonal Component\",\n       x = \"Month\",\n       y = \"Seasonal\") +\n  scale_x_yearmonth(\n    labels = scales::label_date(\"%b\"),  \n    breaks = \"1 months\"                    \n  )  + \n  theme(plot.title = element_text(hjust = 0.5))\n\nplot_decomp_seasonal\n\n\n\n\n\nFig 3 - China’s Exports Commodities Seasonal Component\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig3_seasonal_component.png\", plot = plot_decomp_seasonal, width = 12, height = 6, dpi = 300)\n\n\n\nRandom Component and Macroeconomic Events\nThese unexpected changes often correlate with macroeconomic events like the 2008 Global Financial Crisis. For example, in response to a slowdown (random shock), China increased export tax rebates in March 2015 to stimulate exports, showing how responses can occur after the initial shock. This randomness highlights inherent uncertainties—macroeconomic disruptions may explain some shocks, but many remain unpredictable. Understanding the random component helps us see where patterns could align—but also where uncertainties remain.\n\n\nShow the code\nhighlight_months &lt;- c(\"1992 Oct\", \"1994 Jan\", \"1994 Apr\", \"1994 Sep\", \n                      \"1995 Dec\", \"1997 Jul\", \"1998 Jan\", \"1999 Apr\", \n                      \"1999 Nov\", \"2001 Dec\", \"2003 Apr\", \"2004 Jul\", \n                      \"2005 Jul\", \"2008 Sep\", \"2008 Nov\", \"2009 Jun\", \n                      \"2010 May\", \"2010 Dec\", \"2015 Mar\", \"2017 Jan\", \n                      \"2018 Jul\", \"2019 Oct\", \"2019 Dec\", \"2020 Jan\", \n                      \"2020 Mar\")\n\nhighlight_dates &lt;- tsibble::yearmonth(highlight_months)\n\nplot_random_component &lt;- ggplot(china_decompose_mult %&gt;% \n                                  filter(index &gt;= yearmonth('1992 Jul') & \n                                         index &lt;= yearmonth('2023 Dec')), \n                                aes(x = index, y = random)) +\n  geom_line(group = 1, color = \"blue\") +\n  geom_point(data = china_decompose_mult %&gt;% filter(index %in% highlight_dates),\n             aes(x = index, y = random),\n             shape = 24, color = \"red\", fill = \"red\", size = 4) + \n  labs(title = \"Fig 4 - Exports Commodities Random Component vs Macroeconomic Events\",\n       x = \"\",\n       y = \"Random\") +\n  scale_x_yearmonth(\n    labels = scales::label_date(\"%Y %b\"),  \n    breaks = \"36 months\"                    \n  )\n\nplot_random_component\n\n\n\n\n\nFig 4 - Exports Commodities Random Component vs Macroeconomic Events\n\n\n\n\nShow the code\n# Save the plot as a PNG file\n#ggsave(\"fig4_random_vs_macroeventns.png\", plot = plot_random_component, width = 12, height = 6, dpi = 300)\n\n\n\n\nWhy a Multiplicative Decomposition Model Fits\nThe multiplicative model is particularly suitable because the amplitude of seasonal changes and irregular fluctuations increases along with the trend. In China’s export data, as exports grow, the seasonal variations and irregularities also become more pronounced. A multiplicative model accounts for this proportionality, making it appropriate for capturing the dynamics of the data.\n\n\nWhy an Additive Decomposition Model Would Not Work\nAn additive model assumes that the size of seasonal and irregular variations remains constant regardless of the trend. However, the data shows that fluctuations grow as the trend grows. Therefore, an additive model would fail to accurately represent the increasing variability linked to higher export levels.\n\n\nAdditive Classical Decomposition Model Plot\n\n\nShow the code\nchina_decompose_add &lt;- cexports_ts |&gt;\n  model(feasts::classical_decomposition(exports_commodities, type = \"add\"))  |&gt;\n  components()\n\nautoplot(china_decompose_add) +\n  ggtitle(\"Fig 5 - Classical Additive Decomposition of China's Exports\")\n\n\n\n\n\nFig 5 - Classical Additive Decomposition of China’s Exports",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#key-takeaways",
    "href": "projects/project1.html#key-takeaways",
    "title": "Time Series: China Export Commodities",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSeasonal Patterns: Understanding these can help companies optimize their supply chains, especially during high-demand months.\nEconomic Resilience: The trend shows that China’s export sector has consistently been increasing over time.\nStrategic Planning: By anticipating seasonal lows (like in February) and leveraging peaks (in December), businesses can better align their operations with demand cycles.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#recommendations",
    "href": "projects/project1.html#recommendations",
    "title": "Time Series: China Export Commodities",
    "section": "Recommendations",
    "text": "Recommendations\nGiven the observed seasonality, trend, and irregular components of China’s export data, it is important to prepare for upcoming political events that could potentially lead to a temporary fall in exports. However, based on historical patterns, China has demonstrated resilience and the ability to recover fairly quickly from these disruptions. Therefore, we recommend the following actionable items:\n\nDiversify Supply Chains: Identify and establish relationships with alternative suppliers and markets to reduce dependence on China during potential disruptions.\nIncrease Inventory Buffer: Build up inventory reserves during periods of high export activity to prepare for potential downturns caused by political events.\nMonitor Key Indicators: Closely monitor geopolitical developments and economic indicators to respond proactively to any signals of impending disruptions.\nEstablish Contingency Plans: Develop and maintain contingency plans to address short-term declines in supply, including logistical adjustments and finding new suppliers.\nCommunicate with Partners: Maintain open communication with Chinese suppliers and partners for early warnings of potential issues.\nLeverage Seasonal Patterns: Use the identified seasonal peaks to strategically schedule production and export needs, ensuring that reserves are built up during times of predictable high export activity.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project1.html#conclusion",
    "href": "projects/project1.html#conclusion",
    "title": "Time Series: China Export Commodities",
    "section": "Conclusion",
    "text": "Conclusion\nThe decomposition of China’s export data reveals a strong upward trend, along with predictable seasonal fluctuations and irregular disruptions. While upcoming political events may lead to short-term declines, historical data shows that these are typically followed by rapid recovery. By understanding these trends, seasonality, and random components, businesses can better prepare for potential disruptions without significant long-term impacts.\nThe key insight is that while risks are present, proactive preparation can mitigate their effects. By leveraging strategic planning, maintaining communication, and building contingency buffers, companies can continue to operate smoothly and minimize disruptions. This approach ensures resilience in the face of political uncertainties, relying on the observed adaptability of China’s export capabilities.\nWhat’s Next? I’ll continue refining the analysis and exploring potential forecast models to enhance predictive capabilities.",
    "crumbs": [
      "Project 1",
      "Time Series: China Export Commodities"
    ]
  },
  {
    "objectID": "projects/project3.html",
    "href": "projects/project3.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Project 3",
      "Project title here"
    ]
  },
  {
    "objectID": "tools/markdown_visuals.html",
    "href": "tools/markdown_visuals.html",
    "title": "Markdown Visuals",
    "section": "",
    "text": "Callout notes\n\n\n\n\n\n\nType I Errors\n\n\n\nThis is a note callout with:\n\nicon enabled\ndefault appearance\na custom title\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nThis is a tip callout with: - icon disabled - minimal appearance - a custom title\n\n\n\n\n\n\n\n\nCritical Info\n\n\n\nThis is an important callout with: - icon enabled - default appearance - a custom title\n\n\n\n\n\n\n\n\nWarning Alert\n\n\n\nThis is a warning callout with: - icon disabled - minimal appearance - a custom title\n\n\nCode\nprint(\"can also have code chunks inside\")\n\n\n[1] \"can also have code chunks inside\"\n\n\n\n\n\n\n\n\n\n\nThis is a caution callout with: - icon disabled - minimal appearance - no title\n\n\n\ncan do collapse and within\n\n\n\n\n\n\nSecond-Order Properties of a Random Walk\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is a random walk, then the population has the following properties.\n\\[ \\mu_x = 0 \\] and \\[\n  cov(x_t, x_{t+k}) = t \\sigma^2\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = t \\sigma^2\\)?\nFirst, note that that since the terms in the white noise series are independent,\n\\[\ncov ( w_i, w_j ) =\n  \\begin{cases}\n    \\sigma^2, & \\text{if } ~ i=j \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\]\nAlso, when random variables are independent, the covariance of a sum is the sum of the covariance.\nHence, \\[\\begin{align*}\n  cov(x_t, x_{t+k})\n    &= cov ( \\sum_{i=1}^t w_i, \\sum_{j=1}^{t+K} w_j ) \\\\\n    &= \\sum_{i=j} cov ( w_i, w_j ) \\\\\n    &= \\sum_{i=1}^t \\sigma^2 \\\\\n    &= t \\sigma^2\n\\end{align*}\\]\n\n\n\nIf \\(k&gt;0\\) and \\(t&gt;0\\), the correlation function is\n\\[\n  \\rho_k\n  =\n    \\frac{\n            cov(x_t, x_{t+k})\n          }{\n            \\sqrt{var(x_t)} \\sqrt{var(x_{t+k})}\n          }\n  =\n    \\frac{t \\sigma^2}{\\sqrt{t \\sigma^2} \\sqrt{(t+k) \\sigma^2}}\n  =\n    \\frac{1}{\\sqrt{1+\\frac{k}{t}}}\n\\]\n\n\n\n\nColor words\nThis is a normal text and this is purple text in the same line.\npurtle text will then be in purple\n\n\n\n\n Back to top",
    "crumbs": [
      "Tools, Help & Ideas",
      "Markdown Visuals"
    ]
  },
  {
    "objectID": "tools/stepsforDate_index_formatting.html",
    "href": "tools/stepsforDate_index_formatting.html",
    "title": "Steps for formatting Date and Creating Index",
    "section": "",
    "text": "Code\n#source(\"common_functions.R\") # should i be using this one?? not a good idea since many of the formulas return different values\n\n# Loading R packages. originally was using common functions but trying not to use\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,\n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate,\n               rio,\n               ggplot2,\n               kableExtra, tidyquant\n               )\n\n\nThe following is steps to check in what format the date column is in in a new dataset. Then to convert to DATE format to do time series research\n\nImport Data:\nCheck date Column Type:\nConvert date to Date Format:\n\n\n\nCode\n# the rename is for a different df\n\n# Step 1: Import your data\ndf &lt;- read.csv(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") \n# We'll assume the data has a column called 'date' (replace with the actual column name) and a 'value' column\n\n\n# Step 2: Check the structure of the 'date' column to verify its type\nstr(df$date)  # This will show you if the date column is a character, Date, or something else\n\n\n# Convert 'date' column to Date type if it's in character format \n# 2.1 reomve unwanted columns or\ndf1 &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date) # date is date columm name\n         # mdy(date) mdy is current order of date, lubridate will format to ymd. \n       #dplyr::select(-comments) # remove unwanted columns\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) # rename columns\n\n# 2.1 Convert 'Date' column to yearquarter format\ndf$Date &lt;- lubridate::ymd(df$Date) # df2\n\n\n\n# Verify the 'date' column is now in the correct format (should be Date type)\nstr(df$date)  # Should now return 'date'\n\n\nDoing mutate by getting weekly average\n\n\nCode\n# Set symbol and date range for Apple\nsymbol &lt;- \"AAPL\"\ndate_start &lt;- \"2022-01-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Download the stock data\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Convert to a tsibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(dates = lubridate::ymd(date), value = adjusted) %&gt;%\n  mutate(year_week = yearweek(dates)) |&gt;\n  group_by(year_week) |&gt;\n  summarise(value = mean(value)) |&gt;\n  ungroup() |&gt;\n  as_tsibble(index = year_week)\n\n# Time plot of the daily closing prices\nautoplot(stock_ts, value) +\n  labs(title = \"Time Plot of Apple (AAPL) Daily Closing Prices\",\n       x = \"Date\", y = \"Closing Price (USD)\")\n\n\n\n\n\n\n\n\n\nthe code below was taken from project one, made to plot the time series without doing the monthly mean. The first two lines of code are missing\ndata is for daily data.\n\n\nCode\n# this code is also not complete\n\n\n# the first 2 ts can almost be taken from the previous code.\n# this code was first use but replace by the above code\n\n  dplyr::select(dates, year, months, value)  |&gt; # ts3\n  arrange(dates) |&gt; # ts4\n  mutate(index = tsibble::yearmonth(dates)) |&gt; # ts5\n  as_tsibble(index = index) |&gt; # ts6\n  dplyr::select(index, dates, year, months, value) |&gt; # ts7\n  rename(Vessels = value) # rename value to emphasize data context\nvessels_ts |&gt; #ts8\n  autoplot(.vars = Vessels) +\n  labs(\n    x = \"Month\",\n    y = \"Vessels Cleared Monthly\",\n    title = \"Vessels Cleared Monthly in Foreing Trade for United Sates\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\nworking with two variables and period column\n\n\nCode\n# I think this file is all for samples, and I edit them so now the code is all mix up hence all the eval=false! sucks \n\ndf &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/constructionequip_manu_orders_shipments.csv\")\n\n# str(df$date)\n\n\ndf1 &lt;- df |&gt;\n  mutate(date = lubridate::mdy(date),\n         constructionequip_ord = as.numeric(constructionequip_ord), # make sure numeric for x variables\n         constructionequip_ship = as.numeric(constructionequip_ship)\n         ) |&gt;\n  rename(x = constructionequip_ord, y = constructionequip_ship) |&gt; # renames columns and converts to numeric\n  select(date, x, y) # re orders and or removes not selected columns\n  \ndf2 &lt;- df1 |&gt; # this makes a new df so either df before or this one is use. \n  mutate(obs = row_number()) |&gt; # makes new column with periods\n  select(obs, x, y)\n\n# can the obs code to make multiple df with different columns. \n\n\n\n\ndata has gaps\nTyson notes\n\nfilling with previous variable\n\nThen there is 100% correlation with the previous variable\n\nFilling with average\n\nsome data’s average, like weather can vary so it can trow off random,\nusing the same variable as last year is an option, it will just mess with the seasonality\n\ntaking the sum of the lag and lead periods, and divide by two to replace missing\n\nits a good one\n\n\n\n\nCode\ndf0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/UNRATENSA.csv\") |&gt;\n    mutate(YM = yearmonth(lubridate::mdy(date)))\ndf &lt;- as_tsibble(df0, index = YM) |&gt;\n  select(ym, cdebt)\ninterval(df) # gives interval: M, D ot Y etc\nhas_gaps(df) # false if none and vice versa True\n\n\n\n\nReading different file formats\n\n\nCode\n# wine_dat &lt;- read_table(\"data/wine.dat\") # resource 3.4.2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Date & Index formatting",
      "Steps for formatting Date and Creating Index"
    ]
  },
  {
    "objectID": "outcomes.html#footnotes",
    "href": "outcomes.html#footnotes",
    "title": "Applied Time Series Analysis Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA time series model that is stationary in the mean is ergodic in the mean if the time average for a single time series tends to the ensemble mean as the length of the time series increases (2.2 2.2.3).↩︎"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Applied Time Series Analysis Outcomes",
    "section": "",
    "text": "e3.1 cross covariance function (\\(ccvf\\)), $_k (x, y), as a function of the lag, k:\n\\[\n\\gamma_k (x, y) = E[(x_{t + k} - \\mu_x)(y_t - \\mu_y)]\n\\]\ne3.2\nSome textbooks define ccvf with the variable y lagging when k is positive, but we have used the definition that is consistent with R. Whichever way you choose to define the ccvf\n\\[\n\\gamma_k (x, y) = \\gamma_{- k} (y, x)\n\\]\ne3.3 cross-correlation function (\\(ccf\\))\nWhen we have several variables and wish to refer to the acvf of one rather than the ccvf of a pair, we can write it as, for example, \\(γ_k(x,x)\\). The lag \\(k\\) cross-correlation function (\\(ccf\\)), \\(p_k(x,y)\\), is defined by\n\\[\np_k(x, y) = \\frac{\\gamma_k(x, y)}{\\sigma_x\\sigma_y}\n\\]\nThe ccvf and ccf can be estimated from a time series by their sample equivalents.\ne3.4 sample cross-covariance function (\\(ccvf\\)) or (\\(sccf\\)) is calculatred as\n\\[\nc_k(x, y) = \\frac{1}{n} \\sum_{t = 1}^{n - k} (x_{t + k} - \\bar{x})(y_t - \\bar{y})\n\\]\ne3.5 sample autocorrelation function (\\(acf\\)) is define as\n\\[\nr_k (x, y) = \\frac{c_k(x, y)}{\\sqrt{c_0(x, x)c_0(y, y)}}\n\\]\ne3.6 Bass Formula\nThe Bass formula for the number of people, \\(N_t\\), who have bought a product at time \\(t\\) depends on three parameters: the total number of people who eventually buy the product, \\(m\\); the coefficient of innovation, \\(p\\); and the coefficient of imitation, \\(q\\). The Bass formula is\n\\[\nN_{t +1} = N_t + p(m - N_t) + qN_t (m - N_t) / m\n\\]\nAccording to the model, the increase in sales, \\(N_{t+1} − N_t\\), over the next time period is equal to the sum of a fixed proportion p and a time varying proportion \\(q\\frac{N_t}{m}\\) of people who will eventually buy the product but have not yet done so. The rationale for the model is that initial sales will be to people who are interested in the novelty of the product, whereas later sales will be to people who are drawn to the product after seeing their friends and acquaintances use it. Equation (3.6) is a difference equation and its solution is (e3.7)\ne3.7 Bass Formula Solution?\n\\[\nN_t = m\\frac{1 - e^{-(p + q)t}}{1 + (q / p)e^{-(p + q)t}}\n\\]\nIt is easier to verify this result for the continuous-time version of the model.\ne3.8 hazard\nOne interpretation of the Bass model is that the time from product launch until purchase is assumed to have a probability distribution that can be parametrised in terms of \\(p\\) and \\(q\\). A plot of sales per time unit against time is obtained by multiplying the probability density by the number of people, \\(m\\), who eventually buy the product. Let \\(f(t)\\), \\(F(t)\\), and \\(h(t)\\) be the density, cumulative distribution function (\\(cdf\\)), and hazard, respectively, of the distribution of time until purchase. The definition of the hazard …\nthe definition of the hazard is\n\\[\nh(t) = \\frac{f(t)}{1 - F(t)}\n\\]\nThe interpretation of the hazard is that if it is multiplied by a small time increment it gives the probability that a random purchaser who has not yet made the purchase will do so in the next small time increment (Exercise 2). Then the continuous time model of the Bass formula can be expressed in terms of the hazard (e3.9):\ne3.9 Continues time model of the Bass formula can be expressed in terms of the hazard\n\\[\nh(t) = p + qF(t)\n\\]\nEquation (3.6) is the discrete form of Equation (3.9) (Exercise 2). The solution of Equation (3.8), with \\(h(t)\\) given by Equation (3.9), for \\(F(t)\\) is (e3.10)\ne3.10\n\\[\nF(t) = \\frac {1 - e^{-(p+q)t}}{1 + (q/p) e^{-(p+q)t}}\n\\]\nTwo special cases of the distribution are the exponential distribution and logistic distribution, which arise when \\(q = 0\\) and \\(p = 0\\), respectively. The logistic distribution closely resembles the normal distribution (Exercise 3). Cumulative sales are given by the product of \\(m\\) and \\(F(t)\\). The pdf is the derivative of Equation (3.10):\ne3.11\n\\[\nf(t) = \\frac{(p + q)^2 e^{-(p+q)t}}{p[1 + (q/p)e^{-(p+q)t}]^2}\n\\]\nSales per unit time at time \\(t\\) are (e3.12)\ne3.12\n\\[\nS(t) = mf(t) = \\frac{m(p + q)^2 e^{-(p + q)t}} {p [ 1 + (q/p)e^{-(p+q)t}]^2}\n\\]\nThe time to peak is (e3.13)\ne3.13 The time to peak is\n\\[\nt_{peak} = \\frac {log(q) - log(p)} {p + q}\n\\]\ne3.14 Forecasting Sales\n3.4.1 Exponential smoothing:\nOur objective is to predict some future value \\(x_{n+k}\\) given a past history \\({x1,x2,...,xn}\\) of observations up to time \\(n\\). In this subsection we assume there is no systematic trend or seasonal effects in the process, or that these have been identified and removed. The mean of the process can change from one time step to the next, but we have no information about the likely direction of these changes. A typical application is forecasting sales of a well-established product in a stable market. The model is\n\\[\nx_t = \\mu_t + w_t\n\\]\nwhere \\(\\mu_t\\) is the non-stationary mean of the process at time \\(t\\) and \\(w_t\\) are independent random deviations with a mean of 0 and a standard deviation \\(\\sigma\\). We will follow the notation in R and let at be our estimate of \\(\\mu_t\\). Given that there is no systematic trend, an intuitively reasonable estimate of the mean at time \\(t\\) is given by a weighted average of our observation at time \\(t\\) and our estimate of the mean at time \\(t−1\\) (e3.15):\ne3.15 exponentially weighted moving average (\\(EWMA\\))\n\\[\na_t = \\alpha x_t + (1 - \\alpha)a_{t-1}   \\qquad \\qquad     0 &lt; \\alpha &lt; 1\n\\]\n\nThe \\(a_t\\) in Equation (3.15) is the exponentially weighted moving average (\\(EWMA\\)) at time \\(t\\). The value of \\(\\alpha\\) determines the amount of smoothing, and it is referred to as the smoothing parameter. If \\(\\alpha\\) is near 1, there is little smoothing and \\(a_t\\) is approximately \\(x_t\\). This would only be appropriate if the changes in the mean level were expected to be large by comparison with \\(\\sigma\\). At the other extreme, a value of \\(\\alpha\\) near \\(0\\) gives highly smoothed estimates of the mean level and takes little account of the most recent observation. This would only be appropriate if the changes in the mean level were expected to be small compared with \\(\\sigma\\). A typical compromise figure for \\(\\alpha\\) is 0.2 since in practice we usually expect that the change in the mean between time \\(t − 1\\) and time \\(t\\) is likely to be smaller than \\(\\sigma\\). Alternatively, R can provide an estimate for \\(\\alpha\\), and we discuss this option below. Since we have assumed that there is no systematic trend and that there are no seasonal effects, forecasts made at time \\(n\\) for any lead time are just the estimated mean at time \\(n\\). The forecasting equation is\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tools_help_ideas.html#key-differences-between-general-and-sampling-distributions",
    "href": "tools_help_ideas.html#key-differences-between-general-and-sampling-distributions",
    "title": "Tools, Resources and Help Ideas",
    "section": "Key Differences Between General and Sampling Distributions",
    "text": "Key Differences Between General and Sampling Distributions\n\n\n\n\n\n\n\n\nAspect\nGeneral Distributions\nSampling Distributions\n\n\n\n\nFocus\nDistribution of raw data or random variables\nDistribution of a statistic (e.g., sample mean, proportion)\n\n\nPopulation or Sample\nDescribes population or a single sample\nDerived from repeated sampling of a population\n\n\nExamples\nNormal, Uniform, Exponential\nSampling distribution of sample mean, proportion, etc.\n\n\nDerived From\nDirectly observed data\nRepeatedly calculated from samples\n\n\nShape\nDepends on the data (e.g., normal, skewed)\nDepends on sample size and population (Central Limit Theorem often applies)\n\n\nRole in Statistics\nDescribe the data or population characteristics\nUsed for inferential statistics (e.g., confidence intervals, hypothesis testing)",
    "crumbs": [
      "Tools, Help & Ideas",
      "Tools, Resources and Help Ideas"
    ]
  }
]